{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA over unstructured data\n",
    "\n",
    "Using Match LSTM, Pointer Networks, as mentioned in paper https://arxiv.org/pdf/1608.07905.pdf\n",
    "\n",
    "We start with the pre-processing provided by https://github.com/MurtyShikhar/Question-Answering to clean up the data and make neat para, ques files.\n",
    "\n",
    "\n",
    "### @TODOs:\n",
    "\n",
    "1. Figure out how to put in real, pre-trained embeddings in embeddings layer.\n",
    "2. Explicitly provide batch size when instantiating model\n",
    "3. is ./val.ids.* validation set or test set?\n",
    "4. Instead of test loss, calculate test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda:3\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug Legend\n",
    "\n",
    "- 5: Print everything that goes in every tensor.\n",
    "- 4: ??\n",
    "- 3: Check every model individually\n",
    "- 2: Print things in training loops\n",
    "- 1: ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macros \n",
    "DATA_LOC = './data/squad/'\n",
    "DEBUG = 2\n",
    "\n",
    "# nn Macros\n",
    "QUES_LEN, PARA_LEN =  30, 770\n",
    "VOCAB_SIZE = 115299                  # @TODO: get actual size\n",
    "HIDDEN_DIM = 128\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 814                    # Might have total 100 batches.\n",
    "EPOCHS = 10\n",
    "TEST_EVERY_ = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder \n",
    "Use a simple lstm class to have encoder for question and paragraph. \n",
    "The output of these will be used in the match lstm\n",
    "\n",
    "$H^p = LSTM(P)$ \n",
    "\n",
    "\n",
    "$H^q = LSTM(Q)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputlen, hiddendim, embeddingdim, vocablen):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Catch dim\n",
    "        self.inputlen, self.hiddendim, self.embeddingdim, self.vocablen = inputlen, hiddendim, embeddingdim, vocablen\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(self.vocablen, self.embeddingdim)\n",
    "       \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(self.embeddingdim, self.hiddendim)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        # Returns a new hidden layer var for LSTM\n",
    "        return (torch.zeros((1, BATCH_SIZE, self.hiddendim), device=device), \n",
    "                torch.zeros((1, BATCH_SIZE, self.hiddendim), device=device))\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        \n",
    "        # Input: x (1, batch, ) (current input)\n",
    "        # Hidden: h (1, batch, hiddendim) (last hidden state)\n",
    "        \n",
    "        if DEBUG > 4: print(\"x:\\t\", x.shape)\n",
    "        if DEBUG > 4: print(\"h:\\t\", h[0].shape, h[1].shape)\n",
    "        \n",
    "        x_emb = self.embedding(x)\n",
    "        if DEBUG > 4: print(\"x_emb:\\t\", x_emb.shape)\n",
    "            \n",
    "        ycap, h = self.lstm(x_emb.view(-1, BATCH_SIZE, self.embeddingdim), h)\n",
    "        if DEBUG > 4: print(\"ycap:\\t\", ycap.shape)\n",
    "        \n",
    "        return ycap, h\n",
    "    \n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     print (\"Trying out question encoder LSTM\")\n",
    "#     model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "#     dummy_x = torch.tensor([22,45,12], dtype=torch.long)\n",
    "#     hidden = model.init_hidden()\n",
    "#     ycap, h = model(dummy_x, hidden)\n",
    "    \n",
    "#     print(ycap.shape)\n",
    "#     print(h[0].shape, h[1].shape)\n",
    "\n",
    "\n",
    "if DEBUG > 2:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        dummy_para = torch.randint(0,VOCAB_SIZE-1,(PARA_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,PARA_LEN).long()\n",
    "    #     print (dummy_para.shape)\n",
    "        dummy_question = torch.randint(0,VOCAB_SIZE-1,(QUES_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,QUES_LEN).long()\n",
    "    #     print (dummy_question.shape)\n",
    "\n",
    "    #     print(\"LSTM with batches\")\n",
    "        ques_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE).cuda(device)\n",
    "        para_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE).cuda(device)\n",
    "        ques_hidden = ques_model.init_hidden()\n",
    "        para_hidden = para_model.init_hidden()\n",
    "        ques_embedded,hidden_ques = ques_model(dummy_question,ques_hidden)\n",
    "        para_embedded,hidden_para = para_model(dummy_para,para_hidden)\n",
    "        \n",
    "        print (ques_embedded.shape) # question_length,batch,embedding_dim\n",
    "        print (para_embedded.shape) # para_length,batch,embedding_dim\n",
    "        print (hidden_para[0].shape,hidden_para[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match LSTM\n",
    "\n",
    "Use a match LSTM to compute a **summarized sequential vector** for the paragraph w.r.t the question.\n",
    "\n",
    "Consider the summarized vector ($H^r$) as the output of a new decoder, where the inputs are $H^p, H^q$ computed above. \n",
    "\n",
    "1. Attend the para word $i$ with the entire question ($H^q$)\n",
    "  \n",
    "    1. $\\vec{G}_i = tanh(W^qH^q + repeat(W^ph^p_i + W^r\\vec{h^r_{i-1} + b^p}))$\n",
    "    \n",
    "    2. *Computing it*: Here, $\\vec{G}_i$ is equivalent to `energy`, computed differently.\n",
    "    \n",
    "    3. Use a linear layer to compute the content within the $repeat$ fn.\n",
    "    \n",
    "    4. Add with another linear (without bias) with $H_q$\n",
    "    \n",
    "    5. $tanh$ the bloody thing\n",
    "  \n",
    "  \n",
    "2. Softmax over it to get $\\alpha$ weights.\n",
    "\n",
    "    1. $\\vec{\\alpha_i} = softmax(w^t\\vec{G}_i + repeat(b))$\n",
    "    \n",
    "3. Use the attention weight vector $\\vec{\\alpha_i}$ to obtain a weighted version of the question and concat it with the current token of the passage to form a vector $\\vec{z_i}$\n",
    "\n",
    "4. Use $\\vec{z_i}$ to compute the desired $h^r_i$:\n",
    "\n",
    "    1. $ h^r_i = LSTM(\\vec{z_i}, h^r_{i-1}) $\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchLSTMEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, ques_len ):\n",
    "        \n",
    "        super(MatchLSTMEncoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim, self.ques_len = hidden_dim, ques_len\n",
    "        \n",
    "        # Catch lens and params\n",
    "        self.lin_g_repeat = nn.Linear(2*self.hidden_dim, hidden_dim)\n",
    "        self.lin_g_nobias = nn.Linear(self.hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.alpha_i_w = nn.Parameter(torch.FloatTensor(self.hidden_dim, 1))\n",
    "        self.alpha_i_b= nn.Parameter(torch.FloatTensor((1)))\n",
    "        \n",
    "        self.lstm_summary = nn.LSTM(self.hidden_dim*(self.ques_len+2), self.hidden_dim)\n",
    "                                      \n",
    "    \n",
    "    def forward(self, h_pi, h_ri, H_q, hidden):\n",
    "        \n",
    "        # For h_r i\n",
    "        # encoded para word: h_pi (1, batch, hidden_dim  )\n",
    "        # encoded ques all: H_q (seqlen, batch, hidden_dim)\n",
    "        # last hidden state: h_ri (1, batch, hidden_dim) (i-1th) \n",
    "        \n",
    "        if DEBUG > 4:\n",
    "            print( \"h_pi:\\t\\t\\t\", h_pi.shape)\n",
    "            print( \"h_ri:\\t\\t\\t\", h_ri.shape)\n",
    "            print( \"H_q:\\t\\t\\t\", H_q.shape)\n",
    "        \n",
    "        lin_repeat_input = torch.cat((h_pi, h_ri), dim=2)\n",
    "        if DEBUG > 4: print(\"lin_repeat_input:\\t\", lin_repeat_input.shape)\n",
    "        \n",
    "        lin_g_input_b = self.lin_g_repeat(lin_repeat_input)\n",
    "        if DEBUG > 4: print(\"lin_g_input_b unrepeated:\", lin_g_input_b.shape)\n",
    "            \n",
    "        lin_g_input_b = lin_g_input_b.repeat(H_q.shape[0], 1, 1)\n",
    "        if DEBUG > 4: print(\"lin_g_input_b:\\t\\t\", lin_g_input_b.shape)\n",
    "            \n",
    "        # lin_g_input_a = self.lin_g_nobias.matmul(H_q.view(-1, self.ques_len, self.hidden_dim)) #self.lin_g_nobias(H_q)\n",
    "        lin_g_input_a =  self.lin_g_nobias(H_q)\n",
    "        if DEBUG > 4: print(\"lin_g_input_a:\\t\\t\", lin_g_input_a.shape)\n",
    "            \n",
    "        G_i = F.tanh(lin_g_input_a + lin_g_input_b)\n",
    "        if DEBUG > 4: print(\"G_i:\\t\\t\\t\", G_i.shape)\n",
    "        # Note; G_i should be a 1D vector over ques_len\n",
    "        \n",
    "        # Attention weights\n",
    "        alpha_i_input_a = G_i.view(BATCH_SIZE, -1, self.hidden_dim).matmul(self.alpha_i_w).view(BATCH_SIZE, 1, -1)\n",
    "        if DEBUG > 4: print(\"alpha_i_input_a:\\t\", alpha_i_input_a.shape)\n",
    "            \n",
    "        alpha_i_input = alpha_i_input_a.add_(self.alpha_i_b.view(-1,1,1).repeat(1,1,self.ques_len))\n",
    "        if DEBUG > 4: print(\"alpha_i_input:\\t\\t\", alpha_i_input.shape)\n",
    "        \n",
    "        # Softmax over alpha inputs\n",
    "        alpha_i = F.softmax(alpha_i_input, dim=-1)\n",
    "        if DEBUG > 4: print(\"alpha_i:\\t\\t\", alpha_i.shape)\n",
    "            \n",
    "        # Weighted summary of question with alpha    \n",
    "        z_i_input_b = (\n",
    "                        H_q.view(BATCH_SIZE, QUES_LEN, -1) *\n",
    "                       (alpha_i.view(BATCH_SIZE, self.ques_len, -1).repeat(1,1,self.hidden_dim))\n",
    "                      ).view(self.ques_len,BATCH_SIZE,-1)\n",
    "        if DEBUG > 4: print(\"z_i_input_b:\\t\\t\", z_i_input_b.shape)\n",
    "            \n",
    "        z_i = torch.cat((h_pi, z_i_input_b), dim=0)\n",
    "        if DEBUG > 4: print(\"z_i:\\t\\t\\t\", z_i.shape)\n",
    "                        \n",
    "        # Pass z_i, h_ri to the LSTM \n",
    "        lstm_input = torch.cat((z_i.view(1,BATCH_SIZE,-1), h_ri), dim=2)\n",
    "        if DEBUG > 4: print(\"lstm_input:\\t\\t\", lstm_input.shape)\n",
    "        \n",
    "        h_ri, hidden = self.lstm_summary(lstm_input, hidden)\n",
    "        if DEBUG > 4:\n",
    "            print(\"h_ri new:\\t\\t\", h_ri.shape)\n",
    "            print(\"hidden new:\\t\\t\", hidden[0].shape, hidden[1].shape)\n",
    "        \n",
    "        \n",
    "        return h_ri, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros((1, BATCH_SIZE, self.hidden_dim), device=device),\n",
    "                torch.zeros((1, BATCH_SIZE, self.hidden_dim), device=device))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN)\n",
    "#     h_pi = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "#     h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "#     hidden = model.init_hidden()\n",
    "#     H_q = torch.randn(QUES_LEN, BATCH_SIZE, HIDDEN_DIM)\n",
    "    \n",
    "#     op, hid = model(h_pi, h_ri, H_q, hidden)\n",
    "    \n",
    "#     print(\"\\nDone:op\", op.shape)\n",
    "#     print(\"Done:hid\", hid[0].shape, hid[1].shape)\n",
    "\n",
    "if DEBUG > 2:\n",
    "    with torch.no_grad():\n",
    "        matchLSTMEncoder = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN).cuda(device)\n",
    "        hidden = matchLSTMEncoder.init_hidden()\n",
    "        h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "    #     if DEBUG:\n",
    "    #         print (\"init h_ri shape is: \", h_ri.shape)\n",
    "    #         print (\"the para length is \", len(para_embedded))\n",
    "        for i in range(len(para_embedded)):\n",
    "            h_ri, hidden =  matchLSTMEncoder(para_embedded[i].view(1,BATCH_SIZE,-1), h_ri, ques_embedded, hidden)\n",
    "            para_embedded[i] = h_ri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network\n",
    "\n",
    "Using a ptrnet over $H_r$ to unfold and get most probable spans.\n",
    "We use the **boundry model** to do that (predict start and end of seq).\n",
    "\n",
    "A simple energy -> softmax -> decoder. Where softmaxed energy is supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PointerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super(PointerDecoder, self).__init__()\n",
    "        \n",
    "        # Keep args\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lin_f_repeat = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.lin_f_nobias = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        \n",
    "        self.beta_k_w = nn.Parameter(torch.FloatTensor(self.hidden_dim, 1))\n",
    "        self.beta_k_b = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.hidden_dim*(PARA_LEN+1), self.hidden_dim)\n",
    "\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros((1, 1, self.hidden_dim), device=device),\n",
    "                torch.zeros((1, 1, self.hidden_dim), device=device))\n",
    "    \n",
    "    def forward(self, h_ak, H_r, hidden):\n",
    "        \n",
    "        # h_ak (current decoder's last op) (1,batch,hiddendim)\n",
    "        # H_r (weighted summary of para) (P, batch, hiddendim)\n",
    "        \n",
    "        if DEBUG > 4:\n",
    "            print(\"h_ak:\\t\\t\\t\", h_ak.shape)\n",
    "            print(\"H_r:\\t\\t\\t\", H_r.shape)\n",
    "            print(\"hidden:\\t\\t\\t\", hidden[0].shape, hidden[1].shape)\n",
    "            \n",
    "        # Prepare inputs for the tanh used to compute energy\n",
    "        f_input_b = self.lin_f_repeat(h_ak)\n",
    "        if DEBUG > 4: print(\"f_input_b unrepeated:  \", f_input_b.shape)\n",
    "        \n",
    "        #H_r shape is ([PARA_LEN, BATCHSIZE, EmbeddingDIM])\n",
    "        f_input_b = f_input_b.repeat(H_r.shape[0], 1, 1)\n",
    "        if DEBUG: print(\"f_input_b repeated:\\t\", f_input_b.shape)\n",
    "            \n",
    "        f_input_a = self.lin_f_nobias(H_r)\n",
    "        if DEBUG > 4: print(\"f_input_a:\\t\\t\", f_input_a.shape)\n",
    "            \n",
    "        # Send it off to tanh now\n",
    "        F_k = F.tanh(f_input_a+f_input_b)\n",
    "        if DEBUG > 4: print(\"F_k:\\t\\t\\t\", F_k.shape) #PARA_LEN,BATCHSIZE,EmbeddingDim\n",
    "            \n",
    "        # Attention weights\n",
    "        beta_k_input_a = F_k.view(BATCH_SIZE, -1, self.hidden_dim).matmul(self.beta_k_w).view(BATCH_SIZE, 1, -1)\n",
    "        if DEBUG > 4: print(\"beta_k_input_a:\\t\\t\", beta_k_input_a.shape)\n",
    "            \n",
    "        beta_k_input = beta_k_input_a.add_(self.beta_k_b.repeat(1,1,PARA_LEN))\n",
    "        if DEBUG > 4: print(\"beta_k_input:\\t\\t\", beta_k_input.shape)\n",
    "            \n",
    "        beta_k = F.softmax(beta_k_input, dim=-1)\n",
    "        if DEBUG > 4: print(\"beta_k:\\t\\t\\t\", beta_k.shape)\n",
    "            \n",
    "        lstm_input_a = H_r.view(BATCH_SIZE, PARA_LEN, -1) * (beta_k.view(BATCH_SIZE, PARA_LEN, -1).repeat(1,1,self.hidden_dim))\n",
    "        if DEBUG > 4: print(\"lstm_input_a:\\t\\t\", lstm_input_a.shape)\n",
    "            \n",
    "        lstm_input = torch.cat((lstm_input_a.view(1, BATCH_SIZE,-1), h_ak.view(1, BATCH_SIZE, -1)), dim=2)\n",
    "        if DEBUG > 4: print(\"lstm_input:\\t\\t\", lstm_input.shape)\n",
    "        \n",
    "        h_ak, hidden = self.lstm(lstm_input, hidden)\n",
    "        \n",
    "        return h_ak, hidden, beta_k\n",
    "            \n",
    "if DEBUG > 2:\n",
    "    with torch.no_grad():\n",
    "        pointerDecoder = PointerDecoder(HIDDEN_DIM).cuda(device)\n",
    "        h_ak = torch.randn(1,BATCH_SIZE,HIDDEN_DIM, device=device)\n",
    "    #     H_r = torch.randn(PARA_LEN, BATCH_SIZE, HIDDEN_DIM)\n",
    "        pointerHidden = pointerDecoder.init_hidden()\n",
    "        h_ak, hidden, beta_k = pointerDecoder(h_ak, para_embedded, hidden)\n",
    "        print (beta_k.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Idiot Proofing the code-so-far"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_dummy_data(batch_size,dimension,vocab_size,max_passage_length=50,max_question_length=10):\n",
    "    '''\n",
    "        Create dummy data of given batch size. \n",
    "        If batch size is -1 then the function returns a pair of passage and question\n",
    "    '''\n",
    "    #@TODO: Implement logic for batch != -1\n",
    "    min_index = 1\n",
    "    max_index = vocab_size\n",
    "    \n",
    "    if batch_size == -1:\n",
    "        passage_length = max_passage_length\n",
    "        passage_node = torch.randint(min_index,max_index,(passage_length,)).long()\n",
    "        question_length = max_question_length\n",
    "        question_node = torch.randint(min_index,max_index,(question_length,)).long()\n",
    "        answer_start_node = torch.zeros((passage_length,)).long()\n",
    "        answer_start_node[passage_length-4] = 1\n",
    "        answer_end_node = torch.zeros((passage_length,)).long()\n",
    "        answer_end_node[passage_length-1] = 1\n",
    "        return passage_node,question_node,answer_start_node,answer_end_node\n",
    "    else:\n",
    "        passage_length = max_passage_length\n",
    "        passage_node = torch.randint(min_index,max_index,(passage_length*batch_size,)).long()\n",
    "        passage_node = passage_node.view(batch_size,passage_length)\n",
    "        \n",
    "        question_length = max_question_length\n",
    "        question_node = torch.randint(min_index,max_index,(question_length*batch_size,)).long()\n",
    "        question_node = question_node.view(batch_size,question_length)\n",
    "        \n",
    "        answer_start_node = torch.zeros((passage_length,)).long()\n",
    "        answer_start_node[passage_length-4] = 1\n",
    "        answer_start_node = answer_start_node.repeat(batch_size,1).view(batch_size,-1)\n",
    "        \n",
    "        answer_end_node = torch.zeros((passage_length,)).long()\n",
    "        answer_end_node[passage_length-1] = 1\n",
    "        answer_end_node = answer_end_node.repeat(batch_size,1).view(batch_size,-1)\n",
    "        \n",
    "        return passage_node,question_node,answer_start_node,answer_end_node\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "passage_node,question_node,answer_start_node,answer_end_node =  (create_dummy_data(12,10,10,max_passage_length=50,max_question_length=10))\n",
    "print(\"Passage_node: \", passage_node.shape)\n",
    "print(\"Question_node: \", question_node.shape)\n",
    "print(\"Answer_start_node: \", answer_start_node.shape)\n",
    "print(\"Answer_end_node: \", answer_end_node.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Testing the entire deal in a neat no_grad"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# with torch.no_grad():\n",
    "\n",
    "    dummy_para = torch.randint(0,VOCAB_SIZE-1,(PARA_LEN*BATCH_SIZE,)).view(BATCH_SIZE,PARA_LEN).long()\n",
    "    print (dummy_para.shape)\n",
    "    dummy_question = torch.randint(0,VOCAB_SIZE-1,(QUES_LEN*BATCH_SIZE,)).view(BATCH_SIZE,QUES_LEN).long()\n",
    "    print (dummy_question.shape)\n",
    "    \n",
    "    print(\"LSTM with batches\")\n",
    "    ques_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "    para_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "    ques_hidden = ques_model.init_hidden()\n",
    "    para_hidden = para_model.init_hidden()\n",
    "    ques_embedded,hidden_ques = ques_model(dummy_question,ques_hidden)\n",
    "    para_embedded,hidden_para = para_model(dummy_para,para_hidden)\n",
    "    \n",
    "    matchLSTMEncoder = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN)\n",
    "    hidden = matchLSTMEncoder.init_hidden()\n",
    "    h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "    if DEBUG:\n",
    "        print (\"init h_ri shape is: \", h_ri.shape)\n",
    "        print (\"the para length is \", len(para_embedded))\n",
    "    for i in range(len(para_embedded)):\n",
    "        h_ri, hidden =  matchLSTMEncoder(para_embedded[i].view(1,BATCH_SIZE,-1), h_ri, ques_embedded, hidden)\n",
    "        para_embedded[i] = h_ri\n",
    "        DEBUG = False\n",
    "    DEBUG = not DEBUG   \n",
    "    \n",
    "    pointerDecoder = PointerDecoder(HIDDEN_DIM)\n",
    "    h_ak = torch.randn(1,BATCH_SIZE,HIDDEN_DIM)\n",
    "#     H_r = torch.randn(PARA_LEN, BATCH_SIZE, HIDDEN_DIM)\n",
    "    pointerHidden = pointerDecoder.init_hidden()\n",
    "    h_ak, hidden, beta_k = pointerDecoder(h_ak, para_embedded, hidden)\n",
    "    print (beta_k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull the real data from disk.\n",
    "\n",
    "Files stored in `./data/squad/train.ids.*`\n",
    "Pull both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Q:  (81403,)\n",
      "Train P:  (81403,)\n",
      "Train Y:  (81403, 2)\n",
      "Test Q:  (4285,)\n",
      "Test P:  (4285,)\n",
      "Test Y:  (4285, 2)\n"
     ]
    }
   ],
   "source": [
    "train_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(DATA_LOC, 'train.ids.question')))])\n",
    "train_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(DATA_LOC, 'train.ids.context')))])\n",
    "train_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(DATA_LOC, 'train.span')))])\n",
    "\n",
    "test_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(DATA_LOC, 'val.ids.question')))])\n",
    "test_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(DATA_LOC, 'val.ids.context')))])\n",
    "test_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(DATA_LOC, 'val.span')))])\n",
    "\n",
    "print(\"Train Q: \", train_q.shape)\n",
    "print(\"Train P: \", train_p.shape)\n",
    "print(\"Train Y: \", train_y.shape)\n",
    "print(\"Test Q: \", test_q.shape)\n",
    "print(\"Test P: \", test_p.shape)\n",
    "print(\"Test Y: \", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "index_train, index_test = np.arange(len(train_p)), np.arange(len(test_p))\n",
    "np.random.shuffle(index_train)\n",
    "np.random.shuffle(index_test)\n",
    "\n",
    "train_p, train_q, train_y = train_p[index_train], train_q[index_train], train_y[index_train]\n",
    "test_p, test_q, test_y = test_p[index_test], test_q[index_test], test_y[index_test]\n",
    "\n",
    "# Pad and prepare\n",
    "train_P = np.zeros((len(train_p), PARA_LEN))\n",
    "train_Q = np.zeros((len(train_q), QUES_LEN))\n",
    "train_Y_start = np.zeros((len(train_p), PARA_LEN))\n",
    "train_Y_end = np.zeros((len(train_p), PARA_LEN))\n",
    "\n",
    "test_P = np.zeros((len(test_p), PARA_LEN))\n",
    "test_Q = np.zeros((len(test_q), QUES_LEN))\n",
    "test_Y_start = np.zeros((len(test_p), PARA_LEN))\n",
    "test_Y_end = np.zeros((len(test_p), PARA_LEN))\n",
    "\n",
    "crop_train = []    # Remove these rows from training\n",
    "for i in range(len(train_p)):\n",
    "    p = train_p[i]\n",
    "    q = train_q[i]\n",
    "    y = train_y[i]\n",
    "    \n",
    "    # First see if you can keep this example or not (due to size)\n",
    "    if y[0] > PARA_LEN or y[1] > PARA_LEN:\n",
    "        crop.append(i)\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    train_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "    train_Q[i, :min(QUES_LEN, len(q))] = p[:min(QUES_LEN, len(q))]\n",
    "    train_Y_start[i, y[0]] = 1\n",
    "    train_Y_end[i, y[1]] = 1\n",
    "    \n",
    "crop_test = []\n",
    "for i in range(len(test_p)):\n",
    "    p = test_p[i]\n",
    "    q = test_q[i]\n",
    "    y = test_y[i]\n",
    "    \n",
    "    # First see if you can keep this example or not (due to size)\n",
    "    if y[0] > PARA_LEN or y[1] > PARA_LEN:\n",
    "        crop.append(i)\n",
    "        continue\n",
    "        \n",
    "    test_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "    test_Q[i, :min(QUES_LEN, len(q))] = p[:min(QUES_LEN, len(q))]\n",
    "    test_Y_start[i, y[0]] = 1\n",
    "    test_Y_end[i, y[1]] = 1\n",
    "    \n",
    "    \n",
    "# Let's free up some memory now\n",
    "train_p, train_q, train_y, test_p, test_q, test_y = None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, and running the model\n",
    "- Write a train fn\n",
    "- Write a training loop invoking it\n",
    "- Fill in real data\n",
    "\n",
    "----------\n",
    "\n",
    "Feats:\n",
    "- Function to test every n epochs.\n",
    "- Report train accuracy every epoch\n",
    "- Store the train, test accuracy for every instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(para_batch,\n",
    "          ques_batch,\n",
    "          answer_start_batch,\n",
    "          answer_end_batch,\n",
    "          ques_model,\n",
    "          para_model,\n",
    "          match_LSTM_encoder_model,\n",
    "          pointer_decoder_model,\n",
    "          optimizer, \n",
    "          loss_fn):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    :param para_batch: paragraphs (batch, max_seq_len_para) \n",
    "    :param ques_batch: questions corresponding to para (batch, max_seq_len_ques)\n",
    "    :param answer_start_batch: one-hot vector denoting pos of span start (batch, max_seq_len_para)\n",
    "    :param answer_end_batch: one-hot vector denoting pos of span end (batch, max_seq_len_para)\n",
    "    \n",
    "    # Models\n",
    "    :param ques_model: model to encode ques\n",
    "    :param para_model: model to encode para\n",
    "    :param match_LSTM_encoder_model: model to match para, ques to get para summary\n",
    "    :param pointer_decoder_model: model to get a pointer over start and end span pointer\n",
    "    \n",
    "    # Loss and Optimizer.\n",
    "    :param loss_fn: \n",
    "    :param optimizer: \n",
    "    \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    if DEBUG >=2: \n",
    "        print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "        print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "        print(\"\\tanswer_start_batch:\\t\", answer_start_batch.shape)\n",
    "        print(\"\\tanswer_end_batch:\\t\\t\", answer_end_batch.shape)\n",
    "    \n",
    "    # Wiping all gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Initializing all hidden states.\n",
    "    hidden_quesenc = ques_model.init_hidden()\n",
    "    hidden_paraenc = para_model.init_hidden()\n",
    "    hidden_mlstm = match_LSTM_encoder_model.init_hidden()\n",
    "    hidden_ptrnet = pointer_decoder_model.init_hidden()\n",
    "    h_ri = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "    h_ak = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "    \n",
    "    if DEBUG >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "    \n",
    "    #passing the data through LSTM pre-processing layer\n",
    "    H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc)\n",
    "    H_p, para_model_hidden = para_model(para_batch, hidden_paraenc)\n",
    "    \n",
    "    print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "    print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "    \n",
    "    if DEBUG >= 2: \n",
    "        raw_input(\"Check memory and ye shall continue\")\n",
    "        print(\"------------Encoded hidden states------------\")\n",
    "    \n",
    "    H_r = [0 for x in range(len(H_p))]\n",
    "    #Augmenting the paragraph embedding with attentioned weighted question\n",
    "    for i in range(len(H_p)):\n",
    "        h_pi = H_p[i]\n",
    "        h_ri, hidden_mlstm =  match_LSTM_encoder_model(h_pi.view(1,BATCH_SIZE,-1),\n",
    "                                                                    h_ri, H_q, hidden_mlstm)\n",
    "        H_r[i] = h_ri\n",
    "        if DEBUG >= 2: \n",
    "            print(\"\\th_ri\\t\\t\\t:\", h_ri.shape, \"\\titer: \", i)\n",
    "#         DEBUG = False\n",
    "#     DEBUG = True\n",
    "    \n",
    "    H_r = torch.stack(H_r)\n",
    "\n",
    "    if DEBUG >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "    \n",
    "    \n",
    "    #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "    h_ak, hidden_ptrnet , beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet)\n",
    "    h_ak, hidden_ptrnet , beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet)\n",
    "    \n",
    "    if DEBUG >= 2: print(\"------------Passed through pointernet------------\")\n",
    "    \n",
    "    #How will we manage batches for loss.\n",
    "    loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "    loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "    \n",
    "    if DEBUG >= 2: print(\"------------Calculated loss------------\")\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    if DEBUG >= 2: print(\"------------Calculated Gradients------------\")\n",
    "    \n",
    "    #optimization step\n",
    "    optimizer.step()\n",
    "    \n",
    "    if DEBUG >= 2: print(\"------------Updated weights.------------\")\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(para_batch,\n",
    "          ques_batch,\n",
    "          answer_start_batch,\n",
    "          answer_end_batch,\n",
    "            \n",
    "          ques_model,\n",
    "          para_model,\n",
    "          match_LSTM_encoder_model,\n",
    "          pointer_decoder_model,\n",
    "          \n",
    "            loss_fn):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    :param para_batch: paragraphs (batch, max_seq_len_para) \n",
    "    :param ques_batch: questions corresponding to para (batch, max_seq_len_ques)\n",
    "    :param answer_start_batch: one-hot vector denoting pos of span start (batch, max_seq_len_para)\n",
    "    :param answer_end_batch: one-hot vector denoting pos of span end (batch, max_seq_len_para)\n",
    "    \n",
    "    # Models\n",
    "    :param ques_model: model to encode ques\n",
    "    :param para_model: model to encode para\n",
    "    :param match_LSTM_encoder_model: model to match para, ques to get para summary\n",
    "    :param pointer_decoder_model: model to get a pointer over start and end span pointer\n",
    "    \n",
    "    # Loss and Optimizer.\n",
    "    :param loss_fn: \n",
    "    :param optimizer: \n",
    "    \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Initializing all hidden states.\n",
    "        hidden_quesenc = ques_model.init_hidden()\n",
    "        hidden_paraenc = para_model.init_hidden()\n",
    "        hidden_mlstm = match_LSTM_encoder_model.init_hidden()\n",
    "        hidden_ptrnet = pointer_decoder_model.init_hidden()\n",
    "        h_ri = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        h_ak = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "\n",
    "        #passing the data through LSTM pre-processing layer\n",
    "        H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc)\n",
    "        H_p, para_model_hidden = para_model(para_batch, hidden_paraenc)\n",
    "\n",
    "        H_r = [0 for x in range(len(H_p))]\n",
    "        #Augmenting the paragraph embedding with attentioned weighted question\n",
    "        for i in range(len(H_p)):\n",
    "            h_pi = H_p[i]\n",
    "            h_ri, hidden_mlstm =  match_LSTM_encoder_model(h_pi.view(1,BATCH_SIZE,-1),\n",
    "                                                                        h_ri, H_q, hidden_mlstm)\n",
    "            H_r[i] = h_ri\n",
    "#             DEBUG = False\n",
    "#         DEBUG = True\n",
    "        \n",
    "        # Convert H_r to a proper tensor\n",
    "        H_r = torch.stack(H_r)\n",
    "\n",
    "        \n",
    "        print(\"Passed through matchlstm\")\n",
    "        \n",
    "        #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "        h_ak, hidden_ptrnet , beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet)\n",
    "        h_ak, hidden_ptrnet , beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet)\n",
    "        \n",
    "        #How will we manage batches for loss.\n",
    "        loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "        loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "    \n",
    "    return loss, beta_k_start, beta_k_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 / 10\n",
      "Batch: 0 \n",
      "\tpara_batch:\t\t torch.Size([814, 770])\n",
      "\tques_batch:\t\t torch.Size([814, 30])\n",
      "\tanswer_start_batch:\t torch.Size([814, 770])\n",
      "\tanswer_end_batch:\t\t torch.Size([814, 770])\n",
      "------------Instantiated hidden states------------\n",
      "\tH_q:\t\t torch.Size([30, 814, 128])\n",
      "\tH_p:\t\t torch.Size([770, 814, 128])\n",
      "Check memory and ye shall continue\n",
      "------------Encoded hidden states------------\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  0\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  1\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  2\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  3\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  4\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  5\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  6\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  7\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  8\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  9\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  10\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  11\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  12\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  13\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  14\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  15\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  16\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  17\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  18\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  19\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  20\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  21\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  22\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  23\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  24\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  25\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  26\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  27\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  28\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  29\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  30\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  31\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  32\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  33\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  34\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  35\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  36\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  37\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  38\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  39\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  40\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  41\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  42\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  43\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  44\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  45\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  46\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  47\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  48\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  49\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  50\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  51\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  52\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  53\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  54\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  55\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  56\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  57\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  58\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  59\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  60\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  61\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  62\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  63\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  64\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  65\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  66\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  67\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  68\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  69\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  70\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  71\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  72\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  73\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  74\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  75\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  76\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  77\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  78\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  79\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  80\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  81\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  82\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  83\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  84\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  85\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  86\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  87\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  88\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  89\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  90\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  91\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  92\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  93\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  94\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  95\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  96\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  97\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  98\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  99\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  100\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  101\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  102\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  103\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  104\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  105\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  106\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  107\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  108\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  109\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  110\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  111\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  112\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  113\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  114\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  115\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  116\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  117\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  118\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  119\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  120\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  121\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  122\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  123\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  124\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  125\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  126\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  127\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  128\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  129\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  130\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  131\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  132\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  133\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  134\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  135\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  136\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  137\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  138\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  139\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  140\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  141\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  142\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  143\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  144\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  145\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  146\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  147\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  148\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  149\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  150\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  151\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  152\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  153\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  154\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  155\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  156\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  157\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  158\n",
      "\th_ri\t\t\t: torch.Size([1, 814, 128]) \titer:  159\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /pytorch/aten/src/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fa57fbe46ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mpointer_decoder_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpointer_decoder_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         )\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b35adebeab89>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(para_batch, ques_batch, answer_start_batch, answer_end_batch, ques_model, para_model, match_LSTM_encoder_model, pointer_decoder_model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mh_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         h_ri, match_LSTM_encoder_hidden =  match_LSTM_encoder_model(h_pi.view(1,BATCH_SIZE,-1),\n\u001b[0;32m---> 67\u001b[0;31m                                                                     h_ri, H_q, hidden_mlstm)\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mH_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_ri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/priyansh/virtualenvironment/pwc-reg-m/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f4824925ba88>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h_pi, h_ri, H_q, hidden)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lstm_input:\\t\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mh_ri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h_ri new:\\t\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_ri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/priyansh/virtualenvironment/pwc-reg-m/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/priyansh/virtualenvironment/pwc-reg-m/local/lib/python2.7/site-packages/torch/nn/modules/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/priyansh/virtualenvironment/pwc-reg-m/local/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/priyansh/virtualenvironment/pwc-reg-m/local/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hx, batch_sizes)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvariable_length\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             dropout_ts)\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /pytorch/aten/src/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "\"\"\"\n",
    "    > Instantiate models\n",
    "    > Instantiate loss, optimizer\n",
    "    > Instantiate ways to store loss\n",
    "    \n",
    "    > Per epoch\n",
    "        > sample batch and give to train fn\n",
    "        > get loss\n",
    "        > if epoch %k ==0: get test accuracy\n",
    "    \n",
    "    > have fn to calculate test accuracy\n",
    "\"\"\"\n",
    "\n",
    "# Instantiate models\n",
    "ques_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE).cuda(device)\n",
    "para_model = Encoder(PARA_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE).cuda(device)\n",
    "match_LSTM_encoder_model = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN).cuda(device)\n",
    "pointer_decoder_model = PointerDecoder(HIDDEN_DIM).cuda(device)\n",
    "\n",
    "# Instantiate Loss\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(list(ques_model.parameters()) + \n",
    "                       list(para_model.parameters()) + \n",
    "                       list(match_LSTM_encoder_model.parameters()) + \n",
    "                       list(pointer_decoder_model.parameters()))\n",
    "\n",
    "# Losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch: \", epoch, \"/\", EPOCHS)\n",
    "        \n",
    "    epoch_loss = 0.0\n",
    "    epoch_time = time.time()\n",
    "        \n",
    "    for iter in range(int(len(train_P)/BATCH_SIZE)):\n",
    "        print(\"Batch: %d \" % iter)\n",
    "        \n",
    "        # Sample batch and train on it\n",
    "        sample_index = np.random.randint(0, len(train_P), BATCH_SIZE)\n",
    "        \n",
    "        loss = train(\n",
    "            para_batch = torch.tensor(train_P[sample_index], dtype=torch.long, device=device),\n",
    "            ques_batch = torch.tensor(train_Q[sample_index], dtype=torch.long, device=device),\n",
    "            answer_start_batch = torch.tensor(train_Y_start[sample_index], dtype=torch.long, device=device),\n",
    "            answer_end_batch = torch.tensor(train_Y_end[sample_index], dtype=torch.long, device=device),\n",
    "            ques_model = ques_model,\n",
    "            para_model = para_model,\n",
    "            match_LSTM_encoder_model = match_LSTM_encoder_model,\n",
    "            pointer_decoder_model = pointer_decoder_model,\n",
    "            optimizer = optimizer, \n",
    "            loss_fn= loss_fn\n",
    "        )\n",
    "    \n",
    "        epoch_loss += loss\n",
    "        \n",
    "    print(\"Time taken: %s\" % (time.time() - epoch_time))\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    if epoch % TEST_EVERY_ == 0:\n",
    "        # Then get acc/loss on testset too\n",
    "        \n",
    "#         sample_index = np.random.randint(0, len(train_P), BATCH_SIZE)\n",
    "\n",
    "#         test_loss = predict(\n",
    "#             para_batch = \n",
    "#             ques_batch = np.random.choice(train_Q, BATCH_SIZE),\n",
    "#             answer_start_batch = np.random.choice(train_Y_start, BATCH_SIZE),\n",
    "#             answer_end_batch = np.random.choice(train_Y_end, BATCH_SIZE),\n",
    "#             ques_model = ques_model,\n",
    "#             para_model = para_model,\n",
    "#             match_LSTM_encoder_model = match_LSTM_encoder_model,\n",
    "#             pointer_decoder_model = pointer_decoder_model, \n",
    "#             loss_fn = loss_fn)[0]\n",
    "#         if DEBUG: print(\"TEST_LOSS: \", test_loss)\n",
    "            \n",
    "#         test_losses.append(test_loss)\n",
    "        print(\"Poop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
