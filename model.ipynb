{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA over unstructured data\n",
    "\n",
    "Using Match LSTM, Pointer Networks, as mentioned in paper https://arxiv.org/pdf/1608.07905.pdf\n",
    "\n",
    "We start with the pre-processing provided by https://github.com/MurtyShikhar/Question-Answering to clean up the data and make neat para, ques files.\n",
    "\n",
    "\n",
    "### @TODOs:\n",
    "\n",
    "1. [done] _Figure out how to put in real, pre-trained embeddings in embeddings layer._\n",
    "2. [done] _Explicitly provide batch size when instantiating model_\n",
    "3. is ./val.ids.* validation set or test set?: **validation**\n",
    "4. [done:em] emInstead of test loss, calculate test acc metrics\n",
    "    1. todo: new metrics like P, R, F1\n",
    "5. Update unit test codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock to pull up embeddings. Needs to run before following imports\n",
    "import numpy as np\n",
    "\n",
    "# Macros \n",
    "DATA_LOC = './data/squad/'\n",
    "EMBEDDING_FILE = 'glove.trimmed.300.npz'\n",
    "VOCAB_FILE = 'vocab.dat'\n",
    "\n",
    "file_loc = DATA_LOC + EMBEDDING_FILE\n",
    "glove_file = np.load(open(file_loc))['glove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "from io import open\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug Legend\n",
    "\n",
    "- 5: Print everything that goes in every tensor.\n",
    "- 4: ??\n",
    "- 3: Check every model individually\n",
    "- 2: Print things in training loops\n",
    "- 1: ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macros \n",
    "DATA_LOC = './data/squad/'\n",
    "DEBUG = 2\n",
    "\n",
    "# nn Macros\n",
    "QUES_LEN, PARA_LEN =  30, 770\n",
    "VOCAB_SIZE = glove_file.shape[1]                  # @TODO: get actual size\n",
    "HIDDEN_DIM = 150\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 81                  # Might have total 100 batches.\n",
    "EPOCHS = 1\n",
    "TEST_EVERY_ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder \n",
    "Use a simple lstm class to have encoder for question and paragraph. \n",
    "The output of these will be used in the match lstm\n",
    "\n",
    "$H^p = LSTM(P)$ \n",
    "\n",
    "\n",
    "$H^q = LSTM(Q)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputlen, macros, glove_file):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Catch dim\n",
    "        self.inputlen = inputlen\n",
    "        self.hiddendim = macros['hidden_dim']\n",
    "        self.embeddingdim =  macros['embedding_dim']\n",
    "        self.vocablen = macros['vocab_size']\n",
    "        \n",
    "        self.batch_size = macros['batch_size']\n",
    "        self.debug = macros['debug']\n",
    "        \n",
    "        # Embedding Layer\n",
    "#         self.embedding = nn.Embedding(self.vocablen, self.embeddingdim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(glove_file))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "       \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(self.embeddingdim, self.hiddendim)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        # Returns a new hidden layer var for LSTM\n",
    "        return (torch.zeros((1, batch_size, self.hiddendim), device=device), \n",
    "                torch.zeros((1, batch_size, self.hiddendim), device=device))\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        \n",
    "        # Input: x (batch, len ) (current input)\n",
    "        # Hidden: h (1, batch, hiddendim) (last hidden state)\n",
    "        \n",
    "        # Batchsize: b int (inferred)\n",
    "        b = x.shape[0]\n",
    "        \n",
    "        if self.debug > 4: print(\"x:\\t\", x.shape)\n",
    "        if self.debug > 4: print(\"h:\\t\", h[0].shape, h[1].shape)\n",
    "        \n",
    "        x_emb = self.embedding(x)\n",
    "        if self.debug > 4: print(\"x_emb:\\t\", x_emb.shape)\n",
    "            \n",
    "        ycap, h = self.lstm(x_emb.view(-1, b, self.embeddingdim), h)\n",
    "        if self.debug > 4: print(\"ycap:\\t\", ycap.shape)\n",
    "        \n",
    "        return ycap, h\n",
    "    \n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     print (\"Trying out question encoder LSTM\")\n",
    "#     model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "#     dummy_x = torch.tensor([22,45,12], dtype=torch.long)\n",
    "#     hidden = model.init_hidden()\n",
    "#     ycap, h = model(dummy_x, hidden)\n",
    "    \n",
    "#     print(ycap.shape)\n",
    "#     print(h[0].shape, h[1].shape)\n",
    "\n",
    "\n",
    "if DEBUG > 2:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        dummy_para = torch.randint(0,VOCAB_SIZE-1,(PARA_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,PARA_LEN).long()\n",
    "    #     print (dummy_para.shape)\n",
    "        dummy_question = torch.randint(0,VOCAB_SIZE-1,(QUES_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,QUES_LEN).long()\n",
    "    #     print (dummy_question.shape)\n",
    "\n",
    "    #     print(\"LSTM with batches\")\n",
    "        ques_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE).cuda(device)\n",
    "        para_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE).cuda(device)\n",
    "        ques_hidden = ques_model.init_hidden()\n",
    "        para_hidden = para_model.init_hidden()\n",
    "        ques_embedded,hidden_ques = ques_model(dummy_question,ques_hidden)\n",
    "        para_embedded,hidden_para = para_model(dummy_para,para_hidden)\n",
    "        \n",
    "        print (ques_embedded.shape) # question_length,batch,embedding_dim\n",
    "        print (para_embedded.shape) # para_length,batch,embedding_dim\n",
    "        print (hidden_para[0].shape,hidden_para[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match LSTM\n",
    "\n",
    "Use a match LSTM to compute a **summarized sequential vector** for the paragraph w.r.t the question.\n",
    "\n",
    "Consider the summarized vector ($H^r$) as the output of a new decoder, where the inputs are $H^p, H^q$ computed above. \n",
    "\n",
    "1. Attend the para word $i$ with the entire question ($H^q$)\n",
    "  \n",
    "    1. $\\vec{G}_i = tanh(W^qH^q + repeat(W^ph^p_i + W^r\\vec{h^r_{i-1} + b^p}))$\n",
    "    \n",
    "    2. *Computing it*: Here, $\\vec{G}_i$ is equivalent to `energy`, computed differently.\n",
    "    \n",
    "    3. Use a linear layer to compute the content within the $repeat$ fn.\n",
    "    \n",
    "    4. Add with another linear (without bias) with $H_q$\n",
    "    \n",
    "    5. $tanh$ the bloody thing\n",
    "  \n",
    "  \n",
    "2. Softmax over it to get $\\alpha$ weights.\n",
    "\n",
    "    1. $\\vec{\\alpha_i} = softmax(w^t\\vec{G}_i + repeat(b))$\n",
    "    \n",
    "3. Use the attention weight vector $\\vec{\\alpha_i}$ to obtain a weighted version of the question and concat it with the current token of the passage to form a vector $\\vec{z_i}$\n",
    "\n",
    "4. Use $\\vec{z_i}$ to compute the desired $h^r_i$:\n",
    "\n",
    "    1. $ h^r_i = LSTM(\\vec{z_i}, h^r_{i-1}) $\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchLSTMEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, macros):\n",
    "        \n",
    "        super(MatchLSTMEncoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = macros['hidden_dim']\n",
    "        self.ques_len = macros['ques_len']\n",
    "        self.batch_size = macros['batch_size']\n",
    "        self.debug = macros['debug']    \n",
    "        \n",
    "        # Catch lens and params\n",
    "        self.lin_g_repeat = nn.Linear(2*self.hidden_dim, self.hidden_dim)\n",
    "        self.lin_g_nobias = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        self.alpha_i_w = nn.Parameter(torch.FloatTensor(self.hidden_dim, 1))\n",
    "        self.alpha_i_b = nn.Parameter(torch.FloatTensor((1)))\n",
    "        \n",
    "        self.lstm_summary = nn.LSTM(self.hidden_dim*(self.ques_len+2), self.hidden_dim)\n",
    "                                      \n",
    "    \n",
    "    def forward(self, H_p, h_ri, H_q, hidden):\n",
    "        \"\"\"\n",
    "            Ideally, we would have manually unrolled the lstm \n",
    "            but due to memory constraints, we do it in the module.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find the batchsize\n",
    "        batch_size = H_p.shape[1]\n",
    "        \n",
    "        H_r = torch.empty((0, batch_size, self.hidden_dim), device=device, dtype=torch.float)\n",
    "        H_r = torch.cat((H_r, h_ri), dim=0)\n",
    "        \n",
    "        if self.debug > 4:\n",
    "            print( \"H_p:\\t\\t\\t\", H_p.shape)\n",
    "            print( \"h_ri:\\t\\t\\t\", h_ri.shape)\n",
    "            print( \"H_q:\\t\\t\\t\", H_q.shape)\n",
    "        \n",
    "        for i in range(H_p.shape[0]):\n",
    "            \n",
    "            lin_repeat_input = torch.cat((H_p[i].view(1, batch_size, -1), H_r[i].view(1, batch_size, -1)), dim=2)\n",
    "            if self.debug > 4: print(\"lin_repeat_input:\\t\", lin_repeat_input.shape)\n",
    "\n",
    "            lin_g_input_b = self.lin_g_repeat(lin_repeat_input)\n",
    "            if self.debug > 4: print(\"lin_g_input_b unrepeated:\", lin_g_input_b.shape)\n",
    "\n",
    "            lin_g_input_b = lin_g_input_b.repeat(H_q.shape[0], 1, 1)\n",
    "            if self.debug > 4: print(\"lin_g_input_b:\\t\\t\", lin_g_input_b.shape)\n",
    "\n",
    "            # lin_g_input_a = self.lin_g_nobias.matmul(H_q.view(-1, self.ques_len, self.hidden_dim)) #self.lin_g_nobias(H_q)\n",
    "            lin_g_input_a =  self.lin_g_nobias(H_q)\n",
    "            if self.debug > 4: print(\"lin_g_input_a:\\t\\t\", lin_g_input_a.shape)\n",
    "\n",
    "            G_i = F.tanh(lin_g_input_a + lin_g_input_b)\n",
    "            if self.debug > 4: print(\"G_i:\\t\\t\\t\", G_i.shape)\n",
    "            # Note; G_i should be a 1D vector over ques_len\n",
    "\n",
    "            # Attention weights\n",
    "            alpha_i_input_a = G_i.view(batch_size, -1, self.hidden_dim).matmul(self.alpha_i_w).view(batch_size, 1, -1)\n",
    "            if self.debug > 4: print(\"alpha_i_input_a:\\t\", alpha_i_input_a.shape)\n",
    "\n",
    "            alpha_i_input = alpha_i_input_a.add_(self.alpha_i_b.view(-1,1,1).repeat(1,1,self.ques_len))\n",
    "            if self.debug > 4: print(\"alpha_i_input:\\t\\t\", alpha_i_input.shape)\n",
    "\n",
    "            # Softmax over alpha inputs\n",
    "            alpha_i = F.softmax(alpha_i_input, dim=-1)\n",
    "            if self.debug > 4: print(\"alpha_i:\\t\\t\", alpha_i.shape)\n",
    "\n",
    "            # Weighted summary of question with alpha    \n",
    "            z_i_input_b = (\n",
    "                            H_q.view(batch_size, self.ques_len, -1) *\n",
    "                           (alpha_i.view(batch_size, self.ques_len, -1).repeat(1, 1, self.hidden_dim))\n",
    "                          ).view(self.ques_len,batch_size, -1)\n",
    "            if self.debug > 4: print(\"z_i_input_b:\\t\\t\", z_i_input_b.shape)\n",
    "\n",
    "            z_i = torch.cat((H_p[i].view(1, batch_size, -1), z_i_input_b), dim=0)\n",
    "            if self.debug > 4: print(\"z_i:\\t\\t\\t\", z_i.shape)\n",
    "\n",
    "            # Pass z_i, h_ri to the LSTM \n",
    "            lstm_input = torch.cat((z_i.view(1, batch_size,-1), H_r[i].view(1, batch_size, -1)), dim=2)\n",
    "            if self.debug > 4: print(\"lstm_input:\\t\\t\", lstm_input.shape)\n",
    "\n",
    "            # Take input from LSTM, concat in H_r and nullify the temp var.\n",
    "            h_ri, hidden = self.lstm_summary(lstm_input, hidden)\n",
    "            H_r = torch.cat((H_r, h_ri), dim=0)\n",
    "            h_ri = None\n",
    "            \n",
    "            if self.debug > 4:\n",
    "                print(\"\\tH_r:\\t\\t\\t\", H_r.shape)\n",
    "#                 print(\"hidden new:\\t\\t\", hidden[0].shape, hidden[1].shape)\n",
    "\n",
    "        return H_r[1:]\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros((1, batch_size, self.hidden_dim), device=device),\n",
    "                torch.zeros((1, batch_size, self.hidden_dim), device=device))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN)\n",
    "#     h_pi = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "#     h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "#     hidden = model.init_hidden()\n",
    "#     H_q = torch.randn(QUES_LEN, BATCH_SIZE, HIDDEN_DIM)\n",
    "    \n",
    "#     op, hid = model(h_pi, h_ri, H_q, hidden)\n",
    "    \n",
    "#     print(\"\\nDone:op\", op.shape)\n",
    "#     print(\"Done:hid\", hid[0].shape, hid[1].shape)\n",
    "\n",
    "if DEBUG > 2:\n",
    "    with torch.no_grad():\n",
    "        matchLSTMEncoder = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN).cuda(device)\n",
    "        hidden = matchLSTMEncoder.init_hidden()\n",
    "        para_embedded = torch.rand((PARA_LEN, BATCH_SIZE, HIDDEN_DIM), device=device)\n",
    "        ques_embedded = torch.rand((QUES_LEN, BATCH_SIZE, HIDDEN_DIM), device=device)\n",
    "        h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "    #     if DEBUG:\n",
    "    #         print (\"init h_ri shape is: \", h_ri.shape)\n",
    "    #         print (\"the para length is \", len(para_embedded))\n",
    "        H_r = matchLSTMEncoder(para_embedded.view(-1,BATCH_SIZE,HIDDEN_DIM),\n",
    "                               h_ri, \n",
    "                               ques_embedded, \n",
    "                               hidden)\n",
    "        print(\"H_r: \", H_r.shape)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network\n",
    "\n",
    "Using a ptrnet over $H_r$ to unfold and get most probable spans.\n",
    "We use the **boundry model** to do that (predict start and end of seq).\n",
    "\n",
    "A simple energy -> softmax -> decoder. Where softmaxed energy is supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PointerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, macros):\n",
    "        super(PointerDecoder, self).__init__()\n",
    "        \n",
    "        # Keep args\n",
    "        self.hidden_dim = macros['hidden_dim']\n",
    "        self.batch_size = macros['batch_size']\n",
    "        self.para_len = macros['para_len']\n",
    "        self.debug = macros['debug']\n",
    "        \n",
    "        self.lin_f_repeat = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.lin_f_nobias = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        \n",
    "        self.beta_k_w = nn.Parameter(torch.FloatTensor(self.hidden_dim, 1))\n",
    "        self.beta_k_b = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.hidden_dim*(PARA_LEN+1), self.hidden_dim)\n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros((1, batch_size, self.hidden_dim), device=device),\n",
    "                torch.zeros((1, batch_size, self.hidden_dim), device=device))\n",
    "    \n",
    "    def forward(self, h_ak, H_r, hidden):\n",
    "        \n",
    "        # h_ak (current decoder's last op) (1,batch,hiddendim)\n",
    "        # H_r (weighted summary of para) (P, batch, hiddendim)\n",
    "        batch_size = H_r.shape[1]\n",
    "        \n",
    "        if self.debug > 4:\n",
    "            print(\"h_ak:\\t\\t\\t\", h_ak.shape)\n",
    "            print(\"H_r:\\t\\t\\t\", H_r.shape)\n",
    "            print(\"hidden:\\t\\t\\t\", hidden[0].shape, hidden[1].shape)\n",
    "            \n",
    "        # Prepare inputs for the tanh used to compute energy\n",
    "        f_input_b = self.lin_f_repeat(h_ak)\n",
    "        if self.debug > 4: print(\"f_input_b unrepeated:  \", f_input_b.shape)\n",
    "        \n",
    "        #H_r shape is ([PARA_LEN, BATCHSIZE, EmbeddingDIM])\n",
    "        f_input_b = f_input_b.repeat(H_r.shape[0], 1, 1)\n",
    "        if self.debug > 4: print(\"f_input_b repeated:\\t\", f_input_b.shape)\n",
    "            \n",
    "        f_input_a = self.lin_f_nobias(H_r)\n",
    "        if self.debug > 4: print(\"f_input_a:\\t\\t\", f_input_a.shape)\n",
    "            \n",
    "        # Send it off to tanh now\n",
    "        F_k = F.tanh(f_input_a+f_input_b)\n",
    "        if self.debug > 4: print(\"F_k:\\t\\t\\t\", F_k.shape) #PARA_LEN,BATCHSIZE,EmbeddingDim\n",
    "            \n",
    "        # Attention weights\n",
    "        beta_k_input_a = F_k.view(batch_size, -1, self.hidden_dim).matmul(self.beta_k_w).view(batch_size, 1, -1)\n",
    "        if self.debug > 4: print(\"beta_k_input_a:\\t\\t\", beta_k_input_a.shape)\n",
    "            \n",
    "        beta_k_input = beta_k_input_a.add_(self.beta_k_b.repeat(1,1,self.para_len))\n",
    "        if self.debug > 4: print(\"beta_k_input:\\t\\t\", beta_k_input.shape)\n",
    "            \n",
    "        beta_k = F.softmax(beta_k_input, dim=-1)\n",
    "        if self.debug > 4: print(\"beta_k:\\t\\t\\t\", beta_k.shape)\n",
    "            \n",
    "        lstm_input_a = H_r.view(batch_size, self.para_len, -1) * (beta_k.view(batch_size, self.para_len, -1).repeat(1,1,self.hidden_dim))\n",
    "        if self.debug > 4: print(\"lstm_input_a:\\t\\t\", lstm_input_a.shape)\n",
    "            \n",
    "        lstm_input = torch.cat((lstm_input_a.view(1, batch_size,-1), h_ak.view(1, batch_size, -1)), dim=2)\n",
    "        if self.debug > 4: print(\"lstm_input:\\t\\t\", lstm_input.shape)\n",
    "        \n",
    "        h_ak, hidden = self.lstm(lstm_input, hidden)\n",
    "        \n",
    "        return h_ak, hidden, beta_k\n",
    "            \n",
    "if DEBUG > 2:\n",
    "    with torch.no_grad():\n",
    "        pointerDecoder = PointerDecoder(HIDDEN_DIM).cuda(device)\n",
    "        h_ak = torch.randn(1,BATCH_SIZE,HIDDEN_DIM, device=device)\n",
    "    #     H_r = torch.randn(PARA_LEN, BATCH_SIZE, HIDDEN_DIM)\n",
    "        pointerHidden = pointerDecoder.init_hidden()\n",
    "        h_ak, hidden, beta_k = pointerDecoder(h_ak, para_embedded, hidden)\n",
    "        print (beta_k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull the real data from disk.\n",
    "\n",
    "Files stored in `./data/squad/train.ids.*`\n",
    "Pull both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_loc, macros):\n",
    "    \"\"\"\n",
    "        Given the dataloc and the data available in a specific format, it would pick the data up, and make trainable matrices,\n",
    "        Harvest train_P, train_Q, train_Y, test_P, test_Q, test_Y matrices in this format\n",
    "        \n",
    "        **return_type**: np matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpacking macros\n",
    "    PARA_LEN = macros['para_len']\n",
    "    QUES_LEN = macros['ques_len']\n",
    "    \n",
    "    train_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.ids.question')))])\n",
    "    train_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.ids.context')))])\n",
    "    train_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.span')))])\n",
    "\n",
    "    test_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.ids.question')))])\n",
    "    test_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.ids.context')))])\n",
    "    test_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.span')))])\n",
    "\n",
    "    print(\"Train Q: \", train_q.shape)\n",
    "    print(\"Train P: \", train_p.shape)\n",
    "    print(\"Train Y: \", train_y.shape)\n",
    "    print(\"Test Q: \", test_q.shape)\n",
    "    print(\"Test P: \", test_p.shape)\n",
    "    print(\"Test Y: \", test_y.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "        Parse the semi-raw data:\n",
    "            - shuffle\n",
    "            - pad, prepare\n",
    "            - dump useless vars\n",
    "    \"\"\"\n",
    "    # Shuffle data\n",
    "    index_train, index_test = np.arange(len(train_p)), np.arange(len(test_p))\n",
    "    np.random.shuffle(index_train)\n",
    "    np.random.shuffle(index_test)\n",
    "\n",
    "    train_p, train_q, train_y = train_p[index_train], train_q[index_train], train_y[index_train]\n",
    "    test_p, test_q, test_y = test_p[index_test], test_q[index_test], test_y[index_test]\n",
    "\n",
    "#     sanity_check(train_p, train_y)\n",
    "    \n",
    "    # Pad and prepare\n",
    "    train_P = np.zeros((len(train_p), PARA_LEN))\n",
    "    train_Q = np.zeros((len(train_q), QUES_LEN))\n",
    "    train_Y_start = np.zeros((len(train_p), PARA_LEN))\n",
    "    train_Y_end = np.zeros((len(train_p), PARA_LEN))\n",
    "\n",
    "    test_P = np.zeros((len(test_p), PARA_LEN))\n",
    "    test_Q = np.zeros((len(test_q), QUES_LEN))\n",
    "    test_Y_start = np.zeros((len(test_p), PARA_LEN))\n",
    "    test_Y_end = np.zeros((len(test_p), PARA_LEN))\n",
    "\n",
    "    crop_train = []    # Remove these rows from training\n",
    "    for i in range(len(train_p)):\n",
    "        p = train_p[i]\n",
    "        q = train_q[i]\n",
    "        y = train_y[i]\n",
    "\n",
    "        # First see if you can keep this example or not (due to size)\n",
    "        if y[0] > PARA_LEN or y[1] > PARA_LEN:\n",
    "            crop.append(i)\n",
    "            continue\n",
    "\n",
    "\n",
    "        train_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "        train_Q[i, :min(QUES_LEN, len(q))] = p[:min(QUES_LEN, len(q))]\n",
    "        train_Y_start[i, y[0]] = 1\n",
    "        train_Y_end[i, y[1]] = 1\n",
    "\n",
    "    crop_test = []\n",
    "    for i in range(len(test_p)):\n",
    "        p = test_p[i]\n",
    "        q = test_q[i]\n",
    "        y = test_y[i]\n",
    "\n",
    "        # First see if you can keep this example or not (due to size)\n",
    "        if y[0] > PARA_LEN or y[1] > PARA_LEN:\n",
    "            crop.append(i)\n",
    "            continue\n",
    "\n",
    "        test_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "        test_Q[i, :min(QUES_LEN, len(q))] = p[:min(QUES_LEN, len(q))]\n",
    "        test_Y_start[i, y[0]] = 1\n",
    "        test_Y_end[i, y[1]] = 1\n",
    "\n",
    "\n",
    "    # Let's free up some memory now\n",
    "    train_p, train_q, train_y, test_p, test_q, test_y = None, None, None, None, None, None\n",
    "    \n",
    "    return train_P, train_Q, train_Y_start, train_Y_end, test_P, test_Q, test_Y_start, test_Y_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, and running the model\n",
    "- Write a train fn\n",
    "- Write a training loop invoking it\n",
    "- Fill in real data\n",
    "\n",
    "----------\n",
    "\n",
    "Feats:\n",
    "- Function to test every n epochs.\n",
    "- Report train accuracy every epoch\n",
    "- Store the train, test accuracy for every instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(para_batch,\n",
    "          ques_batch,\n",
    "          answer_start_batch,\n",
    "          answer_end_batch,\n",
    "          ques_model,\n",
    "          para_model,\n",
    "          match_LSTM_encoder_model,\n",
    "          pointer_decoder_model,\n",
    "          optimizer, \n",
    "          loss_fn,\n",
    "          macros,\n",
    "          debug=2):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    :param para_batch: paragraphs (batch, max_seq_len_para) \n",
    "    :param ques_batch: questions corresponding to para (batch, max_seq_len_ques)\n",
    "    :param answer_start_batch: one-hot vector denoting pos of span start (batch, max_seq_len_para)\n",
    "    :param answer_end_batch: one-hot vector denoting pos of span end (batch, max_seq_len_para)\n",
    "    \n",
    "    # Models\n",
    "    :param ques_model: model to encode ques\n",
    "    :param para_model: model to encode para\n",
    "    :param match_LSTM_encoder_model: model to match para, ques to get para summary\n",
    "    :param pointer_decoder_model: model to get a pointer over start and end span pointer\n",
    "    \n",
    "    # Loss and Optimizer.\n",
    "    :param loss_fn: \n",
    "    :param optimizer: \n",
    "    \n",
    "    :return: \n",
    "    \n",
    "    \n",
    "    NOTE: When using MSE, \n",
    "        - target labels are one-hot\n",
    "        - target label is float tensor\n",
    "        - shape (batch, 1, len)\n",
    "        \n",
    "        When using CrossEntropy\n",
    "        - target is not onehot\n",
    "        - long\n",
    "        - shape (batch, )\n",
    "    \"\"\"\n",
    "    \n",
    "#     DEBUG = debug\n",
    "#     BATCH_SIZE = macros['batch_size']\n",
    "#     HIDDEN_DIM = macros['hidden_dim']\n",
    "    \n",
    "    if debug >=2: \n",
    "        print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "        print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "        print(\"\\tanswer_start_batch:\\t\", answer_start_batch.shape)\n",
    "        print(\"\\tanswer_end_batch:\\t\\t\", answer_end_batch.shape)\n",
    "    \n",
    "    # Wiping all gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Initializing all hidden states.\n",
    "    hidden_quesenc = ques_model.init_hidden(macros['batch_size'])\n",
    "    hidden_paraenc = para_model.init_hidden(macros['batch_size'])\n",
    "    hidden_mlstm = match_LSTM_encoder_model.init_hidden(macros['batch_size'])\n",
    "    hidden_ptrnet = pointer_decoder_model.init_hidden(macros['batch_size'])\n",
    "    h_ri = torch.zeros((1, macros['batch_size'], macros['hidden_dim']), dtype=torch.float, device=device)\n",
    "    h_ak = torch.zeros((1, macros['batch_size'], macros['hidden_dim']), dtype=torch.float, device=device)\n",
    "    if debug >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "    \n",
    "    #passing the data through LSTM pre-processing layer\n",
    "    H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc)\n",
    "    H_p, para_model_hidden = para_model(para_batch, hidden_paraenc)\n",
    "    if debug >= 2: \n",
    "        print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "        print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "        print(\"\\tH_ri:\\t\\t\", h_ri.shape)\n",
    "#         raw_input(\"Check memory and ye shall continue\")\n",
    "        print(\"------------Encoded hidden states------------\")\n",
    "    \n",
    "    H_r = match_LSTM_encoder_model(H_p.view(-1, macros['batch_size'], macros['hidden_dim']), h_ri, H_q, hidden_mlstm)\n",
    "    if debug >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "    \n",
    "    #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "    h_ak, hidden_ptrnet, beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet)\n",
    "    h_ak, hidden_ptrnet, beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet)\n",
    "    if debug >= 2: print(\"------------Passed through pointernet------------\")\n",
    "\n",
    "        \n",
    "    # For crossentropy\n",
    "#     _, answer_start_batch = answer_start_batch.max(dim=1)\n",
    "#     _, answer_end_batch = answer_end_batch.max(dim=1)\n",
    "#     print(\"labels: \", answer_start_batch.shape)\n",
    "    \n",
    "    \n",
    "    #How will we manage batches for loss.\n",
    "    loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "    loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "    if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "    \n",
    "    loss.backward()\n",
    "    if debug >= 2: print(\"------------Calculated Gradients------------\")\n",
    "    \n",
    "    #optimization step\n",
    "    optimizer.step()\n",
    "    if debug >= 2: print(\"------------Updated weights.------------\")\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function (no grad, no eval)\n",
    "def predict(para_batch,\n",
    "            ques_batch,\n",
    "            ques_model,\n",
    "            para_model,\n",
    "            match_LSTM_encoder_model,\n",
    "            pointer_decoder_model,\n",
    "            macros,\n",
    "            debug):\n",
    "    \"\"\"\n",
    "        Function which returns the model's output based on a given set of P&Q's. \n",
    "        Does not convert to strings, gives the direct model output.\n",
    "        \n",
    "        Expects:\n",
    "            four models\n",
    "            data\n",
    "            misc macros\n",
    "    \"\"\"\n",
    "    \n",
    "    BATCH_SIZE = macros['batch_size']\n",
    "    HIDDEN_DIM = macros['hidden_dim']\n",
    "    DEBUG = debug\n",
    "    \n",
    "    if debug >=2: \n",
    "        print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "        print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "        \n",
    "    with torch.no_grad():    \n",
    "\n",
    "        # Initializing all hidden states.\n",
    "        hidden_quesenc = ques_model.init_hidden(BATCH_SIZE)\n",
    "        hidden_paraenc = para_model.init_hidden(BATCH_SIZE)\n",
    "        hidden_mlstm = match_LSTM_encoder_model.init_hidden(BATCH_SIZE)\n",
    "        hidden_ptrnet = pointer_decoder_model.init_hidden(BATCH_SIZE)\n",
    "        h_ri = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        h_ak = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        if DEBUG >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "            \n",
    "        #passing the data through LSTM pre-processing layer\n",
    "        H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc)\n",
    "        H_p, para_model_hidden = para_model(para_batch, hidden_paraenc)\n",
    "        if DEBUG >= 2: \n",
    "            print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "            print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "            print(\"\\tH_ri:\\t\\t\", h_ri.shape)\n",
    "#             raw_input(\"Check memory and ye shall continue\")\n",
    "            print(\"------------Encoded hidden states------------\")\n",
    "\n",
    "        H_r = match_LSTM_encoder_model(H_p.view(-1, BATCH_SIZE, HIDDEN_DIM), h_ri, H_q, hidden_mlstm)\n",
    "        if DEBUG >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "\n",
    "        #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "        _, _, beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet)\n",
    "        _, _, beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet)\n",
    "        if DEBUG >= 2: print(\"------------Passed through pointernet------------\")\n",
    "                            \n",
    "        # For crossentropy\n",
    "#         _, answer_start_batch = answer_start_batch.max(dim=1)\n",
    "#         _, answer_end_batch = answer_end_batch.max(dim=1)\n",
    "#         print(\"labels: \", answer_start_batch.shape)\n",
    "            \n",
    "        #How will we manage batches for loss.\n",
    "        loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "        loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "        if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "            \n",
    "        return (beta_k_start, beta_k_end, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval function (no grad no eval no nothing)\n",
    "def eval(y_cap, y, metrics={'em':None}):\n",
    "    \"\"\" \n",
    "        Returns the exact-match (em) metric by default.\n",
    "        Can specifiy more in a list (TODO)\n",
    "        \n",
    "        Inputs:\n",
    "        - y_cap: list of two tensors (start, end) of dim [BATCH_SIZE, PARA_LEN] each\n",
    "        - y: list of two tensors (start, end) of dim [BATCH_SIZE, 1] each\n",
    "    \"\"\"\n",
    "    y_cap_max_start, y_cap_max_end = torch.argmax(y_cap[0], dim=1).float(), \\\n",
    "                                     torch.argmax(y_cap[1], dim=1).float()\n",
    "    \n",
    "    if \"em\" in metrics.keys():\n",
    "        metrics['em'] = (y[0].eq(y_cap_max_start) & y[1].eq(y_cap_max_end)).sum().item()/ float(y[0].shape[0])\n",
    "        \n",
    "    if DEBUG >= 2: \n",
    "        print(\"Test performance: \", metrics)\n",
    "        print(\"------------Evaluated------------\")\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "if DEBUG >= 5:\n",
    "    # Testing this function\n",
    "    metrics = {'em':None}\n",
    "    y = torch.randint(0, PARA_LEN, (BATCH_SIZE,)).float(), torch.randint(0, PARA_LEN, (BATCH_SIZE,)).float()\n",
    "    y_cap = torch.rand((BATCH_SIZE, PARA_LEN)), torch.rand((BATCH_SIZE, PARA_LEN))\n",
    "    print(eval(y_cap, y))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(_models, _data, _macros, _epochs=EPOCHS, _save_best=False, _test_every=0, _debug=2):\n",
    "    \"\"\"\n",
    "        > Instantiate models\n",
    "        > Instantiate loss, optimizer\n",
    "        > Instantiate ways to store loss\n",
    "\n",
    "        > Per epoch\n",
    "            > sample batch and give to train fn\n",
    "            > get loss\n",
    "            > if epoch %k ==0: get test accuracy\n",
    "\n",
    "        > have fn to calculate test accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack data\n",
    "    DEBUG = _debug\n",
    "    train_P = _data['train']['P']\n",
    "    train_Q = _data['train']['Q']\n",
    "    train_Y_start = _data['train']['Ys']\n",
    "    train_Y_end = _data['train']['Ye']\n",
    "    test_P = _data['test']['P']\n",
    "    test_Q = _data['test']['Q']\n",
    "    test_Y_start = _data['test']['Ys']\n",
    "    test_Y_end = _data['test']['Ye']\n",
    "                                 \n",
    "    ques_model, para_model, match_LSTM_encoder_model, pointer_decoder_model = _models\n",
    "    _data = None\n",
    "\n",
    "    # Instantiate Loss\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adamax(list(filter(lambda p: p.requires_grad, ques_model.parameters())) + \n",
    "                             list(filter(lambda p: p.requires_grad, para_model.parameters())) + \n",
    "                             list(match_LSTM_encoder_model.parameters()) + \n",
    "                             list(pointer_decoder_model.parameters()))\n",
    "\n",
    "    # Losses\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_em = []\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(_epochs):\n",
    "        print(\"Epoch: \", epoch, \"/\", _epochs)\n",
    "\n",
    "        epoch_loss = []\n",
    "        epoch_time = time.time()\n",
    "\n",
    "        for iter in range(int(len(train_P)/BATCH_SIZE)):\n",
    "#         for iter in range(4):\n",
    "\n",
    "            batch_time = time.time()\n",
    "\n",
    "            # Sample batch and train on it\n",
    "            sample_index = np.random.randint(0, len(train_P), _macros['batch_size'])\n",
    "\n",
    "            loss = train(\n",
    "                para_batch = torch.tensor(train_P[sample_index], dtype=torch.long, device=device),\n",
    "                ques_batch = torch.tensor(train_Q[sample_index], dtype=torch.long, device=device),\n",
    "                answer_start_batch = torch.tensor(train_Y_start[sample_index], dtype=torch.float, device=device).view( _macros['batch_size'], 1, _macros['para_len']),\n",
    "                answer_end_batch = torch.tensor(train_Y_end[sample_index], dtype=torch.float, device=device).view(_macros['batch_size'], 1, _macros['para_len']),\n",
    "                ques_model = ques_model,\n",
    "                para_model = para_model,\n",
    "                match_LSTM_encoder_model = match_LSTM_encoder_model,\n",
    "                pointer_decoder_model = pointer_decoder_model,\n",
    "                optimizer = optimizer, \n",
    "                loss_fn= loss_fn,\n",
    "                macros=_macros,\n",
    "                debug=_macros['debug']\n",
    "            )\n",
    "            \n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            print(\"Batch:\\t%d\" % iter,\"/%d\\t: \" % (len(train_P)/_macros['batch_size']),\n",
    "                  \"%s\" % (time.time() - batch_time), \n",
    "                  \"\\t%s\" % (time.time() - epoch_time), \n",
    "                  \"\\tloss:%f\" % loss.item())\n",
    "#                   end=None if iter+1 == 4 else \"\\r\")\n",
    "#                   end=None if iter+1 == int(len(train_P)/BATCH_SIZE) else \"\\r\")\n",
    "                 \n",
    "#         print(\"Time taken in epoch: %s\" % (time.time() - epoch_time))\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        if _test_every and epoch % _test_every == 0:\n",
    "            \n",
    "            \n",
    "            y_cap_start, y_cap_end, test_loss = predict(\n",
    "                para_batch = torch.tensor(test_P, dtype=torch.long, device=device),\n",
    "                ques_batch = torch.tensor(test_Q, dtype=torch.long, device=device),\n",
    "                ques_model = ques_model,\n",
    "                para_model = para_model,\n",
    "                match_LSTM_encoder_model = match_LSTM_encoder_model,\n",
    "                pointer_decoder_model = pointer_decoder_model,\n",
    "                macros = _macros,\n",
    "                debug = _macros['debug']\n",
    "            )\n",
    "            metrics = eval(y=(torch.tensor(test_Y_start, dtype=torch.long, device=device).view( -1, _macros['para_len']),\n",
    "                         torch.tensor(test_Y_end, dtype=torch.long, device=device).view(-1, _macros['para_len'])),\n",
    "                      y_cap=[y_cap_start, y_cap_end])\n",
    "            \n",
    "            test_losses.append(test_loss)\n",
    "            test_em.append(metrics['em'])\n",
    "            \n",
    "        \n",
    "    return train_losses, test_losses, test_em\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "    Saving said models.\n",
    "    TODO\n",
    "\"\"\"\n",
    "# ques_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(loss, _label=\"Some label\", _only_epoch=True):\n",
    "    \"\"\"\n",
    "        Fn to visualize loss.\n",
    "        Expects either\n",
    "            - [int, int] for epoch level stuff\n",
    "            - [ [int, int], [int, int] ] for batch level data. \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = [15, 8] \n",
    "    \n",
    "    # Detect input format\n",
    "    if type(loss[0]) == int:\n",
    "        \n",
    "        plt.plot(loss)\n",
    "        plt.ylabel(_label)\n",
    "        plt.show()\n",
    "        \n",
    "    elif type(loss[0]) == list:\n",
    "        \n",
    "        if _only_epoch:\n",
    "            loss = [ sum(x) for x in loss ]\n",
    "            \n",
    "        else:\n",
    "            loss = [ y for x in loss for y in x ]\n",
    "            \n",
    "        plt.plot(loss)\n",
    "        plt.ylabel(_label)\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator\n",
    "\n",
    "One cell which instantiates and runs everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Q:  (81403,)\n",
      "Train P:  (81403,)\n",
      "Train Y:  (81403, 2)\n",
      "Test Q:  (4285,)\n",
      "Test P:  (4285,)\n",
      "Test Y:  (4285, 2)\n",
      "Epoch:  0 / 1\n",
      "Batch:\t0 /1004\t:  14.0413222313 \t14.0414600372 \tloss:0.005163\n",
      "Batch:\t1 /1004\t:  9.0444560051 \t23.0869269371 \tloss:0.005034\n",
      "Batch:\t2 /1004\t:  6.06823992729 \t29.155602932 \tloss:0.005131\n",
      "Batch:\t3 /1004\t:  6.33431410789 \t35.4911708832 \tloss:0.005163\n",
      "Batch:\t4 /1004\t:  6.00137805939 \t41.493792057 \tloss:0.005163\n",
      "Batch:\t5 /1004\t:  6.22808694839 \t47.7224519253 \tloss:0.005163\n",
      "Batch:\t6 /1004\t:  6.36011505127 \t54.0839819908 \tloss:0.005195\n",
      "Batch:\t7 /1004\t:  6.34052300453 \t60.4247879982 \tloss:0.005195\n",
      "Batch:\t8 /1004\t:  6.04881191254 \t66.4752149582 \tloss:0.005163\n",
      "Batch:\t9 /1004\t:  6.19745612144 \t72.6738300323 \tloss:0.005195\n",
      "Batch:\t10 /1004\t:  6.2030351162 \t78.8786859512 \tloss:0.005195\n",
      "Batch:\t11 /1004\t:  6.26238012314 \t85.1426079273 \tloss:0.005195\n",
      "Batch:\t12 /1004\t:  6.41577196121 \t91.5586099625 \tloss:0.005195\n",
      "Batch:\t13 /1004\t:  6.13599395752 \t97.6958420277 \tloss:0.005195\n",
      "Batch:\t14 /1004\t:  6.30410695076 \t104.000405073 \tloss:0.005195\n",
      "Batch:\t15 /1004\t:  6.05463600159 \t110.05637908 \tloss:0.005163\n",
      "Batch:\t16 /1004\t:  5.89693117142 \t115.954093933 \tloss:0.005163\n",
      "Batch:\t17 /1004\t:  6.13389277458 \t122.088557959 \tloss:0.005195\n",
      "Batch:\t18 /1004\t:  6.09325313568 \t128.182852983 \tloss:0.005195\n",
      "Batch:\t19 /1004\t:  6.11073184013 \t134.294905901 \tloss:0.005163\n",
      "Batch:\t20 /1004\t:  6.00217700005 \t140.29843688 \tloss:0.005195\n",
      "Batch:\t21 /1004\t:  6.10328888893 \t146.402332067 \tloss:0.005195\n",
      "Batch:\t22 /1004\t:  6.31764602661 \t152.721237898 \tloss:0.005099\n",
      "Batch:\t23 /1004\t:  6.06320381165 \t158.785738945 \tloss:0.005131\n",
      "Batch:\t24 /1004\t:  6.20200204849 \t164.98874402 \tloss:0.005163\n",
      "Batch:\t25 /1004\t:  5.95005702972 \t170.939862013 \tloss:0.005195\n",
      "Batch:\t26 /1004\t:  6.44296598434 \t177.383496046 \tloss:0.005195\n",
      "Batch:\t27 /1004\t:  6.07518196106 \t183.460445881 \tloss:0.005163\n",
      "Batch:\t28 /1004\t:  6.05665206909 \t189.518306017 \tloss:0.005195\n",
      "Batch:\t29 /1004\t:  6.16830801964 \t195.687707901 \tloss:0.005195\n",
      "Batch:\t30 /1004\t:  6.10660815239 \t201.796143055 \tloss:0.005163\n",
      "Batch:\t31 /1004\t:  6.34959197044 \t208.145932913 \tloss:0.005195\n",
      "Batch:\t32 /1004\t:  6.41994690895 \t214.566293955 \tloss:0.005195\n",
      "Batch:\t33 /1004\t:  6.08732295036 \t220.655103922 \tloss:0.005163\n",
      "Batch:\t34 /1004\t:  6.43881797791 \t227.095371008 \tloss:0.005163\n",
      "Batch:\t35 /1004\t:  6.01978492737 \t233.116936922 \tloss:0.005163\n",
      "Batch:\t36 /1004\t:  6.04197692871 \t239.159732103 \tloss:0.005131\n",
      "Batch:\t37 /1004\t:  6.01309108734 \t245.174813986 \tloss:0.005195\n",
      "Batch:\t38 /1004\t:  6.36959791183 \t251.544948101 \tloss:0.005195\n",
      "Batch:\t39 /1004\t:  6.49956488609 \t258.046174049 \tloss:0.005163\n",
      "Batch:\t40 /1004\t:  6.02470707893 \t264.071105003 \tloss:0.005195\n",
      "Batch:\t41 /1004\t:  6.37476110458 \t270.447408915 \tloss:0.005131\n",
      "Batch:\t42 /1004\t:  6.15922188759 \t276.607960939 \tloss:0.005195\n",
      "Batch:\t43 /1004\t:  6.44145202637 \t283.050297022 \tloss:0.005163\n",
      "Batch:\t44 /1004\t:  5.97024202347 \t289.021826029 \tloss:0.005163\n",
      "Batch:\t45 /1004\t:  6.07788515091 \t295.100826979 \tloss:0.005195\n",
      "Batch:\t46 /1004\t:  6.20778918266 \t301.309241056 \tloss:0.005195\n",
      "Batch:\t47 /1004\t:  6.23726892471 \t307.546957016 \tloss:0.005163\n",
      "Batch:\t48 /1004\t:  6.07718205452 \t313.625592947 \tloss:0.005163\n",
      "Batch:\t49 /1004\t:  6.09116601944 \t319.718713045 \tloss:0.005195\n",
      "Batch:\t50 /1004\t:  6.18680906296 \t325.906198978 \tloss:0.005195\n",
      "Batch:\t51 /1004\t:  6.20587396622 \t332.113610029 \tloss:0.005163\n",
      "Batch:\t52 /1004\t:  6.05210399628 \t338.167054892 \tloss:0.005195\n",
      "Batch:\t53 /1004\t:  6.20394897461 \t344.372041941 \tloss:0.005195\n",
      "Batch:\t54 /1004\t:  6.15068793297 \t350.52391696 \tloss:0.005195\n",
      "Batch:\t55 /1004\t:  6.4250729084 \t356.950211048 \tloss:0.005195\n",
      "Batch:\t56 /1004\t:  6.04911184311 \t363.000943899 \tloss:0.005195\n",
      "Batch:\t57 /1004\t:  6.02529191971 \t369.028891087 \tloss:0.005195\n",
      "Batch:\t58 /1004\t:  6.3117108345 \t375.341723919 \tloss:0.005195\n",
      "Batch:\t59 /1004\t:  5.96352005005 \t381.306539059 \tloss:0.005195\n",
      "Batch:\t60 /1004\t:  6.17766594887 \t387.487485886 \tloss:0.005195\n",
      "Batch:\t61 /1004\t:  6.25101494789 \t393.739612103 \tloss:0.005195\n",
      "Batch:\t62 /1004\t:  6.03276705742 \t399.772646904 \tloss:0.005131\n",
      "Batch:\t63 /1004\t:  6.12148809433 \t405.894443989 \tloss:0.005195\n",
      "Batch:\t64 /1004\t:  6.00839996338 \t411.904823065 \tloss:0.005163\n",
      "Batch:\t65 /1004\t:  6.11277508736 \t418.01828289 \tloss:0.005131\n",
      "Batch:\t66 /1004\t:  6.19152498245 \t424.211797953 \tloss:0.005195\n",
      "Batch:\t67 /1004\t:  5.85469913483 \t430.066783905 \tloss:0.005195\n",
      "Batch:\t68 /1004\t:  6.42220616341 \t436.490258932 \tloss:0.005195\n",
      "Batch:\t69 /1004\t:  6.67217707634 \t443.16306591 \tloss:0.005195\n",
      "Batch:\t70 /1004\t:  6.16074705124 \t449.324253082 \tloss:0.005131\n",
      "Batch:\t71 /1004\t:  5.99530601501 \t455.320230007 \tloss:0.005195\n",
      "Batch:\t72 /1004\t:  6.01878499985 \t461.33996892 \tloss:0.005195\n",
      "Batch:\t73 /1004\t:  6.05174422264 \t467.393305063 \tloss:0.005067\n",
      "Batch:\t74 /1004\t:  5.93470191956 \t473.328224897 \tloss:0.005163\n",
      "Batch:\t75 /1004\t:  6.28534793854 \t479.614191055 \tloss:0.005195\n",
      "Batch:\t76 /1004\t:  6.24526000023 \t485.860435963 \tloss:0.005163\n",
      "Batch:\t77 /1004\t:  6.07247591019 \t491.934856892 \tloss:0.005195\n",
      "Batch:\t78 /1004\t:  6.05414700508 \t497.989909887 \tloss:0.005195\n",
      "Batch:\t79 /1004\t:  6.0605931282 \t504.051009893 \tloss:0.005195\n",
      "Batch:\t80 /1004\t:  6.59973597527 \t510.651802063 \tloss:0.005195\n",
      "Batch:\t81 /1004\t:  6.1328599453 \t516.786202908 \tloss:0.005195\n",
      "Batch:\t82 /1004\t:  6.21746110916 \t523.004874945 \tloss:0.005163\n",
      "Batch:\t83 /1004\t:  6.01565909386 \t529.02106595 \tloss:0.005195\n",
      "Batch:\t84 /1004\t:  6.06261610985 \t535.084194899 \tloss:0.005163\n",
      "Batch:\t85 /1004\t:  6.39775109291 \t541.483176947 \tloss:0.005195\n",
      "Batch:\t86 /1004\t:  6.20531797409 \t547.690207958 \tloss:0.005195\n",
      "Batch:\t87 /1004\t:  6.21164107323 \t553.902863026 \tloss:0.005195\n",
      "Batch:\t88 /1004\t:  6.3658080101 \t560.269990921 \tloss:0.005195\n",
      "Batch:\t89 /1004\t:  6.04114508629 \t566.312449932 \tloss:0.005195\n",
      "Batch:\t90 /1004\t:  6.29701185226 \t572.610064983 \tloss:0.005195\n",
      "Batch:\t91 /1004\t:  6.0428750515 \t578.654174089 \tloss:0.005195\n",
      "Batch:\t92 /1004\t:  6.14198112488 \t584.798098087 \tloss:0.005195\n",
      "Batch:\t93 /1004\t:  6.27519392967 \t591.074615955 \tloss:0.005195\n",
      "Batch:\t94 /1004\t:  6.04476499557 \t597.121092081 \tloss:0.005195\n",
      "Batch:\t95 /1004\t:  6.50004220009 \t603.622442007 \tloss:0.005163\n",
      "Batch:\t96 /1004\t:  6.09985995293 \t609.723645926 \tloss:0.005163\n",
      "Batch:\t97 /1004\t:  6.09794092178 \t615.823422909 \tloss:0.005131\n",
      "Batch:\t98 /1004\t:  6.44337701797 \t622.268073082 \tloss:0.005131\n",
      "Batch:\t99 /1004\t:  6.05543518066 \t628.325311899 \tloss:0.005195\n",
      "Batch:\t100 /1004\t:  6.08112621307 \t634.407941103 \tloss:0.005163\n",
      "Batch:\t101 /1004\t:  6.06228804588 \t640.470854998 \tloss:0.005195\n",
      "Batch:\t102 /1004\t:  6.16158390045 \t646.634080887 \tloss:0.005099\n",
      "Batch:\t103 /1004\t:  6.07084298134 \t652.705493927 \tloss:0.005195\n",
      "Batch:\t104 /1004\t:  6.10250377655 \t658.809091091 \tloss:0.005195\n",
      "Batch:\t105 /1004\t:  6.40400600433 \t665.213538885 \tloss:0.005195\n",
      "Batch:\t106 /1004\t:  6.07741117477 \t671.29220295 \tloss:0.005195\n",
      "Batch:\t107 /1004\t:  6.05194807053 \t677.344779968 \tloss:0.005163\n",
      "Batch:\t108 /1004\t:  6.3881380558 \t683.73331809 \tloss:0.005163\n",
      "Batch:\t109 /1004\t:  6.28763079643 \t690.021878004 \tloss:0.005131\n",
      "Batch:\t110 /1004\t:  6.70965886116 \t696.732711077 \tloss:0.005195\n",
      "Batch:\t111 /1004\t:  6.17158293724 \t702.905731916 \tloss:0.005195\n",
      "Batch:\t112 /1004\t:  6.29448699951 \t709.201258898 \tloss:0.005195\n",
      "Batch:\t113 /1004\t:  6.07544994354 \t715.277390957 \tloss:0.005195\n",
      "Batch:\t114 /1004\t:  6.25823092461 \t721.536056995 \tloss:0.005131\n",
      "Batch:\t115 /1004\t:  6.14324688911 \t727.679759979 \tloss:0.005195\n",
      "Batch:\t116 /1004\t:  6.39418792725 \t734.075324059 \tloss:0.005195\n",
      "Batch:\t117 /1004\t:  6.02655911446 \t740.103244066 \tloss:0.005195\n",
      "Batch:\t118 /1004\t:  6.24092793465 \t746.344691992 \tloss:0.005195\n",
      "Batch:\t119 /1004\t:  6.20871281624 \t752.554631948 \tloss:0.005195\n",
      "Batch:\t120 /1004\t:  6.05578303337 \t758.611712933 \tloss:0.005163\n",
      "Batch:\t121 /1004\t:  6.08407402039 \t764.696841955 \tloss:0.005195\n",
      "Batch:\t122 /1004\t:  6.52738595009 \t771.225039005 \tloss:0.005163\n",
      "Batch:\t123 /1004\t:  6.0559489727 \t777.282286882 \tloss:0.005099\n",
      "Batch:\t124 /1004\t:  6.08328008652 \t783.366075993 \tloss:0.005195\n",
      "Batch:\t125 /1004\t:  6.12270116806 \t789.490061998 \tloss:0.005195\n",
      "Batch:\t126 /1004\t:  6.24363899231 \t795.73488903 \tloss:0.005195\n",
      "Batch:\t127 /1004\t:  6.2950720787 \t802.030483961 \tloss:0.005163\n",
      "Batch:\t128 /1004\t:  6.07961797714 \t808.111038923 \tloss:0.005195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t129 /1004\t:  6.08014798164 \t814.191941977 \tloss:0.005195\n",
      "Batch:\t130 /1004\t:  6.12432813644 \t820.316874981 \tloss:0.005195\n",
      "Batch:\t131 /1004\t:  6.21467995644 \t826.532151937 \tloss:0.005163\n",
      "Batch:\t132 /1004\t:  5.91621398926 \t832.448695898 \tloss:0.005195\n",
      "Batch:\t133 /1004\t:  6.31686210632 \t838.766077995 \tloss:0.005195\n",
      "Batch:\t134 /1004\t:  6.21613407135 \t844.983551025 \tloss:0.005195\n",
      "Batch:\t135 /1004\t:  6.07984399796 \t851.065192938 \tloss:0.005195\n",
      "Batch:\t136 /1004\t:  6.6127409935 \t857.679018021 \tloss:0.005163\n",
      "Batch:\t137 /1004\t:  6.13017892838 \t863.810738087 \tloss:0.005195\n",
      "Batch:\t138 /1004\t:  6.40625405312 \t870.217717886 \tloss:0.005195\n",
      "Batch:\t139 /1004\t:  6.27568817139 \t876.493800879 \tloss:0.005195\n",
      "Batch:\t140 /1004\t:  6.16348719597 \t882.659224987 \tloss:0.005195\n",
      "Batch:\t141 /1004\t:  6.25278496742 \t888.913482904 \tloss:0.005195\n",
      "Batch:\t142 /1004\t:  6.14163804054 \t895.056382895 \tloss:0.005163\n",
      "Batch:\t143 /1004\t:  6.10298991203 \t901.160661936 \tloss:0.005195\n",
      "Batch:\t144 /1004\t:  6.3518910408 \t907.513395071 \tloss:0.005195\n",
      "Batch:\t145 /1004\t:  6.28931093216 \t913.806545019 \tloss:0.005195\n",
      "Batch:\t146 /1004\t:  6.14752984047 \t919.955497026 \tloss:0.005195\n",
      "Batch:\t147 /1004\t:  6.42802715302 \t926.384799004 \tloss:0.005195\n",
      "Batch:\t148 /1004\t:  6.20543003082 \t932.590553999 \tloss:0.005195\n",
      "Batch:\t149 /1004\t:  6.14704680443 \t938.739531994 \tloss:0.005195\n",
      "Batch:\t150 /1004\t:  6.54739713669 \t945.288807869 \tloss:0.005195\n",
      "Batch:\t151 /1004\t:  6.44470286369 \t951.734834909 \tloss:0.005195\n",
      "Batch:\t152 /1004\t:  6.0866689682 \t957.822753906 \tloss:0.005195\n",
      "Batch:\t153 /1004\t:  6.15863394737 \t963.982460976 \tloss:0.005195\n",
      "Batch:\t154 /1004\t:  6.30990815163 \t970.295973063 \tloss:0.005131\n",
      "Batch:\t155 /1004\t:  6.40161204338 \t976.698529959 \tloss:0.005195\n",
      "Batch:\t156 /1004\t:  6.34404301643 \t983.04383707 \tloss:0.005195\n",
      "Batch:\t157 /1004\t:  6.3474240303 \t989.392560959 \tloss:0.005131\n",
      "Batch:\t158 /1004\t:  5.94472885132 \t995.33887887 \tloss:0.005163\n",
      "Batch:\t159 /1004\t:  6.12769794464 \t1001.46782708 \tloss:0.005195\n",
      "Batch:\t160 /1004\t:  6.22201299667 \t1007.69107795 \tloss:0.005195\n",
      "Batch:\t161 /1004\t:  6.00690102577 \t1013.69820094 \tloss:0.005195\n",
      "Batch:\t162 /1004\t:  6.28907608986 \t1019.98829103 \tloss:0.005195\n",
      "Batch:\t163 /1004\t:  6.21502113342 \t1026.20518899 \tloss:0.005195\n",
      "Batch:\t164 /1004\t:  6.36512899399 \t1032.57163596 \tloss:0.005195\n",
      "Batch:\t165 /1004\t:  6.21630477905 \t1038.78841996 \tloss:0.005195\n",
      "Batch:\t166 /1004\t:  6.43174505234 \t1045.22146893 \tloss:0.005195\n",
      "Batch:\t167 /1004\t:  5.97176098824 \t1051.19446397 \tloss:0.005195\n",
      "Batch:\t168 /1004\t:  6.38539409637 \t1057.58059001 \tloss:0.005099\n",
      "Batch:\t169 /1004\t:  6.18416118622 \t1063.76669097 \tloss:0.005195\n",
      "Batch:\t170 /1004\t:  6.2446501255 \t1070.01196098 \tloss:0.005195\n",
      "Batch:\t171 /1004\t:  6.39994382858 \t1076.41235805 \tloss:0.005195\n",
      "Batch:\t172 /1004\t:  6.22146391869 \t1082.63473892 \tloss:0.005195\n",
      "Batch:\t173 /1004\t:  6.00728988647 \t1088.6430769 \tloss:0.005195\n",
      "Batch:\t174 /1004\t:  6.4916779995 \t1095.1355691 \tloss:0.005195\n",
      "Batch:\t175 /1004\t:  6.4278151989 \t1101.56434608 \tloss:0.005195\n",
      "Batch:\t176 /1004\t:  6.15913701057 \t1107.72472906 \tloss:0.005163\n",
      "Batch:\t177 /1004\t:  5.97497010231 \t1113.70047593 \tloss:0.005195\n",
      "Batch:\t178 /1004\t:  6.07627797127 \t1119.77860904 \tloss:0.005195\n",
      "Batch:\t179 /1004\t:  6.321819067 \t1126.10153008 \tloss:0.005163\n",
      "Batch:\t180 /1004\t:  6.07518911362 \t1132.17750192 \tloss:0.005195\n",
      "Batch:\t181 /1004\t:  6.20795106888 \t1138.38589501 \tloss:0.005163\n",
      "Batch:\t182 /1004\t:  6.06490302086 \t1144.45129895 \tloss:0.005163\n",
      "Batch:\t183 /1004\t:  6.08061599731 \t1150.53325105 \tloss:0.005195\n",
      "Batch:\t184 /1004\t:  6.14834690094 \t1156.68231201 \tloss:0.005195\n",
      "Batch:\t185 /1004\t:  6.03494977951 \t1162.71828389 \tloss:0.005163\n",
      "Batch:\t186 /1004\t:  5.99638009071 \t1168.71598005 \tloss:0.005163\n",
      "Batch:\t187 /1004\t:  6.35716485977 \t1175.07369804 \tloss:0.005195\n",
      "Batch:\t188 /1004\t:  6.32129406929 \t1181.39534187 \tloss:0.005163\n",
      "Batch:\t189 /1004\t:  6.06659603119 \t1187.46330309 \tloss:0.005195\n",
      "Batch:\t190 /1004\t:  6.07509279251 \t1193.53902507 \tloss:0.005131\n",
      "Batch:\t191 /1004\t:  6.12463998795 \t1199.66544199 \tloss:0.005195\n",
      "Batch:\t192 /1004\t:  6.01878786087 \t1205.68552494 \tloss:0.005195\n",
      "Batch:\t193 /1004\t:  6.16975188255 \t1211.855757 \tloss:0.005195\n",
      "Batch:\t194 /1004\t:  6.1339609623 \t1217.9906559 \tloss:0.005163\n",
      "Batch:\t195 /1004\t:  6.27783894539 \t1224.26964593 \tloss:0.005195\n",
      "Batch:\t196 /1004\t:  6.24764919281 \t1230.51849699 \tloss:0.005163\n",
      "Batch:\t197 /1004\t:  6.42173314095 \t1236.9415729 \tloss:0.005195\n",
      "Batch:\t198 /1004\t:  6.1750600338 \t1243.11689901 \tloss:0.005163\n",
      "Batch:\t199 /1004\t:  6.16254401207 \t1249.28065109 \tloss:0.005195\n",
      "Batch:\t200 /1004\t:  6.05284786224 \t1255.33472991 \tloss:0.005195\n",
      "Batch:\t201 /1004\t:  6.1262588501 \t1261.4623549 \tloss:0.005195\n",
      "Batch:\t202 /1004\t:  6.33766317368 \t1267.80071497 \tloss:0.005131\n",
      "Batch:\t203 /1004\t:  6.38219308853 \t1274.1836319 \tloss:0.005163\n",
      "Batch:\t204 /1004\t:  6.16773700714 \t1280.3526969 \tloss:0.005195\n",
      "Batch:\t205 /1004\t:  6.48204112053 \t1286.83636904 \tloss:0.005195\n",
      "Batch:\t206 /1004\t:  6.13955903053 \t1292.9771049 \tloss:0.005195\n",
      "Batch:\t207 /1004\t:  6.4448530674 \t1299.42239308 \tloss:0.005195\n",
      "Batch:\t208 /1004\t:  6.26426196098 \t1305.68784404 \tloss:0.005195\n",
      "Batch:\t209 /1004\t:  5.92807006836 \t1311.61733103 \tloss:0.005163\n",
      "Batch:\t210 /1004\t:  6.00612187386 \t1317.62476587 \tloss:0.005195\n",
      "Batch:\t211 /1004\t:  6.13151907921 \t1323.75689292 \tloss:0.005195\n",
      "Batch:\t212 /1004\t:  6.29919099808 \t1330.05718803 \tloss:0.005163\n",
      "Batch:\t213 /1004\t:  6.08410191536 \t1336.14275289 \tloss:0.005195\n",
      "Batch:\t214 /1004\t:  5.98742580414 \t1342.1320641 \tloss:0.005195\n",
      "Batch:\t215 /1004\t:  6.06116700172 \t1348.19425106 \tloss:0.005131\n",
      "Batch:\t216 /1004\t:  6.1729619503 \t1354.36855793 \tloss:0.005131\n",
      "Batch:\t217 /1004\t:  6.33589816093 \t1360.70548797 \tloss:0.005195\n",
      "Batch:\t218 /1004\t:  6.27149510384 \t1366.97826695 \tloss:0.005195\n",
      "Batch:\t219 /1004\t:  6.45985102654 \t1373.44002104 \tloss:0.005195\n",
      "Batch:\t220 /1004\t:  6.05029082298 \t1379.49095607 \tloss:0.005195\n",
      "Batch:\t221 /1004\t:  6.27517795563 \t1385.76746893 \tloss:0.005163\n",
      "Batch:\t222 /1004\t:  6.4252820015 \t1392.19453406 \tloss:0.005195\n",
      "Batch:\t223 /1004\t:  6.22528409958 \t1398.42113996 \tloss:0.005163\n",
      "Batch:\t224 /1004\t:  6.08945393562 \t1404.51144099 \tloss:0.005099\n",
      "Batch:\t225 /1004\t:  6.04556298256 \t1410.55739903 \tloss:0.005195\n",
      "Batch:\t226 /1004\t:  6.11281991005 \t1416.6710701 \tloss:0.005195\n",
      "Batch:\t227 /1004\t:  6.24364590645 \t1422.91546702 \tloss:0.005163\n",
      "Batch:\t228 /1004\t:  6.08927202225 \t1429.00680089 \tloss:0.005099\n",
      "Batch:\t229 /1004\t:  6.09577298164 \t1435.1030159 \tloss:0.005195\n",
      "Batch:\t230 /1004\t:  6.21885704994 \t1441.32321501 \tloss:0.005163\n",
      "Batch:\t231 /1004\t:  6.10630702972 \t1447.43088698 \tloss:0.005195\n",
      "Batch:\t232 /1004\t:  6.38464689255 \t1453.81684399 \tloss:0.005163\n",
      "Batch:\t233 /1004\t:  6.00781607628 \t1459.82513595 \tloss:0.005195\n",
      "Batch:\t234 /1004\t:  6.33552384377 \t1466.1618979 \tloss:0.005195\n",
      "Batch:\t235 /1004\t:  6.11005091667 \t1472.2726059 \tloss:0.005195\n",
      "Batch:\t236 /1004\t:  6.49039888382 \t1478.76353097 \tloss:0.005163\n",
      "Batch:\t237 /1004\t:  6.38010907173 \t1485.14466405 \tloss:0.005195\n",
      "Batch:\t238 /1004\t:  6.11496901512 \t1491.26097107 \tloss:0.005163\n",
      "Batch:\t239 /1004\t:  6.0602080822 \t1497.32207108 \tloss:0.005195\n",
      "Batch:\t240 /1004\t:  6.05722308159 \t1503.3805511 \tloss:0.005195\n",
      "Batch:\t241 /1004\t:  6.29193401337 \t1509.67328191 \tloss:0.005195\n",
      "Batch:\t242 /1004\t:  6.41449093819 \t1516.08909988 \tloss:0.005195\n",
      "Batch:\t243 /1004\t:  6.45470309258 \t1522.5457561 \tloss:0.005195\n",
      "Batch:\t244 /1004\t:  5.82088899612 \t1528.3678391 \tloss:0.005195\n",
      "Batch:\t245 /1004\t:  6.11768698692 \t1534.48614287 \tloss:0.005163\n",
      "Batch:\t246 /1004\t:  6.1415309906 \t1540.62967491 \tloss:0.005195\n",
      "Batch:\t247 /1004\t:  6.61138296127 \t1547.24235296 \tloss:0.005131\n",
      "Batch:\t248 /1004\t:  6.05593395233 \t1553.29940391 \tloss:0.005163\n",
      "Batch:\t249 /1004\t:  6.05444908142 \t1559.35464692 \tloss:0.005195\n",
      "Batch:\t250 /1004\t:  6.12874507904 \t1565.48394394 \tloss:0.005195\n",
      "Batch:\t251 /1004\t:  6.25096487999 \t1571.7362411 \tloss:0.005195\n",
      "Batch:\t252 /1004\t:  6.06513690948 \t1577.80184793 \tloss:0.005195\n",
      "Batch:\t253 /1004\t:  6.12466311455 \t1583.92806005 \tloss:0.005163\n",
      "Batch:\t254 /1004\t:  6.17152690887 \t1590.1008451 \tloss:0.005195\n",
      "Batch:\t255 /1004\t:  6.36488699913 \t1596.46605206 \tloss:0.005195\n",
      "Batch:\t256 /1004\t:  6.42845797539 \t1602.89495707 \tloss:0.005163\n",
      "Batch:\t257 /1004\t:  6.103992939 \t1609.0001111 \tloss:0.005163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t258 /1004\t:  6.14872694016 \t1615.149297 \tloss:0.005131\n",
      "Batch:\t259 /1004\t:  6.0907227993 \t1621.24024892 \tloss:0.005131\n",
      "Batch:\t260 /1004\t:  6.16551589966 \t1627.40696406 \tloss:0.005195\n",
      "Batch:\t261 /1004\t:  6.31291794777 \t1633.72010493 \tloss:0.005163\n",
      "Batch:\t262 /1004\t:  6.09975790977 \t1639.82084489 \tloss:0.005131\n",
      "Batch:\t263 /1004\t:  6.23997998238 \t1646.06238103 \tloss:0.005195\n",
      "Batch:\t264 /1004\t:  6.10430502892 \t1652.16752601 \tloss:0.005195\n",
      "Batch:\t265 /1004\t:  6.18547296524 \t1658.35416389 \tloss:0.005195\n",
      "Batch:\t266 /1004\t:  6.17042899132 \t1664.52495503 \tloss:0.005195\n",
      "Batch:\t267 /1004\t:  6.06668305397 \t1670.59192991 \tloss:0.005131\n",
      "Batch:\t268 /1004\t:  6.36845493317 \t1676.961411 \tloss:0.005195\n",
      "Batch:\t269 /1004\t:  6.33138990402 \t1683.29385209 \tloss:0.005163\n",
      "Batch:\t270 /1004\t:  6.14858603477 \t1689.44327903 \tloss:0.005195\n",
      "Batch:\t271 /1004\t:  6.04453492165 \t1695.48946404 \tloss:0.005195\n",
      "Batch:\t272 /1004\t:  6.05099892616 \t1701.54168892 \tloss:0.005099\n",
      "Batch:\t273 /1004\t:  6.05780506134 \t1707.60032701 \tloss:0.005131\n",
      "Batch:\t274 /1004\t:  6.33456611633 \t1713.93505001 \tloss:0.005195\n",
      "Batch:\t275 /1004\t:  6.06985902786 \t1720.00538087 \tloss:0.005163\n",
      "Batch:\t276 /1004\t:  6.08387303352 \t1726.09106588 \tloss:0.005163\n",
      "Batch:\t277 /1004\t:  6.28444194794 \t1732.37631297 \tloss:0.005195\n",
      "Batch:\t278 /1004\t:  6.62848615646 \t1739.00606894 \tloss:0.005195\n",
      "Batch:\t279 /1004\t:  6.66632914543 \t1745.6743331 \tloss:0.005195\n",
      "Batch:\t280 /1004\t:  6.05091309547 \t1751.72564292 \tloss:0.005163\n",
      "Batch:\t281 /1004\t:  6.13066005707 \t1757.85682702 \tloss:0.005163\n",
      "Batch:\t282 /1004\t:  6.11467504501 \t1763.97279406 \tloss:0.005195\n",
      "Batch:\t283 /1004\t:  6.11023712158 \t1770.08364487 \tloss:0.005195\n",
      "Batch:\t284 /1004\t:  6.03257799149 \t1776.11807299 \tloss:0.005195\n",
      "Batch:\t285 /1004\t:  6.37381410599 \t1782.49246597 \tloss:0.005195\n",
      "Batch:\t286 /1004\t:  6.15771985054 \t1788.65138888 \tloss:0.005099\n",
      "Batch:\t287 /1004\t:  6.12482404709 \t1794.77664709 \tloss:0.005163\n",
      "Batch:\t288 /1004\t:  6.12536501884 \t1800.90254402 \tloss:0.005195\n",
      "Batch:\t289 /1004\t:  6.32325911522 \t1807.2292769 \tloss:0.005067\n",
      "Batch:\t290 /1004\t:  6.35491991043 \t1813.58620906 \tloss:0.005131\n",
      "Batch:\t291 /1004\t:  6.14511609077 \t1819.73176789 \tloss:0.005163\n",
      "Batch:\t292 /1004\t:  6.08302807808 \t1825.815377 \tloss:0.005195\n",
      "Batch:\t293 /1004\t:  6.14147186279 \t1831.95720291 \tloss:0.005195\n",
      "Batch:\t294 /1004\t:  6.30246400833 \t1838.2609539 \tloss:0.005195\n",
      "Batch:\t295 /1004\t:  6.47783899307 \t1844.74021888 \tloss:0.005195\n",
      "Batch:\t296 /1004\t:  6.6532459259 \t1851.39442897 \tloss:0.005163\n",
      "Batch:\t297 /1004\t:  6.39452219009 \t1857.79030299 \tloss:0.005195\n",
      "Batch:\t298 /1004\t:  6.13380789757 \t1863.92514396 \tloss:0.005195\n",
      "Batch:\t299 /1004\t:  6.53302717209 \t1870.45891094 \tloss:0.005163\n",
      "Batch:\t300 /1004\t:  6.27438592911 \t1876.73526287 \tloss:0.005163\n",
      "Batch:\t301 /1004\t:  6.06971788406 \t1882.80656195 \tloss:0.005195\n",
      "Batch:\t302 /1004\t:  6.00938916206 \t1888.817384 \tloss:0.005195\n",
      "Batch:\t303 /1004\t:  6.31856107712 \t1895.1368289 \tloss:0.005195\n",
      "Batch:\t304 /1004\t:  6.44338107109 \t1901.5811379 \tloss:0.005195\n",
      "Batch:\t305 /1004\t:  6.19684410095 \t1907.77926087 \tloss:0.005163\n",
      "Batch:\t306 /1004\t:  6.04411292076 \t1913.82540607 \tloss:0.005131\n",
      "Batch:\t307 /1004\t:  6.07682919502 \t1919.90344405 \tloss:0.005131\n",
      "Batch:\t308 /1004\t:  6.35082483292 \t1926.25499105 \tloss:0.005195\n",
      "Batch:\t309 /1004\t:  6.10711503029 \t1932.36405706 \tloss:0.005195\n",
      "Batch:\t310 /1004\t:  6.08107805252 \t1938.44572592 \tloss:0.005195\n",
      "Batch:\t311 /1004\t:  6.32729387283 \t1944.7744329 \tloss:0.005195\n",
      "Batch:\t312 /1004\t:  6.40464997292 \t1951.17966509 \tloss:0.005195\n",
      "Batch:\t313 /1004\t:  6.24345397949 \t1957.4241879 \tloss:0.005163\n",
      "Batch:\t314 /1004\t:  6.05731606483 \t1963.48196697 \tloss:0.005131\n",
      "Batch:\t315 /1004\t:  5.99619722366 \t1969.47993398 \tloss:0.005195\n",
      "Batch:\t316 /1004\t:  6.11392402649 \t1975.59451795 \tloss:0.005163\n",
      "Batch:\t317 /1004\t:  6.13211393356 \t1981.7277391 \tloss:0.005195\n",
      "Batch:\t318 /1004\t:  6.09049892426 \t1987.81911492 \tloss:0.005163\n",
      "Batch:\t319 /1004\t:  6.79473209381 \t1994.61409187 \tloss:0.005163\n",
      "Batch:\t320 /1004\t:  6.39294886589 \t2001.00866008 \tloss:0.005195\n",
      "Batch:\t321 /1004\t:  6.01352596283 \t2007.02325797 \tloss:0.005195\n",
      "Batch:\t322 /1004\t:  6.092856884 \t2013.11752105 \tloss:0.005195\n",
      "Batch:\t323 /1004\t:  6.12672901154 \t2019.2462399 \tloss:0.005195\n",
      "Batch:\t324 /1004\t:  6.14004516602 \t2025.38688898 \tloss:0.005131\n",
      "Batch:\t325 /1004\t:  6.27359604836 \t2031.66177988 \tloss:0.005195\n",
      "Batch:\t326 /1004\t:  6.32595705986 \t2037.98827004 \tloss:0.005163\n",
      "Batch:\t327 /1004\t:  6.15920805931 \t2044.1480689 \tloss:0.005195\n",
      "Batch:\t328 /1004\t:  6.21767711639 \t2050.36692691 \tloss:0.005195\n",
      "Batch:\t329 /1004\t:  6.04225921631 \t2056.41051793 \tloss:0.005195\n",
      "Batch:\t330 /1004\t:  6.08983898163 \t2062.50150108 \tloss:0.005195\n",
      "Batch:\t331 /1004\t:  5.98400211334 \t2068.48648906 \tloss:0.005195\n",
      "Batch:\t332 /1004\t:  6.00113320351 \t2074.48776388 \tloss:0.005195\n",
      "Batch:\t333 /1004\t:  6.16347193718 \t2080.652179 \tloss:0.005163\n",
      "Batch:\t334 /1004\t:  6.20069599152 \t2086.853513 \tloss:0.005195\n",
      "Batch:\t335 /1004\t:  6.09388804436 \t2092.94804907 \tloss:0.005195\n",
      "Batch:\t336 /1004\t:  6.04963803291 \t2098.99839807 \tloss:0.005195\n",
      "Batch:\t337 /1004\t:  6.12471103668 \t2105.12363005 \tloss:0.005195\n",
      "Batch:\t338 /1004\t:  6.10056400299 \t2111.2251761 \tloss:0.005163\n",
      "Batch:\t339 /1004\t:  6.12989497185 \t2117.35529804 \tloss:0.005195\n",
      "Batch:\t340 /1004\t:  6.19656300545 \t2123.55251503 \tloss:0.005195\n",
      "Batch:\t341 /1004\t:  6.06366205215 \t2129.61733103 \tloss:0.005195\n",
      "Batch:\t342 /1004\t:  6.13210892677 \t2135.74991894 \tloss:0.005163\n",
      "Batch:\t343 /1004\t:  6.15399312973 \t2141.90451193 \tloss:0.005131\n",
      "Batch:\t344 /1004\t:  6.05013585091 \t2147.95747709 \tloss:0.005195\n",
      "Batch:\t345 /1004\t:  6.09421777725 \t2154.05283809 \tloss:0.005195\n",
      "Batch:\t346 /1004\t:  6.03379511833 \t2160.08717203 \tloss:0.005163\n",
      "Batch:\t347 /1004\t:  5.96457600594 \t2166.0523479 \tloss:0.005195\n",
      "Batch:\t348 /1004\t:  6.28647518158 \t2172.33970904 \tloss:0.005163\n",
      "Batch:\t349 /1004\t:  6.10482501984 \t2178.44536805 \tloss:0.005195\n",
      "Batch:\t350 /1004\t:  6.45628190041 \t2184.90187287 \tloss:0.005195\n",
      "Batch:\t351 /1004\t:  6.10636901855 \t2191.00913405 \tloss:0.005195\n",
      "Batch:\t352 /1004\t:  6.04518485069 \t2197.0556109 \tloss:0.005195\n",
      "Batch:\t353 /1004\t:  6.27280187607 \t2203.32867002 \tloss:0.005195\n",
      "Batch:\t354 /1004\t:  6.36020207405 \t2209.69012094 \tloss:0.005163\n",
      "Batch:\t355 /1004\t:  6.15276193619 \t2215.84349799 \tloss:0.005163\n",
      "Batch:\t356 /1004\t:  6.39307284355 \t2222.24078989 \tloss:0.005195\n",
      "Batch:\t357 /1004\t:  6.49366116524 \t2228.73575401 \tloss:0.005195\n",
      "Batch:\t358 /1004\t:  6.03824591637 \t2234.77427888 \tloss:0.005195\n",
      "Batch:\t359 /1004\t:  6.33322286606 \t2241.107939 \tloss:0.005195\n",
      "Batch:\t360 /1004\t:  6.11224484444 \t2247.220608 \tloss:0.005195\n",
      "Batch:\t361 /1004\t:  6.50057888031 \t2253.72253108 \tloss:0.005163\n",
      "Batch:\t362 /1004\t:  6.26478719711 \t2259.98841691 \tloss:0.005163\n",
      "Batch:\t363 /1004\t:  6.14916491508 \t2266.13887095 \tloss:0.005163\n",
      "Batch:\t364 /1004\t:  5.96482992172 \t2272.1038909 \tloss:0.005195\n",
      "Batch:\t365 /1004\t:  6.49344205856 \t2278.59870791 \tloss:0.005195\n",
      "Batch:\t366 /1004\t:  6.32585906982 \t2284.92649508 \tloss:0.005195\n",
      "Batch:\t367 /1004\t:  6.40318107605 \t2291.3299911 \tloss:0.005163\n",
      "Batch:\t368 /1004\t:  6.09101486206 \t2297.42144299 \tloss:0.005195\n",
      "Batch:\t369 /1004\t:  6.06875705719 \t2303.49077797 \tloss:0.005131\n",
      "Batch:\t370 /1004\t:  6.10826396942 \t2309.60016108 \tloss:0.005195\n",
      "Batch:\t371 /1004\t:  6.14965581894 \t2315.75125003 \tloss:0.005195\n",
      "Batch:\t372 /1004\t:  6.32334113121 \t2322.07519197 \tloss:0.005163\n",
      "Batch:\t373 /1004\t:  6.10458898544 \t2328.18100405 \tloss:0.005195\n",
      "Batch:\t374 /1004\t:  6.24454903603 \t2334.42681789 \tloss:0.005195\n",
      "Batch:\t375 /1004\t:  6.319688797 \t2340.74772787 \tloss:0.005099\n",
      "Batch:\t376 /1004\t:  5.92816591263 \t2346.6762321 \tloss:0.005195\n",
      "Batch:\t377 /1004\t:  5.95814204216 \t2352.63464093 \tloss:0.005195\n",
      "Batch:\t378 /1004\t:  6.43040704727 \t2359.06531906 \tloss:0.005195\n",
      "Batch:\t379 /1004\t:  6.16033482552 \t2365.22618389 \tloss:0.005195\n",
      "Batch:\t380 /1004\t:  6.2160410881 \t2371.4428699 \tloss:0.005195\n",
      "Batch:\t381 /1004\t:  6.13736605644 \t2377.581424 \tloss:0.005195\n",
      "Batch:\t382 /1004\t:  6.12227487564 \t2383.70412898 \tloss:0.005195\n",
      "Batch:\t383 /1004\t:  6.36459088326 \t2390.07006693 \tloss:0.005099\n",
      "Batch:\t384 /1004\t:  6.1961581707 \t2396.26733208 \tloss:0.005195\n",
      "Batch:\t385 /1004\t:  6.07383108139 \t2402.34254503 \tloss:0.005195\n",
      "Batch:\t386 /1004\t:  6.07331800461 \t2408.41695595 \tloss:0.005131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t387 /1004\t:  6.20923399925 \t2414.62722993 \tloss:0.005099\n",
      "Batch:\t388 /1004\t:  6.07170987129 \t2420.69950604 \tloss:0.005195\n",
      "Batch:\t389 /1004\t:  6.06850409508 \t2426.76903391 \tloss:0.005195\n",
      "Batch:\t390 /1004\t:  6.19575595856 \t2432.96605492 \tloss:0.005163\n",
      "Batch:\t391 /1004\t:  6.32635211945 \t2439.29383898 \tloss:0.005131\n",
      "Batch:\t392 /1004\t:  6.0762860775 \t2445.37135506 \tloss:0.005195\n",
      "Batch:\t393 /1004\t:  6.28105902672 \t2451.65331888 \tloss:0.005195\n",
      "Batch:\t394 /1004\t:  6.39584803581 \t2458.05032897 \tloss:0.005195\n",
      "Batch:\t395 /1004\t:  5.99245095253 \t2464.04414988 \tloss:0.005163\n",
      "Batch:\t396 /1004\t:  6.09572696686 \t2470.14327598 \tloss:0.005195\n",
      "Batch:\t397 /1004\t:  6.07305502892 \t2476.2176621 \tloss:0.005195\n",
      "Batch:\t398 /1004\t:  6.22309494019 \t2482.4412961 \tloss:0.005195\n",
      "Batch:\t399 /1004\t:  6.04945397377 \t2488.49196696 \tloss:0.005195\n",
      "Batch:\t400 /1004\t:  6.4056019783 \t2494.89897609 \tloss:0.005195\n",
      "Batch:\t401 /1004\t:  6.13421297073 \t2501.03442788 \tloss:0.005195\n",
      "Batch:\t402 /1004\t:  6.16437792778 \t2507.19910288 \tloss:0.005195\n",
      "Batch:\t403 /1004\t:  6.19913387299 \t2513.3987 \tloss:0.005195\n",
      "Batch:\t404 /1004\t:  6.08014512062 \t2519.48041105 \tloss:0.005195\n",
      "Batch:\t405 /1004\t:  6.43206620216 \t2525.91360903 \tloss:0.005163\n",
      "Batch:\t406 /1004\t:  6.69810986519 \t2532.61296606 \tloss:0.005195\n",
      "Batch:\t407 /1004\t:  6.10927081108 \t2538.7234931 \tloss:0.005195\n",
      "Batch:\t408 /1004\t:  6.16059398651 \t2544.88498807 \tloss:0.005195\n",
      "Batch:\t409 /1004\t:  6.20657587051 \t2551.09349298 \tloss:0.005195\n",
      "Batch:\t410 /1004\t:  6.21554207802 \t2557.31097007 \tloss:0.005195\n",
      "Batch:\t411 /1004\t:  6.18623304367 \t2563.49860287 \tloss:0.005195\n",
      "Batch:\t412 /1004\t:  6.34892702103 \t2569.8488729 \tloss:0.005195\n",
      "Batch:\t413 /1004\t:  6.23051905632 \t2576.08135605 \tloss:0.005131\n",
      "Batch:\t414 /1004\t:  6.39898085594 \t2582.4811511 \tloss:0.005195\n",
      "Batch:\t415 /1004\t:  6.14737606049 \t2588.62951493 \tloss:0.005163\n",
      "Batch:\t416 /1004\t:  6.05207514763 \t2594.68214297 \tloss:0.005195\n",
      "Batch:\t417 /1004\t:  6.11040592194 \t2600.7936399 \tloss:0.005195\n",
      "Batch:\t418 /1004\t:  6.75893712044 \t2607.55336189 \tloss:0.005163\n",
      "Batch:\t419 /1004\t:  6.45212483406 \t2614.00682306 \tloss:0.005195\n",
      "Batch:\t420 /1004\t:  6.12044906616 \t2620.12839794 \tloss:0.005195\n",
      "Batch:\t421 /1004\t:  6.25487709045 \t2626.38441801 \tloss:0.005195\n",
      "Batch:\t422 /1004\t:  6.31301593781 \t2632.69869709 \tloss:0.005195\n",
      "Batch:\t423 /1004\t:  6.27369499207 \t2638.97298288 \tloss:0.005195\n",
      "Batch:\t424 /1004\t:  6.3046131134 \t2645.27880907 \tloss:0.005163\n",
      "Batch:\t425 /1004\t:  6.44182300568 \t2651.72171307 \tloss:0.005195\n",
      "Batch:\t426 /1004\t:  6.24849796295 \t2657.97116399 \tloss:0.005195\n",
      "Batch:\t427 /1004\t:  6.41997694969 \t2664.39157104 \tloss:0.005163\n",
      "Batch:\t428 /1004\t:  6.38761210442 \t2670.78073692 \tloss:0.005195\n",
      "Batch:\t429 /1004\t:  6.07787203789 \t2676.860502 \tloss:0.005131\n",
      "Batch:\t430 /1004\t:  6.15806007385 \t2683.0190351 \tloss:0.005099\n",
      "Batch:\t431 /1004\t:  6.15421485901 \t2689.17387795 \tloss:0.005195\n",
      "Batch:\t432 /1004\t:  6.02545714378 \t2695.20115995 \tloss:0.005099\n",
      "Batch:\t433 /1004\t:  6.22182703018 \t2701.42354703 \tloss:0.005195\n",
      "Batch:\t434 /1004\t:  6.36112809181 \t2707.78587008 \tloss:0.005163\n",
      "Batch:\t435 /1004\t:  6.17221403122 \t2713.95941496 \tloss:0.005195\n",
      "Batch:\t436 /1004\t:  6.36964702606 \t2720.3295269 \tloss:0.005163\n",
      "Batch:\t437 /1004\t:  6.0710811615 \t2726.4020431 \tloss:0.005195\n",
      "Batch:\t438 /1004\t:  6.24111199379 \t2732.64425898 \tloss:0.005195\n",
      "Batch:\t439 /1004\t:  6.22979593277 \t2738.87476707 \tloss:0.005195\n",
      "Batch:\t440 /1004\t:  6.42816996574 \t2745.30395198 \tloss:0.005195\n",
      "Batch:\t441 /1004\t:  6.05161499977 \t2751.35667896 \tloss:0.005163\n",
      "Batch:\t442 /1004\t:  6.32143497467 \t2757.67829704 \tloss:0.005195\n",
      "Batch:\t443 /1004\t:  6.37108898163 \t2764.0501411 \tloss:0.005195\n",
      "Batch:\t444 /1004\t:  6.25105810165 \t2770.30246687 \tloss:0.005163\n",
      "Batch:\t445 /1004\t:  6.07014703751 \t2776.37383604 \tloss:0.005195\n",
      "Batch:\t446 /1004\t:  6.1410279274 \t2782.51565409 \tloss:0.005163\n",
      "Batch:\t447 /1004\t:  6.05887293816 \t2788.5758419 \tloss:0.005195\n",
      "Batch:\t448 /1004\t:  6.09769296646 \t2794.67407703 \tloss:0.005163\n",
      "Batch:\t449 /1004\t:  6.48539304733 \t2801.15996599 \tloss:0.005195\n",
      "Batch:\t450 /1004\t:  6.16573500633 \t2807.32698607 \tloss:0.005163\n",
      "Batch:\t451 /1004\t:  6.11883997917 \t2813.4476409 \tloss:0.005195\n",
      "Batch:\t452 /1004\t:  6.34633111954 \t2819.79434299 \tloss:0.005163\n",
      "Batch:\t453 /1004\t:  6.02067303658 \t2825.81546402 \tloss:0.005195\n",
      "Batch:\t454 /1004\t:  6.35763478279 \t2832.17507601 \tloss:0.005099\n",
      "Batch:\t455 /1004\t:  6.16003894806 \t2838.33702087 \tloss:0.005099\n",
      "Batch:\t456 /1004\t:  6.42985582352 \t2844.76841402 \tloss:0.005195\n",
      "Batch:\t457 /1004\t:  6.47621512413 \t2851.24581409 \tloss:0.005163\n",
      "Batch:\t458 /1004\t:  6.25346493721 \t2857.50026202 \tloss:0.005195\n",
      "Batch:\t459 /1004\t:  6.18353199959 \t2863.68510008 \tloss:0.005195\n",
      "Batch:\t460 /1004\t:  6.16767215729 \t2869.85361409 \tloss:0.005195\n",
      "Batch:\t461 /1004\t:  6.21104598045 \t2876.06581187 \tloss:0.005163\n",
      "Batch:\t462 /1004\t:  5.99230289459 \t2882.05996704 \tloss:0.005195\n",
      "Batch:\t463 /1004\t:  6.09286308289 \t2888.15327597 \tloss:0.005163\n",
      "Batch:\t464 /1004\t:  6.1414039135 \t2894.29597688 \tloss:0.005195\n",
      "Batch:\t465 /1004\t:  6.19071316719 \t2900.48730803 \tloss:0.005195\n",
      "Batch:\t466 /1004\t:  6.36565494537 \t2906.85421109 \tloss:0.005195\n",
      "Batch:\t467 /1004\t:  6.26135420799 \t2913.116786 \tloss:0.005195\n",
      "Batch:\t468 /1004\t:  6.02249908447 \t2919.13955903 \tloss:0.005195\n",
      "Batch:\t469 /1004\t:  5.93347597122 \t2925.07395697 \tloss:0.005195\n",
      "Batch:\t470 /1004\t:  6.06706309319 \t2931.14123392 \tloss:0.005131\n",
      "Batch:\t471 /1004\t:  6.00254797935 \t2937.14500809 \tloss:0.005163\n",
      "Batch:\t472 /1004\t:  6.07185101509 \t2943.21842098 \tloss:0.005131\n",
      "Batch:\t473 /1004\t:  6.09893417358 \t2949.31853604 \tloss:0.005195\n",
      "Batch:\t474 /1004\t:  6.1689350605 \t2955.48806 \tloss:0.005195\n",
      "Batch:\t475 /1004\t:  6.14923381805 \t2961.63866687 \tloss:0.005195\n",
      "Batch:\t476 /1004\t:  6.10576701164 \t2967.74562097 \tloss:0.005195\n",
      "Batch:\t477 /1004\t:  6.08351111412 \t2973.82936192 \tloss:0.005195\n",
      "Batch:\t478 /1004\t:  6.20530104637 \t2980.03520989 \tloss:0.005195\n",
      "Batch:\t479 /1004\t:  6.19832801819 \t2986.23398495 \tloss:0.005163\n",
      "Batch:\t480 /1004\t:  6.12369012833 \t2992.35897899 \tloss:0.005195\n",
      "Batch:\t481 /1004\t:  6.53440690041 \t2998.89396596 \tloss:0.005163\n",
      "Batch:\t482 /1004\t:  6.23211812973 \t3005.12712288 \tloss:0.005163\n",
      "Batch:\t483 /1004\t:  6.13593506813 \t3011.26478195 \tloss:0.005195\n",
      "Batch:\t484 /1004\t:  6.03459000587 \t3017.29980302 \tloss:0.005195\n",
      "Batch:\t485 /1004\t:  6.25852012634 \t3023.55895901 \tloss:0.005131\n",
      "Batch:\t486 /1004\t:  6.56813502312 \t3030.12910295 \tloss:0.005131\n",
      "Batch:\t487 /1004\t:  6.28097105026 \t3036.41193295 \tloss:0.005195\n",
      "Batch:\t488 /1004\t:  6.16683721542 \t3042.57999897 \tloss:0.005195\n",
      "Batch:\t489 /1004\t:  6.06187701225 \t3048.64210796 \tloss:0.005131\n",
      "Batch:\t490 /1004\t:  5.99749207497 \t3054.64114308 \tloss:0.005163\n",
      "Batch:\t491 /1004\t:  6.09620189667 \t3060.73775387 \tloss:0.005195\n",
      "Batch:\t492 /1004\t:  6.20854306221 \t3066.94686604 \tloss:0.005195\n",
      "Batch:\t493 /1004\t:  6.12530183792 \t3073.07266903 \tloss:0.005195\n",
      "Batch:\t494 /1004\t:  6.25839591026 \t3079.33238292 \tloss:0.005195\n",
      "Batch:\t495 /1004\t:  6.47994089127 \t3085.81416798 \tloss:0.005195\n",
      "Batch:\t496 /1004\t:  6.10919094086 \t3091.92490196 \tloss:0.005195\n",
      "Batch:\t497 /1004\t:  6.62139415741 \t3098.54735994 \tloss:0.005195\n",
      "Batch:\t498 /1004\t:  6.23178386688 \t3104.7804451 \tloss:0.005163\n",
      "Batch:\t499 /1004\t:  6.30317497253 \t3111.08557987 \tloss:0.005195\n",
      "Batch:\t500 /1004\t:  6.13148403168 \t3117.21733904 \tloss:0.005163\n",
      "Batch:\t501 /1004\t:  6.1092479229 \t3123.32862902 \tloss:0.005195\n",
      "Batch:\t502 /1004\t:  6.42484307289 \t3129.75475192 \tloss:0.005195\n",
      "Batch:\t503 /1004\t:  6.15082597733 \t3135.90728998 \tloss:0.005195\n",
      "Batch:\t504 /1004\t:  6.39480900764 \t3142.30255294 \tloss:0.005163\n",
      "Batch:\t505 /1004\t:  5.87130403519 \t3148.174196 \tloss:0.005195\n",
      "Batch:\t506 /1004\t:  6.33982896805 \t3154.51432991 \tloss:0.005163\n",
      "Batch:\t507 /1004\t:  6.23468112946 \t3160.75096703 \tloss:0.005195\n",
      "Batch:\t508 /1004\t:  6.03819608688 \t3166.79007101 \tloss:0.005195\n",
      "Batch:\t509 /1004\t:  6.17140293121 \t3172.96280408 \tloss:0.005195\n",
      "Batch:\t510 /1004\t:  6.23587298393 \t3179.20018888 \tloss:0.005163\n",
      "Batch:\t511 /1004\t:  6.16348099709 \t3185.36427307 \tloss:0.005195\n",
      "Batch:\t512 /1004\t:  6.09940910339 \t3191.46420908 \tloss:0.005195\n",
      "Batch:\t513 /1004\t:  5.97470712662 \t3197.4401021 \tloss:0.005131\n",
      "Batch:\t514 /1004\t:  6.12399506569 \t3203.56471992 \tloss:0.005195\n",
      "Batch:\t515 /1004\t:  6.59578895569 \t3210.16108799 \tloss:0.005195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t516 /1004\t:  6.5406639576 \t3216.70316195 \tloss:0.005195\n",
      "Batch:\t517 /1004\t:  6.16429901123 \t3222.86875796 \tloss:0.005163\n",
      "Batch:\t518 /1004\t:  6.47137093544 \t3229.34196591 \tloss:0.005195\n",
      "Batch:\t519 /1004\t:  6.37333893776 \t3235.71664596 \tloss:0.005195\n",
      "Batch:\t520 /1004\t:  6.18980622292 \t3241.90771508 \tloss:0.005195\n",
      "Batch:\t521 /1004\t:  6.41914510727 \t3248.32730389 \tloss:0.005195\n",
      "Batch:\t522 /1004\t:  6.10790205002 \t3254.43649697 \tloss:0.005131\n",
      "Batch:\t523 /1004\t:  6.09047198296 \t3260.52825904 \tloss:0.005195\n",
      "Batch:\t524 /1004\t:  6.00865888596 \t3266.5380249 \tloss:0.005195\n",
      "Batch:\t525 /1004\t:  6.58293199539 \t3273.12139201 \tloss:0.005195\n",
      "Batch:\t526 /1004\t:  6.02468705177 \t3279.1464951 \tloss:0.005195\n",
      "Batch:\t527 /1004\t:  5.96345186234 \t3285.11011004 \tloss:0.005195\n",
      "Batch:\t528 /1004\t:  6.12014102936 \t3291.23157096 \tloss:0.005195\n",
      "Batch:\t529 /1004\t:  6.38738012314 \t3297.62050605 \tloss:0.005195\n",
      "Batch:\t530 /1004\t:  6.21269893646 \t3303.83344889 \tloss:0.005195\n",
      "Batch:\t531 /1004\t:  6.11884284019 \t3309.95274401 \tloss:0.005131\n",
      "Batch:\t532 /1004\t:  6.16932201385 \t3316.1225071 \tloss:0.005195\n",
      "Batch:\t533 /1004\t:  6.33628988266 \t3322.45933795 \tloss:0.005195\n",
      "Batch:\t534 /1004\t:  6.25766682625 \t3328.71900702 \tloss:0.005131\n",
      "Batch:\t535 /1004\t:  6.28294086456 \t3335.0030961 \tloss:0.005195\n",
      "Batch:\t536 /1004\t:  6.13191890717 \t3341.13634205 \tloss:0.005163\n",
      "Batch:\t537 /1004\t:  6.19320392609 \t3347.33016109 \tloss:0.005195\n",
      "Batch:\t538 /1004\t:  6.0700750351 \t3353.401438 \tloss:0.005195\n",
      "Batch:\t539 /1004\t:  6.06856894493 \t3359.47087502 \tloss:0.005195\n",
      "Batch:\t540 /1004\t:  6.02399587631 \t3365.49620605 \tloss:0.005195\n",
      "Batch:\t541 /1004\t:  6.35990905762 \t3371.85716891 \tloss:0.005131\n",
      "Batch:\t542 /1004\t:  6.1347951889 \t3377.99308896 \tloss:0.005195\n",
      "Batch:\t543 /1004\t:  6.2339758873 \t3384.22750092 \tloss:0.005195\n",
      "Batch:\t544 /1004\t:  6.18084096909 \t3390.40890908 \tloss:0.005163\n",
      "Batch:\t545 /1004\t:  6.24511885643 \t3396.65533304 \tloss:0.005195\n",
      "Batch:\t546 /1004\t:  6.29334998131 \t3402.94966292 \tloss:0.005195\n",
      "Batch:\t547 /1004\t:  6.34493303299 \t3409.29651499 \tloss:0.005195\n",
      "Batch:\t548 /1004\t:  6.59654283524 \t3415.89482188 \tloss:0.005099\n",
      "Batch:\t549 /1004\t:  6.11311101913 \t3422.00925398 \tloss:0.005163\n",
      "Batch:\t550 /1004\t:  6.1011300087 \t3428.111274 \tloss:0.005195\n",
      "Batch:\t551 /1004\t:  6.10860109329 \t3434.22104692 \tloss:0.005163\n",
      "Batch:\t552 /1004\t:  6.28688406944 \t3440.511163 \tloss:0.005195\n",
      "Batch:\t553 /1004\t:  6.17212319374 \t3446.68381596 \tloss:0.005195\n",
      "Batch:\t554 /1004\t:  6.27395009995 \t3452.95967603 \tloss:0.005195\n",
      "Batch:\t555 /1004\t:  6.22130489349 \t3459.18235087 \tloss:0.005195\n",
      "Batch:\t556 /1004\t:  6.09353899956 \t3465.27786708 \tloss:0.005195\n",
      "Batch:\t557 /1004\t:  6.25239300728 \t3471.53070498 \tloss:0.005195\n",
      "Batch:\t558 /1004\t:  6.15646314621 \t3477.68839192 \tloss:0.005163\n",
      "Batch:\t559 /1004\t:  6.249355793 \t3483.93886805 \tloss:0.005195\n",
      "Batch:\t560 /1004\t:  6.35345315933 \t3490.29386306 \tloss:0.005195\n",
      "Batch:\t561 /1004\t:  6.10250687599 \t3496.3976891 \tloss:0.005195\n",
      "Batch:\t562 /1004\t:  6.09556293488 \t3502.49401188 \tloss:0.005163\n",
      "Batch:\t563 /1004\t:  6.48344397545 \t3508.98133206 \tloss:0.005195\n",
      "Batch:\t564 /1004\t:  6.50333380699 \t3515.48647809 \tloss:0.005163\n",
      "Batch:\t565 /1004\t:  6.15907406807 \t3521.64583707 \tloss:0.005163\n",
      "Batch:\t566 /1004\t:  6.07999300957 \t3527.72704792 \tloss:0.005195\n",
      "Batch:\t567 /1004\t:  6.1560151577 \t3533.88481307 \tloss:0.005195\n",
      "Batch:\t568 /1004\t:  6.1095290184 \t3539.99486494 \tloss:0.005195\n",
      "Batch:\t569 /1004\t:  6.0663819313 \t3546.06320596 \tloss:0.005195\n",
      "Batch:\t570 /1004\t:  6.11147022247 \t3552.17490602 \tloss:0.005195\n",
      "Batch:\t571 /1004\t:  6.32972407341 \t3558.50522304 \tloss:0.005131\n",
      "Batch:\t572 /1004\t:  6.71674108505 \t3565.22317004 \tloss:0.005195\n",
      "Batch:\t573 /1004\t:  6.30728697777 \t3571.53206301 \tloss:0.005163\n",
      "Batch:\t574 /1004\t:  6.2268538475 \t3577.76019287 \tloss:0.005195\n",
      "Batch:\t575 /1004\t:  6.23867082596 \t3584.00015688 \tloss:0.005131\n",
      "Batch:\t576 /1004\t:  6.34493994713 \t3590.3459661 \tloss:0.005195\n",
      "Batch:\t577 /1004\t:  6.42239189148 \t3596.77170706 \tloss:0.005099\n",
      "Batch:\t578 /1004\t:  6.00195598602 \t3602.77485991 \tloss:0.005163\n",
      "Batch:\t579 /1004\t:  6.22936487198 \t3609.00445604 \tloss:0.005195\n",
      "Batch:\t580 /1004\t:  5.99613308907 \t3615.00174594 \tloss:0.005163\n",
      "Batch:\t581 /1004\t:  6.12587499619 \t3621.12896395 \tloss:0.005195\n",
      "Batch:\t582 /1004\t:  6.14856791496 \t3627.27806592 \tloss:0.005163\n",
      "Batch:\t583 /1004\t:  6.06852793694 \t3633.34780788 \tloss:0.005195\n",
      "Batch:\t584 /1004\t:  6.26421308517 \t3639.61332107 \tloss:0.005163\n",
      "Batch:\t585 /1004\t:  6.28210783005 \t3645.89656091 \tloss:0.005195\n",
      "Batch:\t586 /1004\t:  6.06634998322 \t3651.96429992 \tloss:0.005131\n",
      "Batch:\t587 /1004\t:  6.13566207886 \t3658.10131598 \tloss:0.005163\n",
      "Batch:\t588 /1004\t:  6.10587000847 \t3664.208462 \tloss:0.005195\n",
      "Batch:\t589 /1004\t:  6.15060782433 \t3670.35977793 \tloss:0.005099\n",
      "Batch:\t590 /1004\t:  6.0277929306 \t3676.38943791 \tloss:0.005163\n",
      "Batch:\t591 /1004\t:  6.61725592613 \t3683.007972 \tloss:0.005195\n",
      "Batch:\t592 /1004\t:  6.39843797684 \t3689.40726399 \tloss:0.005195\n",
      "Batch:\t593 /1004\t:  6.19567894936 \t3695.60496187 \tloss:0.005195\n",
      "Batch:\t594 /1004\t:  6.40055704117 \t3702.00752592 \tloss:0.005195\n",
      "Batch:\t595 /1004\t:  6.1861770153 \t3708.19501495 \tloss:0.005163\n",
      "Batch:\t596 /1004\t:  6.12238001823 \t3714.31858301 \tloss:0.005195\n",
      "Batch:\t597 /1004\t:  6.11196708679 \t3720.43134594 \tloss:0.005195\n",
      "Batch:\t598 /1004\t:  6.04154014587 \t3726.47417402 \tloss:0.005163\n",
      "Batch:\t599 /1004\t:  5.98402118683 \t3732.45988607 \tloss:0.005131\n",
      "Batch:\t600 /1004\t:  6.35615801811 \t3738.81730008 \tloss:0.005195\n",
      "Batch:\t601 /1004\t:  6.01255106926 \t3744.83107805 \tloss:0.005131\n",
      "Batch:\t602 /1004\t:  6.09988594055 \t3750.93296289 \tloss:0.005195\n",
      "Batch:\t603 /1004\t:  6.15342998505 \t3757.08698392 \tloss:0.005195\n",
      "Batch:\t604 /1004\t:  6.42901301384 \t3763.51760197 \tloss:0.005195\n",
      "Batch:\t605 /1004\t:  6.37466287613 \t3769.89279604 \tloss:0.005131\n",
      "Batch:\t606 /1004\t:  6.12228703499 \t3776.01651406 \tloss:0.005195\n",
      "Batch:\t607 /1004\t:  6.51181197166 \t3782.52909088 \tloss:0.005163\n",
      "Batch:\t608 /1004\t:  6.11622905731 \t3788.64676094 \tloss:0.005195\n",
      "Batch:\t609 /1004\t:  6.28085589409 \t3794.9286449 \tloss:0.005131\n",
      "Batch:\t610 /1004\t:  6.08314085007 \t3801.01255989 \tloss:0.005195\n",
      "Batch:\t611 /1004\t:  6.21694684029 \t3807.23004103 \tloss:0.005195\n",
      "Batch:\t612 /1004\t:  6.10935211182 \t3813.33983397 \tloss:0.005195\n",
      "Batch:\t613 /1004\t:  6.15996909142 \t3819.5004189 \tloss:0.005195\n",
      "Batch:\t614 /1004\t:  6.19019389153 \t3825.69123602 \tloss:0.005195\n",
      "Batch:\t615 /1004\t:  6.08844304085 \t3831.78154302 \tloss:0.005195\n",
      "Batch:\t616 /1004\t:  6.49741387367 \t3838.27951694 \tloss:0.005131\n",
      "Batch:\t617 /1004\t:  6.17004299164 \t3844.45034409 \tloss:0.005195\n",
      "Batch:\t618 /1004\t:  6.10740494728 \t3850.55845094 \tloss:0.005195\n",
      "Batch:\t619 /1004\t:  6.46139979362 \t3857.02009201 \tloss:0.005163\n",
      "Batch:\t620 /1004\t:  6.24492502213 \t3863.26635098 \tloss:0.005195\n",
      "Batch:\t621 /1004\t:  6.50235295296 \t3869.76992393 \tloss:0.005195\n",
      "Batch:\t622 /1004\t:  6.46737098694 \t3876.23805499 \tloss:0.005163\n",
      "Batch:\t623 /1004\t:  6.18446493149 \t3882.42414904 \tloss:0.005195\n",
      "Batch:\t624 /1004\t:  6.62578392029 \t3889.05126405 \tloss:0.005163\n",
      "Batch:\t625 /1004\t:  6.13702201843 \t3895.18965387 \tloss:0.005195\n",
      "Batch:\t626 /1004\t:  6.20090723038 \t3901.39162803 \tloss:0.005163\n",
      "Batch:\t627 /1004\t:  6.45805191994 \t3907.84999204 \tloss:0.005163\n",
      "Batch:\t628 /1004\t:  6.26954293251 \t3914.1206491 \tloss:0.005195\n",
      "Batch:\t629 /1004\t:  6.26774311066 \t3920.389534 \tloss:0.005195\n",
      "Batch:\t630 /1004\t:  6.27367687225 \t3926.66460109 \tloss:0.005195\n",
      "Batch:\t631 /1004\t:  6.15051603317 \t3932.81633687 \tloss:0.005163\n",
      "Batch:\t632 /1004\t:  6.12147808075 \t3938.93914795 \tloss:0.005163\n",
      "Batch:\t633 /1004\t:  6.39112901688 \t3945.33080196 \tloss:0.005099\n",
      "Batch:\t634 /1004\t:  6.18844389915 \t3951.52121902 \tloss:0.005195\n",
      "Batch:\t635 /1004\t:  6.14338898659 \t3957.66590595 \tloss:0.005131\n",
      "Batch:\t636 /1004\t:  6.10225892067 \t3963.7687521 \tloss:0.005195\n",
      "Batch:\t637 /1004\t:  6.37498998642 \t3970.14476109 \tloss:0.005163\n",
      "Batch:\t638 /1004\t:  6.17739200592 \t3976.32368588 \tloss:0.005195\n",
      "Batch:\t639 /1004\t:  6.40792608261 \t3982.73204494 \tloss:0.005195\n",
      "Batch:\t640 /1004\t:  6.0069360733 \t3988.74071503 \tloss:0.005195\n",
      "Batch:\t641 /1004\t:  6.27628302574 \t3995.01812792 \tloss:0.005195\n",
      "Batch:\t642 /1004\t:  6.10566806793 \t4001.12456393 \tloss:0.005195\n",
      "Batch:\t643 /1004\t:  6.1727309227 \t4007.2986331 \tloss:0.005195\n",
      "Batch:\t644 /1004\t:  6.12473392487 \t4013.42391992 \tloss:0.005195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t645 /1004\t:  6.09649085999 \t4019.52063203 \tloss:0.005195\n",
      "Batch:\t646 /1004\t:  6.14832305908 \t4025.67012787 \tloss:0.005163\n",
      "Batch:\t647 /1004\t:  6.18088293076 \t4031.85187793 \tloss:0.005195\n",
      "Batch:\t648 /1004\t:  6.10018205643 \t4037.95294905 \tloss:0.005195\n",
      "Batch:\t649 /1004\t:  6.28775000572 \t4044.24092889 \tloss:0.005163\n",
      "Batch:\t650 /1004\t:  6.24008584023 \t4050.48159909 \tloss:0.005131\n",
      "Batch:\t651 /1004\t:  6.43125700951 \t4056.91343689 \tloss:0.005163\n",
      "Batch:\t652 /1004\t:  6.10264587402 \t4063.01806903 \tloss:0.005163\n",
      "Batch:\t653 /1004\t:  6.37452292442 \t4069.39390588 \tloss:0.005163\n",
      "Batch:\t654 /1004\t:  6.29731893539 \t4075.69258404 \tloss:0.005131\n",
      "Batch:\t655 /1004\t:  6.20227599144 \t4081.89608002 \tloss:0.005195\n",
      "Batch:\t656 /1004\t:  6.35914802551 \t4088.25690198 \tloss:0.005195\n",
      "Batch:\t657 /1004\t:  6.11828899384 \t4094.37695289 \tloss:0.005195\n",
      "Batch:\t658 /1004\t:  6.56483602524 \t4100.94306803 \tloss:0.005195\n",
      "Batch:\t659 /1004\t:  6.33607411385 \t4107.2811079 \tloss:0.005163\n",
      "Batch:\t660 /1004\t:  6.15590000153 \t4113.43780994 \tloss:0.005163\n",
      "Batch:\t661 /1004\t:  6.38708686829 \t4119.82654095 \tloss:0.005195\n",
      "Batch:\t662 /1004\t:  6.18089079857 \t4126.00876093 \tloss:0.005195\n",
      "Batch:\t663 /1004\t:  6.26275086403 \t4132.27278399 \tloss:0.005195\n",
      "Batch:\t664 /1004\t:  6.24843215942 \t4138.52240801 \tloss:0.005195\n",
      "Batch:\t665 /1004\t:  6.34944486618 \t4144.87279105 \tloss:0.005099\n",
      "Batch:\t666 /1004\t:  5.98227787018 \t4150.85689187 \tloss:0.005195\n",
      "Batch:\t667 /1004\t:  6.07483196259 \t4156.93256688 \tloss:0.005163\n",
      "Batch:\t668 /1004\t:  6.32362389565 \t4163.2575171 \tloss:0.005163\n",
      "Batch:\t669 /1004\t:  6.10913300514 \t4169.36789203 \tloss:0.005163\n",
      "Batch:\t670 /1004\t:  6.43956708908 \t4175.80883503 \tloss:0.005163\n",
      "Batch:\t671 /1004\t:  6.54232287407 \t4182.35246992 \tloss:0.005195\n",
      "Batch:\t672 /1004\t:  6.27567219734 \t4188.62905788 \tloss:0.005195\n",
      "Batch:\t673 /1004\t:  6.43831801414 \t4195.06764793 \tloss:0.005163\n",
      "Batch:\t674 /1004\t:  6.12309098244 \t4201.19183588 \tloss:0.005163\n",
      "Batch:\t675 /1004\t:  6.09611392021 \t4207.28881192 \tloss:0.005195\n",
      "Batch:\t676 /1004\t:  6.51758480072 \t4213.80662298 \tloss:0.005195\n",
      "Batch:\t677 /1004\t:  6.30611610413 \t4220.11377501 \tloss:0.005163\n",
      "Batch:\t678 /1004\t:  6.07980203629 \t4226.19514394 \tloss:0.005099\n",
      "Batch:\t679 /1004\t:  6.10385894775 \t4232.30020094 \tloss:0.005195\n",
      "Batch:\t680 /1004\t:  6.12058901787 \t4238.42105889 \tloss:0.005195\n",
      "Batch:\t681 /1004\t:  6.59808087349 \t4245.02039003 \tloss:0.005163\n",
      "Batch:\t682 /1004\t:  6.08642482758 \t4251.10744405 \tloss:0.005195\n",
      "Batch:\t683 /1004\t:  6.09079194069 \t4257.199476 \tloss:0.005163\n",
      "Batch:\t684 /1004\t:  6.58311700821 \t4263.78318596 \tloss:0.005195\n",
      "Batch:\t685 /1004\t:  6.41027402878 \t4270.19453788 \tloss:0.005163\n",
      "Batch:\t686 /1004\t:  6.22669196129 \t4276.42233396 \tloss:0.005163\n",
      "Batch:\t687 /1004\t:  6.39127397537 \t4282.81472397 \tloss:0.005195\n",
      "Batch:\t688 /1004\t:  6.24409985542 \t4289.06073904 \tloss:0.005195\n",
      "Batch:\t689 /1004\t:  6.17783403397 \t4295.23915505 \tloss:0.005131\n",
      "Batch:\t690 /1004\t:  6.58416104317 \t4301.82520008 \tloss:0.005195\n",
      "Batch:\t691 /1004\t:  6.40150117874 \t4308.22800589 \tloss:0.005195\n",
      "Batch:\t692 /1004\t:  6.22314691544 \t4314.45230794 \tloss:0.005195\n",
      "Batch:\t693 /1004\t:  6.0965089798 \t4320.5499661 \tloss:0.005163\n",
      "Batch:\t694 /1004\t:  6.13365197182 \t4326.68386292 \tloss:0.005131\n",
      "Batch:\t695 /1004\t:  6.59838008881 \t4333.28357005 \tloss:0.005163\n",
      "Batch:\t696 /1004\t:  6.88380813599 \t4340.16929102 \tloss:0.005163\n",
      "Batch:\t697 /1004\t:  6.95860219002 \t4347.12958503 \tloss:0.005163\n",
      "Batch:\t698 /1004\t:  6.78356790543 \t4353.91503692 \tloss:0.005195\n",
      "Batch:\t699 /1004\t:  6.48491406441 \t4360.40039206 \tloss:0.005195\n",
      "Batch:\t700 /1004\t:  6.61543393135 \t4367.01679707 \tloss:0.005195\n",
      "Batch:\t701 /1004\t:  6.726115942 \t4373.74410892 \tloss:0.005163\n",
      "Batch:\t702 /1004\t:  6.54371595383 \t4380.2891469 \tloss:0.005195\n",
      "Batch:\t703 /1004\t:  6.81716799736 \t4387.10783792 \tloss:0.005131\n",
      "Batch:\t704 /1004\t:  6.75235700607 \t4393.86169505 \tloss:0.005195\n",
      "Batch:\t705 /1004\t:  6.61171793938 \t4400.47479606 \tloss:0.005195\n",
      "Batch:\t706 /1004\t:  6.33726215363 \t4406.81292009 \tloss:0.005195\n",
      "Batch:\t707 /1004\t:  6.6405620575 \t4413.45711207 \tloss:0.005195\n",
      "Batch:\t708 /1004\t:  6.35344600677 \t4419.81182504 \tloss:0.005163\n",
      "Batch:\t709 /1004\t:  6.84477090836 \t4426.65788507 \tloss:0.005131\n",
      "Batch:\t710 /1004\t:  6.63705897331 \t4433.29653907 \tloss:0.005195\n",
      "Batch:\t711 /1004\t:  6.84511208534 \t4440.14345789 \tloss:0.005131\n",
      "Batch:\t712 /1004\t:  6.60607600212 \t4446.75012398 \tloss:0.005163\n",
      "Batch:\t713 /1004\t:  6.56059789658 \t4453.31257296 \tloss:0.005131\n",
      "Batch:\t714 /1004\t:  6.44912910461 \t4459.76291609 \tloss:0.005195\n",
      "Batch:\t715 /1004\t:  6.49503302574 \t4466.25932288 \tloss:0.005195\n",
      "Batch:\t716 /1004\t:  6.43859410286 \t4472.69837904 \tloss:0.005195\n",
      "Batch:\t717 /1004\t:  6.75954985619 \t4479.45897508 \tloss:0.005163\n",
      "Batch:\t718 /1004\t:  6.48284101486 \t4485.94316387 \tloss:0.005163\n",
      "Batch:\t719 /1004\t:  6.7687690258 \t4492.71372008 \tloss:0.005163\n",
      "Batch:\t720 /1004\t:  6.89286208153 \t4499.60769701 \tloss:0.005163\n",
      "Batch:\t721 /1004\t:  6.90731096268 \t4506.51610899 \tloss:0.005163\n",
      "Batch:\t722 /1004\t:  6.94739580154 \t4513.46485901 \tloss:0.005099\n",
      "Batch:\t723 /1004\t:  6.58747601509 \t4520.05276299 \tloss:0.005195\n",
      "Batch:\t724 /1004\t:  6.50864100456 \t4526.56270909 \tloss:0.005195\n",
      "Batch:\t725 /1004\t:  6.96080708504 \t4533.5238719 \tloss:0.005195\n",
      "Batch:\t726 /1004\t:  6.6851131916 \t4540.21022797 \tloss:0.005195\n",
      "Batch:\t727 /1004\t:  6.57942795753 \t4546.79073691 \tloss:0.005195\n",
      "Batch:\t728 /1004\t:  6.63541913033 \t4553.42661905 \tloss:0.005195\n",
      "Batch:\t729 /1004\t:  6.69428300858 \t4560.122087 \tloss:0.005195\n",
      "Batch:\t730 /1004\t:  6.6978700161 \t4566.82091594 \tloss:0.005067\n",
      "Batch:\t731 /1004\t:  6.75773191452 \t4573.58044505 \tloss:0.005195\n",
      "Batch:\t732 /1004\t:  6.29083585739 \t4579.87171388 \tloss:0.005195\n",
      "Batch:\t733 /1004\t:  6.73820090294 \t4586.61146188 \tloss:0.005195\n",
      "Batch:\t734 /1004\t:  7.05625796318 \t4593.66953301 \tloss:0.005195\n",
      "Batch:\t735 /1004\t:  6.72525596619 \t4600.39609599 \tloss:0.005163\n",
      "Batch:\t736 /1004\t:  6.4831609726 \t4606.8808949 \tloss:0.005195\n",
      "Batch:\t737 /1004\t:  6.76464676857 \t4613.64662695 \tloss:0.005195\n",
      "Batch:\t738 /1004\t:  6.70104408264 \t4620.34931087 \tloss:0.005195\n",
      "Batch:\t739 /1004\t:  6.44684410095 \t4626.7975781 \tloss:0.005195\n",
      "Batch:\t740 /1004\t:  6.45287489891 \t4633.25204897 \tloss:0.005195\n",
      "Batch:\t741 /1004\t:  6.24317097664 \t4639.49652791 \tloss:0.005195\n",
      "Batch:\t742 /1004\t:  6.55342721939 \t4646.05132294 \tloss:0.005163\n",
      "Batch:\t743 /1004\t:  6.83947515488 \t4652.89279795 \tloss:0.005195\n",
      "Batch:\t744 /1004\t:  6.41076493263 \t4659.30489397 \tloss:0.005131\n",
      "Batch:\t745 /1004\t:  7.14072704315 \t4666.44671893 \tloss:0.005195\n",
      "Batch:\t746 /1004\t:  6.55191707611 \t4672.99994707 \tloss:0.005131\n",
      "Batch:\t747 /1004\t:  6.74898219109 \t4679.75031495 \tloss:0.005195\n",
      "Batch:\t748 /1004\t:  6.20320796967 \t4685.95628691 \tloss:0.005099\n",
      "Batch:\t749 /1004\t:  6.30128312111 \t4692.25890708 \tloss:0.005195\n",
      "Batch:\t750 /1004\t:  6.94562911987 \t4699.20576692 \tloss:0.005195\n",
      "Batch:\t751 /1004\t:  6.82075095177 \t4706.02790689 \tloss:0.005195\n",
      "Batch:\t752 /1004\t:  6.68020701408 \t4712.70968509 \tloss:0.005131\n",
      "Batch:\t753 /1004\t:  6.69964504242 \t4719.4104681 \tloss:0.005195\n",
      "Batch:\t754 /1004\t:  6.38417816162 \t4725.79586196 \tloss:0.005195\n",
      "Batch:\t755 /1004\t:  6.73946404457 \t4732.53665495 \tloss:0.005163\n",
      "Batch:\t756 /1004\t:  6.53361701965 \t4739.07112789 \tloss:0.005195\n",
      "Batch:\t757 /1004\t:  6.97628808022 \t4746.04868507 \tloss:0.005195\n",
      "Batch:\t758 /1004\t:  6.71462798119 \t4752.76458693 \tloss:0.005195\n",
      "Batch:\t759 /1004\t:  6.79474902153 \t4759.55980492 \tloss:0.005195\n",
      "Batch:\t760 /1004\t:  6.74228405952 \t4766.30306101 \tloss:0.005195\n",
      "Batch:\t761 /1004\t:  7.0016450882 \t4773.30632901 \tloss:0.005163\n",
      "Batch:\t762 /1004\t:  6.79707717896 \t4780.1049819 \tloss:0.005195\n",
      "Batch:\t763 /1004\t:  6.25321507454 \t4786.35992408 \tloss:0.005131\n",
      "Batch:\t764 /1004\t:  6.78664422035 \t4793.14802599 \tloss:0.005195\n",
      "Batch:\t765 /1004\t:  6.72494506836 \t4799.87396502 \tloss:0.005195\n",
      "Batch:\t766 /1004\t:  6.68901801109 \t4806.56493592 \tloss:0.005163\n",
      "Batch:\t767 /1004\t:  6.7300760746 \t4813.29618597 \tloss:0.005195\n",
      "Batch:\t768 /1004\t:  6.80327010155 \t4820.10057807 \tloss:0.005131\n",
      "Batch:\t769 /1004\t:  6.74118995667 \t4826.8422451 \tloss:0.005195\n",
      "Batch:\t770 /1004\t:  6.98237800598 \t4833.82606006 \tloss:0.005195\n",
      "Batch:\t771 /1004\t:  6.77152395248 \t4840.59878898 \tloss:0.005163\n",
      "Batch:\t772 /1004\t:  7.13556194305 \t4847.73572707 \tloss:0.005195\n",
      "Batch:\t773 /1004\t:  7.15851688385 \t4854.89787602 \tloss:0.005195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t774 /1004\t:  6.5792889595 \t4861.47758007 \tloss:0.005195\n",
      "Batch:\t775 /1004\t:  6.78965497017 \t4868.26903605 \tloss:0.005131\n",
      "Batch:\t776 /1004\t:  6.79726696014 \t4875.0678699 \tloss:0.005195\n",
      "Batch:\t777 /1004\t:  6.56897091866 \t4881.63810587 \tloss:0.005163\n",
      "Batch:\t778 /1004\t:  6.46389508247 \t4888.10346389 \tloss:0.005195\n",
      "Batch:\t779 /1004\t:  6.71716809273 \t4894.82162499 \tloss:0.005163\n",
      "Batch:\t780 /1004\t:  6.75686907768 \t4901.57979894 \tloss:0.005195\n",
      "Batch:\t781 /1004\t:  6.48928499222 \t4908.07054996 \tloss:0.005163\n",
      "Batch:\t782 /1004\t:  6.73110389709 \t4914.80304003 \tloss:0.005131\n",
      "Batch:\t783 /1004\t:  6.68858098984 \t4921.49206495 \tloss:0.005195\n",
      "Batch:\t784 /1004\t:  6.28634881973 \t4927.779598 \tloss:0.005131\n",
      "Batch:\t785 /1004\t:  6.76571297646 \t4934.54660797 \tloss:0.005163\n",
      "Batch:\t786 /1004\t:  6.657351017 \t4941.20492196 \tloss:0.005163\n",
      "Batch:\t787 /1004\t:  6.78531599045 \t4947.99131203 \tloss:0.005163\n",
      "Batch:\t788 /1004\t:  6.15209507942 \t4954.14433503 \tloss:0.005195\n",
      "Batch:\t789 /1004\t:  6.88902902603 \t4961.03447008 \tloss:0.005195\n",
      "Batch:\t790 /1004\t:  6.95160698891 \t4967.98653507 \tloss:0.005195\n",
      "Batch:\t791 /1004\t:  6.60362792015 \t4974.59141994 \tloss:0.005195\n",
      "Batch:\t792 /1004\t:  6.79059004784 \t4981.38334394 \tloss:0.005131\n",
      "Batch:\t793 /1004\t:  6.55978298187 \t4987.94435406 \tloss:0.005195\n",
      "Batch:\t794 /1004\t:  6.44964289665 \t4994.39527893 \tloss:0.005195\n",
      "Batch:\t795 /1004\t:  6.58157515526 \t5000.97859192 \tloss:0.005131\n",
      "Batch:\t796 /1004\t:  6.3705739975 \t5007.35036588 \tloss:0.005163\n",
      "Batch:\t797 /1004\t:  6.52724909782 \t5013.87936091 \tloss:0.005131\n",
      "Batch:\t798 /1004\t:  6.54607200623 \t5020.42716002 \tloss:0.005195\n",
      "Batch:\t799 /1004\t:  7.17577505112 \t5027.60489798 \tloss:0.005195\n",
      "Batch:\t800 /1004\t:  6.58610796928 \t5034.19441605 \tloss:0.005195\n",
      "Batch:\t801 /1004\t:  6.76405119896 \t5040.95977497 \tloss:0.005131\n",
      "Batch:\t802 /1004\t:  6.4603228569 \t5047.42137289 \tloss:0.005195\n",
      "Batch:\t803 /1004\t:  6.82908511162 \t5054.25091195 \tloss:0.005163\n",
      "Batch:\t804 /1004\t:  6.51335310936 \t5060.76550198 \tloss:0.005195\n",
      "Batch:\t805 /1004\t:  6.77173280716 \t5067.53842902 \tloss:0.005195\n",
      "Batch:\t806 /1004\t:  6.52107596397 \t5074.06157207 \tloss:0.005195\n",
      "Batch:\t807 /1004\t:  6.56339120865 \t5080.62690401 \tloss:0.005163\n",
      "Batch:\t808 /1004\t:  6.17083406448 \t5086.79897499 \tloss:0.005195\n",
      "Batch:\t809 /1004\t:  6.19062685966 \t5092.99011803 \tloss:0.005195\n",
      "Batch:\t810 /1004\t:  6.06776499748 \t5099.05910587 \tloss:0.005195\n",
      "Batch:\t811 /1004\t:  6.10165882111 \t5105.16132998 \tloss:0.005195\n",
      "Batch:\t812 /1004\t:  6.1681599617 \t5111.33075595 \tloss:0.005195\n",
      "Batch:\t813 /1004\t:  6.02743411064 \t5117.35882092 \tloss:0.005195\n",
      "Batch:\t814 /1004\t:  6.11986589432 \t5123.47922707 \tloss:0.005195\n",
      "Batch:\t815 /1004\t:  6.437032938 \t5129.91650796 \tloss:0.005195\n",
      "Batch:\t816 /1004\t:  6.1656999588 \t5136.0824461 \tloss:0.005195\n",
      "Batch:\t817 /1004\t:  6.09905385971 \t5142.18311095 \tloss:0.005195\n",
      "Batch:\t818 /1004\t:  6.26633191109 \t5148.4499681 \tloss:0.005195\n",
      "Batch:\t819 /1004\t:  6.30454301834 \t5154.75585508 \tloss:0.005034\n",
      "Batch:\t820 /1004\t:  6.08671402931 \t5160.84407496 \tloss:0.005163\n",
      "Batch:\t821 /1004\t:  6.25946712494 \t5167.10377693 \tloss:0.005195\n",
      "Batch:\t822 /1004\t:  6.10190796852 \t5173.20748687 \tloss:0.005195\n",
      "Batch:\t823 /1004\t:  6.17877006531 \t5179.38668489 \tloss:0.005195\n",
      "Batch:\t824 /1004\t:  6.35087990761 \t5185.73924088 \tloss:0.005195\n",
      "Batch:\t825 /1004\t:  6.04286313057 \t5191.78307509 \tloss:0.005195\n",
      "Batch:\t826 /1004\t:  6.44994211197 \t5198.23431492 \tloss:0.005163\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Cell which pulls everything together.\n",
    "\n",
    "    > init models\n",
    "    > get data prepared\n",
    "    > pass models and data to training loop\n",
    "    > gets trained models and loss\n",
    "    > saves models\n",
    "    > visualizes loss?\n",
    "\n",
    "No other function but this one ever sees global macros!\n",
    "\"\"\"\n",
    "macros = {\n",
    "    \"ques_len\": QUES_LEN,\n",
    "    \"hidden_dim\": HIDDEN_DIM, \n",
    "    \"vocab_size\": VOCAB_SIZE, \n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"para_len\": PARA_LEN,\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"debug\": 1\n",
    "} \n",
    "\n",
    "data = {'train':{}, 'test':{}}\n",
    "data['train']['P'], data['train']['Q'], data['train']['Ys'], data['train']['Ye'], \\\n",
    "data['test']['P'], data['test']['Q'], data['test']['Ys'], data['test']['Ye'] = \\\n",
    "    prepare_data(DATA_LOC, macros)\n",
    "\n",
    "# # Instantiate models\n",
    "ques_model = Encoder(QUES_LEN, macros, glove_file).cuda(device)\n",
    "para_model = Encoder(PARA_LEN, macros, glove_file).cuda(device)\n",
    "match_LSTM_encoder_model = MatchLSTMEncoder(macros).cuda(device)\n",
    "pointer_decoder_model = PointerDecoder(macros).cuda(device)\n",
    "\n",
    "# # Instantiate models\n",
    "# ques_model = Encoder(QUES_LEN, macros, glove_file)\n",
    "# para_model = Encoder(PARA_LEN, macros, glove_file)\n",
    "# match_LSTM_encoder_model = MatchLSTMEncoder(macros)\n",
    "# pointer_decoder_model = PointerDecoder(macros)\n",
    "\n",
    "op = training_loop(_models=[ques_model, para_model, match_LSTM_encoder_model, pointer_decoder_model],\n",
    "                       _data=data,\n",
    "                       _debug=macros['debug'],\n",
    "                      _save_best=True,\n",
    "#                           _test_every=TEST_EVERY_,\n",
    "                       _test_every=0,\n",
    "                      _epochs=EPOCHS,\n",
    "                      _macros=macros)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.005162738263607025,\n",
       "  0.005034471862018108,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005066538229584694,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005066538229584694,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005066538229584694,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005034471862018108,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005098605062812567,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005066538229584694,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.00513067189604044,\n",
       "  0.005098605062812567,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005194805096834898,\n",
       "  0.005162738263607025,\n",
       "  0.005194805096834898,\n",
       "  ...]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "\n",
    "So far, we plot the training losss. \n",
    "Shall we superimpose test loss on it too? We don't calculate test loss per batch though (fortunately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "print(\"Training Loss\")\n",
    "visualize_loss(op[0], \"train loss\", _only_epoch=False)\n",
    "\n",
    "if len(op[1]) > 0:\n",
    "\n",
    "    print(\"Validation Loss\")\n",
    "    visualize_loss(op[1], \"validation loss\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "    Scripts for working with MatchLSTM's\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Macros \n",
    "# DATA_LOC = './data/squad/'\n",
    "DATA_LOC = ''\n",
    "EMBEDDING_FILE = 'glove.trimmed.300.npz'\n",
    "VOCAB_FILE = 'vocab.dat'\n",
    "\n",
    "file_loc = DATA_LOC + EMBEDDING_FILE\n",
    "glove_file = np.load(open(file_loc))['glove']\n",
    "vocab = open(DATA_LOC+VOCAB_FILE)\n",
    "vocab = vocab.readlines()\n",
    "vocab = [v[:-1] for v in vocab]\n",
    "\n",
    "assert(len(vocab) == len(glove_file))\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "for index,v in enumerate(vocab):\n",
    "    word_to_id[v] = index\n",
    "    id_to_word[index] = v\n",
    "\n",
    "#Now word_to_index and index_to_word (vocab)\n",
    "\n",
    "def tokenize(sequence):\n",
    "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"') for token in nltk.word_tokenize(sequence)]\n",
    "    return map(lambda x:x.encode('utf8'), tokens)\n",
    "text = 'Blah b;lah b;;ha'\n",
    "\n",
    "\n",
    "\n",
    "def text_to_id(text):\n",
    "    text_tokens = tokenize(text)\n",
    "    id_tokens = []\n",
    "    for t in text_tokens:\n",
    "        try:\n",
    "            id_tokens.append(word_to_id[t])\n",
    "        except KeyError:\n",
    "            id_tokens.append(word_to_id['<unk>'])\n",
    "    return id_tokens\n",
    "\n",
    "def id_to_text(text_id):\n",
    "    text = []\n",
    "    for t in text_id:\n",
    "        text.append(id_to_word[t])\n",
    "    return text\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
