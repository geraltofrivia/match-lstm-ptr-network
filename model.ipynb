{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA over unstructured data\n",
    "\n",
    "Using Match LSTM, Pointer Networks, as mentioned in paper https://arxiv.org/pdf/1608.07905.pdf\n",
    "\n",
    "We start with the pre-processing provided by https://github.com/MurtyShikhar/Question-Answering to clean up the data and make neat para, ques files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 9])\n",
      "torch.Size([12, 4])\n"
     ]
    }
   ],
   "source": [
    "# Macros \n",
    "DATA_LOC = './data/squad/'\n",
    "DEBUG = True\n",
    "\n",
    "# nn Macros\n",
    "QUES_LEN, PARA_LEN =  4, 9\n",
    "VOCAB_SIZE = 3000                    # @TODO: get actual size\n",
    "HIDDEN_DIM = 10\n",
    "EMBEDDING_DIM = 30\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "\n",
    "dummy_para = torch.randint(0,VOCAB_SIZE-1,(PARA_LEN*BATCH_SIZE,)).view(BATCH_SIZE,PARA_LEN).long()\n",
    "print (dummy_para.shape)\n",
    "dummy_question = torch.randint(0,VOCAB_SIZE-1,(QUES_LEN*BATCH_SIZE,)).view(BATCH_SIZE,QUES_LEN).long()\n",
    "print (dummy_question.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder \n",
    "Use a simple lstm class to have encoder for question and paragraph. \n",
    "The output of these will be used in the match lstm\n",
    "\n",
    "$H^p = LSTM(P)$ \n",
    "\n",
    "\n",
    "$H^q = LSTM(Q)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM with batches\n",
      "x:  torch.Size([12, 4])\n",
      "h:  torch.Size([1, 12, 10]) torch.Size([1, 12, 10])\n",
      "x_embedded:  torch.Size([12, 4, 30])\n",
      "ycap:  torch.Size([4, 12, 10])\n",
      "x:  torch.Size([12, 9])\n",
      "h:  torch.Size([1, 12, 10]) torch.Size([1, 12, 10])\n",
      "x_embedded:  torch.Size([12, 9, 30])\n",
      "ycap:  torch.Size([9, 12, 10])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputlen, hiddendim, embeddingdim, vocablen):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Catch dim\n",
    "        self.inputlen, self.hiddendim, self.embeddingdim, self.vocablen = inputlen, hiddendim, embeddingdim, vocablen\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(self.vocablen, self.embeddingdim)\n",
    "       \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(self.embeddingdim, self.hiddendim)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        # Returns a new hidden layer var for LSTM\n",
    "        return (torch.zeros(1, BATCH_SIZE, self.hiddendim), torch.zeros(1, BATCH_SIZE, self.hiddendim))\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        \n",
    "        # Input: x (1, batch, ) (current input)\n",
    "        # Hidden: h (1, batch, hiddendim) (last hidden state)\n",
    "        \n",
    "        if DEBUG: print(\"x: \", x.shape)\n",
    "        if DEBUG: print(\"h: \", h[0].shape, h[1].shape)\n",
    "        \n",
    "        x_emb = self.embedding(x)\n",
    "        if DEBUG: print(\"x_embedded: \", x_emb.shape)\n",
    "            \n",
    "        ycap, h = self.lstm(x_emb.view(-1, BATCH_SIZE, self.embeddingdim), h)\n",
    "        if DEBUG: print(\"ycap: \", ycap.shape)\n",
    "        \n",
    "        return ycap, h\n",
    "    \n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     print (\"Trying out question encoder LSTM\")\n",
    "#     model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "#     dummy_x = torch.tensor([22,45,12], dtype=torch.long)\n",
    "#     hidden = model.init_hidden()\n",
    "#     ycap, h = model(dummy_x, hidden)\n",
    "    \n",
    "#     print(ycap.shape)\n",
    "#     print(h[0].shape, h[1].shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"LSTM with batches\")\n",
    "    ques_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "    para_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "    ques_hidden = ques_model.init_hidden()\n",
    "    para_hidden = para_model.init_hidden()\n",
    "    ques_embedded,hidden_ques = ques_model(dummy_question,ques_hidden)\n",
    "    para_embedded,hidden_para = para_model(dummy_para,para_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 12, 10])\n",
      "torch.Size([9, 12, 10])\n",
      "torch.Size([1, 12, 10]) torch.Size([1, 12, 10])\n"
     ]
    }
   ],
   "source": [
    "print (ques_embedded.shape) # question_length,batch,embedding_dim\n",
    "print (para_embedded.shape) # para_length,batch,embedding_dim\n",
    "print (hidden_para[0].shape,hidden_para[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# QuesEncoder = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "# ParaEncoder = Encoder(PARA_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match LSTM\n",
    "\n",
    "Use a match LSTM to compute a **summarized sequential vector** for the paragraph w.r.t the question.\n",
    "\n",
    "Consider the summarized vector ($H^r$) as the output of a new decoder, where the inputs are $H^p, H^q$ computed above. \n",
    "\n",
    "1. Attend the para word $i$ with the entire question ($H^q$)\n",
    "  \n",
    "    1. $\\vec{G}_i = tanh(W^qH^q + repeat(W^ph^p_i + W^r\\vec{h^r_{i-1} + b^p}))$\n",
    "    \n",
    "    2. *Computing it*: Here, $\\vec{G}_i$ is equivalent to `energy`, computed differently.\n",
    "    \n",
    "    3. Use a linear layer to compute the content within the $repeat$ fn.\n",
    "    \n",
    "    4. Add with another linear (without bias) with $H_q$\n",
    "    \n",
    "    5. $tanh$ the bloody thing\n",
    "  \n",
    "  \n",
    "2. Softmax over it to get $\\alpha$ weights.\n",
    "\n",
    "    1. $\\vec{\\alpha_i} = softmax(w^t\\vec{G}_i + repeat(b))$\n",
    "    \n",
    "3. Use the attention weight vector $\\vec{\\alpha_i}$ to obtain a weighted version of the question and combine it with the current token of the passage to form a vector $\\vec{z_i}$\n",
    "\n",
    "[@TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init h_ri shape is:  torch.Size([1, 12, 10])\n",
      "the para length is  9\n",
      "h_pi: \t\t   torch.Size([1, 12, 10])\n",
      "h_ri: \t\t   torch.Size([1, 12, 10])\n",
      "H_q: \t\t   torch.Size([4, 12, 10])\n",
      "lin_repeat_input:  torch.Size([1, 12, 20])\n",
      "lin_g_input_b unrepeated:  torch.Size([1, 12, 10])\n",
      "lin_g_input_b: \t torch.Size([4, 12, 10])\n",
      "lin_g_input_a:  torch.Size([4, 12, 10])\n",
      "G_i:  torch.Size([4, 12, 10])\n",
      "alpha_i_input_a:  torch.Size([12, 1, 4])\n",
      "alpha_i_input:  torch.Size([12, 1, 4])\n",
      "alpha_i:  torch.Size([12, 1, 4])\n",
      "z_i_input_b:  torch.Size([4, 12, 10])\n",
      "z_i:  torch.Size([5, 12, 10])\n",
      "lstm_input:  torch.Size([1, 12, 60])\n",
      "h_ri new:  torch.Size([1, 12, 10])\n",
      "hidden new:  torch.Size([1, 12, 10]) torch.Size([1, 12, 10])\n"
     ]
    }
   ],
   "source": [
    "class MatchLSTMEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, ques_len ):\n",
    "        \n",
    "        super(MatchLSTMEncoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim, self.ques_len = hidden_dim, ques_len\n",
    "        \n",
    "        # Catch lens and params\n",
    "        self.lin_g_repeat = nn.Linear(2*self.hidden_dim, hidden_dim)\n",
    "        self.lin_g_nobias = nn.Linear(self.hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.alpha_i_w = nn.Parameter(torch.FloatTensor(self.hidden_dim, 1))\n",
    "        self.alpha_i_b= nn.Parameter(torch.FloatTensor((1)))\n",
    "        \n",
    "        self.lstm_summary = nn.LSTM(self.hidden_dim*(self.ques_len+2), self.hidden_dim)\n",
    "                                      \n",
    "    \n",
    "    def forward(self, h_pi, h_ri, H_q, hidden):\n",
    "        \n",
    "        # For h_r i\n",
    "        # encoded para word: h_pi (1, batch, hidden_dim  )\n",
    "        # encoded ques all: H_q (seqlen, batch, hidden_dim)\n",
    "        # last hidden state: h_ri (1, batch, hidden_dim) (i-1th) \n",
    "        \n",
    "        if DEBUG:\n",
    "            print( \"h_pi: \\t\\t  \", h_pi.shape)\n",
    "            print( \"h_ri: \\t\\t  \", h_ri.shape)\n",
    "            print( \"H_q: \\t\\t  \", H_q.shape)\n",
    "        \n",
    "        lin_repeat_input = torch.cat((h_pi, h_ri), dim=2)\n",
    "        if DEBUG: print(\"lin_repeat_input: \", lin_repeat_input.shape)\n",
    "        \n",
    "        lin_g_input_b = self.lin_g_repeat(lin_repeat_input)\n",
    "        if DEBUG: print(\"lin_g_input_b unrepeated: \", lin_g_input_b.shape)\n",
    "            \n",
    "        lin_g_input_b = lin_g_input_b.repeat(H_q.shape[0], 1, 1)\n",
    "        if DEBUG: print(\"lin_g_input_b: \\t\", lin_g_input_b.shape)\n",
    "            \n",
    "        # lin_g_input_a = self.lin_g_nobias.matmul(H_q.view(-1, self.ques_len, self.hidden_dim)) #self.lin_g_nobias(H_q)\n",
    "        lin_g_input_a =  self.lin_g_nobias(H_q)\n",
    "        if DEBUG: print(\"lin_g_input_a: \", lin_g_input_a.shape)\n",
    "            \n",
    "        G_i = F.tanh(lin_g_input_a + lin_g_input_b)\n",
    "        if DEBUG: print(\"G_i: \", G_i.shape)\n",
    "        # Note; G_i should be a 1D vector over ques_len\n",
    "        \n",
    "        # Attention weights\n",
    "        alpha_i_input_a = G_i.view(BATCH_SIZE, -1, self.hidden_dim).matmul(self.alpha_i_w).view(BATCH_SIZE, 1, -1)\n",
    "        if DEBUG: print(\"alpha_i_input_a: \", alpha_i_input_a.shape)\n",
    "            \n",
    "        alpha_i_input = alpha_i_input_a.add_(self.alpha_i_b.view(-1,1,1).repeat(1,1,self.ques_len))\n",
    "        if DEBUG: print(\"alpha_i_input: \", alpha_i_input.shape)\n",
    "        \n",
    "        # Softmax over alpha inputs\n",
    "        alpha_i = F.softmax(alpha_i_input, dim=-1)\n",
    "        if DEBUG: print(\"alpha_i: \", alpha_i.shape)\n",
    "            \n",
    "        # Weighted summary of question with alpha    \n",
    "        z_i_input_b = (\n",
    "                        H_q.view(BATCH_SIZE, QUES_LEN, -1) *\n",
    "                       (alpha_i.view(BATCH_SIZE, self.ques_len, -1).repeat(1,1,self.hidden_dim))\n",
    "                      ).view(self.ques_len,BATCH_SIZE,-1)\n",
    "        if DEBUG: print(\"z_i_input_b: \", z_i_input_b.shape)\n",
    "            \n",
    "        z_i = torch.cat((h_pi, z_i_input_b), dim=0)\n",
    "        if DEBUG: print(\"z_i: \", z_i.shape)\n",
    "                        \n",
    "        # Pass z_i, h_ri to the LSTM \n",
    "        lstm_input = torch.cat((z_i.view(1,BATCH_SIZE,-1), h_ri), dim=2)\n",
    "        if DEBUG: print(\"lstm_input: \", lstm_input.shape)\n",
    "        \n",
    "        h_ri, hidden = self.lstm_summary(lstm_input, hidden)\n",
    "        if DEBUG:\n",
    "            print(\"h_ri new: \", h_ri.shape)\n",
    "            print(\"hidden new: \", hidden[0].shape, hidden[1].shape)\n",
    "        \n",
    "        \n",
    "        return h_ri, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, BATCH_SIZE, self.hidden_dim),\n",
    "                torch.zeros(1, BATCH_SIZE, self.hidden_dim))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN)\n",
    "#     h_pi = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "#     h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "#     hidden = model.init_hidden()\n",
    "#     H_q = torch.randn(QUES_LEN, BATCH_SIZE, HIDDEN_DIM)\n",
    "    \n",
    "#     op, hid = model(h_pi, h_ri, H_q, hidden)\n",
    "    \n",
    "#     print(\"\\nDone:op\", op.shape)\n",
    "#     print(\"Done:hid\", hid[0].shape, hid[1].shape)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    matchLSTMEncoder = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN)\n",
    "    hidden = matchLSTMEncoder.init_hidden()\n",
    "    h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "    if DEBUG:\n",
    "        print (\"init h_ri shape is: \", h_ri.shape)\n",
    "        print (\"the para length is \", len(para_embedded))\n",
    "    for i in range(len(para_embedded)):\n",
    "        h_ri, hidden =  matchLSTMEncoder(para_embedded[i].view(1,BATCH_SIZE,-1), h_ri, ques_embedded, hidden)\n",
    "        para_embedded[i] = h_ri\n",
    "        DEBUG = False\n",
    "    DEBUG = True\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "para embedded dim are : torch.Size([9, 12, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"para embedded dim are :\",para_embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network\n",
    "\n",
    "Using a ptrnet over $H_r$ to unfold and get most probable spans.\n",
    "We use the **boundry model** to do that (predict start and end of seq).\n",
    "\n",
    "A simple energy -> softmax -> decoder. Where softmaxed energy is supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_ak: \t\t torch.Size([1, 12, 10])\n",
      "H_r: \t\t torch.Size([9, 12, 10])\n",
      "hidden: \t torch.Size([1, 12, 10]) torch.Size([1, 12, 10])\n",
      "f_input_b unrepeated:  torch.Size([1, 12, 10])\n",
      "f_input_b repeated:  torch.Size([9, 12, 10])\n",
      "f_input_a:  torch.Size([9, 12, 10])\n",
      "F_k:\t torch.Size([9, 12, 10])\n",
      "beta_k_input_a:  torch.Size([12, 1, 9])\n",
      "beta_k_input:  torch.Size([12, 1, 9])\n",
      "beta_k:  torch.Size([12, 1, 9])\n",
      "lstm_input_a:  torch.Size([12, 9, 10])\n",
      "lstm_input:  torch.Size([1, 12, 100])\n",
      "torch.Size([12, 1, 9])\n"
     ]
    }
   ],
   "source": [
    "class PointerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super(PointerDecoder, self).__init__()\n",
    "        \n",
    "        # Keep args\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lin_f_repeat = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.lin_f_nobias = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        \n",
    "        self.beta_k_w = nn.Parameter(torch.FloatTensor(self.hidden_dim, 1))\n",
    "        self.beta_k_b = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.hidden_dim*(PARA_LEN+1), self.hidden_dim)\n",
    "\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, h_ak, H_r, hidden):\n",
    "        \n",
    "        # h_ak (current decoder's last op) (1,batch,hiddendim)\n",
    "        # H_r (weighted summary of para) (P, batch, hiddendim)\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"h_ak: \\t\\t\", h_ak.shape)\n",
    "            print(\"H_r: \\t\\t\", H_r.shape)\n",
    "            print(\"hidden: \\t\", hidden[0].shape, hidden[1].shape)\n",
    "            \n",
    "        # Prepare inputs for the tanh used to compute energy\n",
    "        f_input_b = self.lin_f_repeat(h_ak)\n",
    "        if DEBUG: print(\"f_input_b unrepeated: \", f_input_b.shape)\n",
    "        \n",
    "        #H_r shape is ([PARA_LEN, BATCHSIZE, EmbeddingDIM])\n",
    "        f_input_b = f_input_b.repeat(H_r.shape[0], 1, 1)\n",
    "        if DEBUG: print(\"f_input_b repeated: \", f_input_b.shape)\n",
    "            \n",
    "        f_input_a = self.lin_f_nobias(H_r)\n",
    "        if DEBUG: print(\"f_input_a: \", f_input_a.shape)\n",
    "            \n",
    "        # Send it off to tanh now\n",
    "        F_k = F.tanh(f_input_a+f_input_b)\n",
    "        if DEBUG: print(\"F_k:\\t\", F_k.shape) #PARA_LEN,BATCHSIZE,EmbeddingDim\n",
    "            \n",
    "        # Attention weights\n",
    "        beta_k_input_a = F_k.view(BATCH_SIZE, -1, self.hidden_dim).matmul(self.beta_k_w).view(BATCH_SIZE, 1, -1)\n",
    "        if DEBUG: print(\"beta_k_input_a: \", beta_k_input_a.shape)\n",
    "            \n",
    "        beta_k_input = beta_k_input_a.add_(self.beta_k_b.repeat(1,1,PARA_LEN))\n",
    "        if DEBUG: print(\"beta_k_input: \", beta_k_input.shape)\n",
    "            \n",
    "        beta_k = F.softmax(beta_k_input, dim=-1)\n",
    "        if DEBUG: print(\"beta_k: \", beta_k.shape)\n",
    "            \n",
    "        lstm_input_a = H_r.view(BATCH_SIZE, PARA_LEN, -1) * (beta_k.view(BATCH_SIZE, PARA_LEN, -1).repeat(1,1,self.hidden_dim))\n",
    "        if DEBUG: print(\"lstm_input_a: \", lstm_input_a.shape)\n",
    "            \n",
    "        lstm_input = torch.cat((lstm_input_a.view(1, BATCH_SIZE,-1), h_ak.view(1, BATCH_SIZE, -1)), dim=2)\n",
    "        if DEBUG: print(\"lstm_input: \", lstm_input.shape)\n",
    "        \n",
    "        h_ak, hidden = self.lstm(lstm_input, hidden)\n",
    "        \n",
    "        return h_ak, hidden, beta_k\n",
    "        \n",
    "        \n",
    "        return \"Poop\"\n",
    "    \n",
    "    \n",
    "with torch.no_grad():\n",
    "    pointerDecoder = PointerDecoder(HIDDEN_DIM)\n",
    "    h_ak = torch.randn(1,BATCH_SIZE,HIDDEN_DIM)\n",
    "#     H_r = torch.randn(PARA_LEN, BATCH_SIZE, HIDDEN_DIM)\n",
    "    pointerHidden = pointerDecoder.init_hidden()\n",
    "    h_ak, hidden, beta_k = pointerDecoder(h_ak, para_embedded, hidden)\n",
    "    print (beta_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dummy_data(batch_size,dimension,vocab_size,max_passage_length=50,max_question_length=10):\n",
    "    '''\n",
    "        Create dummy data of given batch size. If batch size is -1 then the function returns a pair of \n",
    "        passage and question\n",
    "    '''\n",
    "    #@TODO: Implement logic for batch != -1\n",
    "    min_index = 1\n",
    "    max_index = vocab_size\n",
    "    \n",
    "    if batch_size == -1:\n",
    "        passage_length = max_passage_length\n",
    "        passage_node = torch.randint(min_index,max_index,(passage_length,)).long()\n",
    "        question_length = max_question_length\n",
    "        question_node = torch.randint(min_index,max_index,(question_length,)).long()\n",
    "        answer_start_node = torch.zeros((passage_length,)).long()\n",
    "        answer_start_node[passage_length-4] = 1\n",
    "        answer_end_node = torch.zeros((passage_length,)).long()\n",
    "        answer_end_node[passage_length-1] = 1\n",
    "        return passage_node,question_node,answer_start_node,answer_end_node\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 7,  4,  2,  2,  6,  6,  5,  7,  2,  8,  7,  5,  5,  9,\n",
      "         6,  7,  5,  5,  6,  8,  3,  8,  1,  3,  2,  6,  4,  8,\n",
      "         9,  4,  6,  4,  6,  3,  1,  5,  9,  3,  8,  7,  9,  9,\n",
      "         8,  3,  5,  5,  1,  7,  1,  3]), tensor([ 7,  4,  7,  7,  1,  7,  7,  4,  5,  2]), tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  1,  0,  0,  0]), tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  1]))\n"
     ]
    }
   ],
   "source": [
    "print (create_dummy_data(-1,10,10,max_passage_length=50,max_question_length=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 9])\n",
      "torch.Size([12, 4])\n",
      "LSTM with batches\n",
      "x:  torch.Size([12, 4])\n",
      "h:  torch.Size([1, 12, 10]) torch.Size([1, 12, 10])\n",
      "x_embedded:  torch.Size([12, 4, 30])\n",
      "ycap:  torch.Size([4, 12, 10])\n",
      "x:  torch.Size([12, 9])\n",
      "h:  torch.Size([1, 12, 10]) torch.Size([1, 12, 10])\n",
      "x_embedded:  torch.Size([12, 9, 30])\n",
      "ycap:  torch.Size([9, 12, 10])\n",
      "init h_ri shape is:  torch.Size([1, 12, 10])\n",
      "the para length is  9\n",
      "h_pi: \t\t   torch.Size([1, 12, 10])\n",
      "h_ri: \t\t   torch.Size([1, 12, 10])\n",
      "H_q: \t\t   torch.Size([4, 12, 10])\n",
      "lin_repeat_input:  torch.Size([1, 12, 20])\n",
      "lin_g_input_b unrepeated:  torch.Size([1, 12, 10])\n",
      "lin_g_input_b: \t torch.Size([4, 12, 10])\n",
      "lin_g_input_a:  torch.Size([4, 12, 10])\n",
      "G_i:  torch.Size([4, 12, 10])\n",
      "alpha_i_input_a:  torch.Size([12, 1, 4])\n",
      "alpha_i_input:  torch.Size([12, 1, 4])\n",
      "alpha_i:  torch.Size([12, 1, 4])\n",
      "z_i_input_b:  torch.Size([4, 12, 10])\n",
      "z_i:  torch.Size([5, 12, 10])\n",
      "lstm_input:  torch.Size([1, 12, 60])\n",
      "h_ri new:  torch.Size([1, 12, 10])\n",
      "hidden new:  torch.Size([1, 12, 10]) torch.Size([1, 12, 10])\n",
      "h_ak: \t\t torch.Size([1, 12, 10])\n",
      "H_r: \t\t torch.Size([9, 12, 10])\n",
      "hidden: \t torch.Size([1, 12, 10]) torch.Size([1, 12, 10])\n",
      "f_input_b unrepeated:  torch.Size([1, 12, 10])\n",
      "f_input_b repeated:  torch.Size([9, 12, 10])\n",
      "f_input_a:  torch.Size([9, 12, 10])\n",
      "F_k:\t torch.Size([9, 12, 10])\n",
      "beta_k_input_a:  torch.Size([12, 1, 9])\n",
      "beta_k_input:  torch.Size([12, 1, 9])\n",
      "beta_k:  torch.Size([12, 1, 9])\n",
      "lstm_input_a:  torch.Size([12, 9, 10])\n",
      "lstm_input:  torch.Size([1, 12, 100])\n",
      "torch.Size([12, 1, 9])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "        # Macros \n",
    "    DATA_LOC = './data/squad/'\n",
    "    DEBUG = True\n",
    "    # nn Macros\n",
    "    QUES_LEN, PARA_LEN =  4, 9\n",
    "    VOCAB_SIZE = 3000                    # @TODO: get actual size\n",
    "    HIDDEN_DIM = 10\n",
    "    EMBEDDING_DIM = 30\n",
    "    BATCH_SIZE = 12\n",
    "\n",
    "\n",
    "    dummy_para = torch.randint(0,VOCAB_SIZE-1,(PARA_LEN*BATCH_SIZE,)).view(BATCH_SIZE,PARA_LEN).long()\n",
    "    print (dummy_para.shape)\n",
    "    dummy_question = torch.randint(0,VOCAB_SIZE-1,(QUES_LEN*BATCH_SIZE,)).view(BATCH_SIZE,QUES_LEN).long()\n",
    "    print (dummy_question.shape)\n",
    "    \n",
    "    print(\"LSTM with batches\")\n",
    "    ques_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "    para_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "    ques_hidden = ques_model.init_hidden()\n",
    "    para_hidden = para_model.init_hidden()\n",
    "    ques_embedded,hidden_ques = ques_model(dummy_question,ques_hidden)\n",
    "    para_embedded,hidden_para = para_model(dummy_para,para_hidden)\n",
    "    \n",
    "    \n",
    "    \n",
    "    matchLSTMEncoder = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN)\n",
    "    hidden = matchLSTMEncoder.init_hidden()\n",
    "    h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "    if DEBUG:\n",
    "        print (\"init h_ri shape is: \", h_ri.shape)\n",
    "        print (\"the para length is \", len(para_embedded))\n",
    "    for i in range(len(para_embedded)):\n",
    "        h_ri, hidden =  matchLSTMEncoder(para_embedded[i].view(1,BATCH_SIZE,-1), h_ri, ques_embedded, hidden)\n",
    "        para_embedded[i] = h_ri\n",
    "        DEBUG = False\n",
    "    DEBUG = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    pointerDecoder = PointerDecoder(HIDDEN_DIM)\n",
    "    h_ak = torch.randn(1,BATCH_SIZE,HIDDEN_DIM)\n",
    "#     H_r = torch.randn(PARA_LEN, BATCH_SIZE, HIDDEN_DIM)\n",
    "    pointerHidden = pointerDecoder.init_hidden()\n",
    "    h_ak, hidden, beta_k = pointerDecoder(h_ak, para_embedded, hidden)\n",
    "    print (beta_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(para_batch,question_batch,answer_batch_start,answer_batch_end,ques_model,para_model,ques_hidden,para_hidden,\n",
    "         match_LSTM_encoder_model,match_LSTM_encoder_hidden,\n",
    "          h_ri,pointer_decoder_model,pointer_decoder_hidden,h_ak,criterion,\n",
    "             ques_model_optim, para_model_optim,match_LSTM_encoder_model_optim,pointer_decoder_model_optim):\n",
    "    '''\n",
    "        para_batch -> paragraphs as batches/x_label_para\n",
    "        question_batch -> question as batches/x_label_question\n",
    "        answer_batch_start,answer_batch_end ->The ground truth/y_label (start and end)\n",
    "        ques_model,para_model are the objects of pre-processing LSTM layer.\n",
    "        ques_hidden, para_hidden are the init state of the pre_processing layer.\n",
    "        match_LSTM_encoder_model,match_LSTM_encoder_hidden -> MatchLSTM model and the init of hidden state\n",
    "        pointer_decoder,pointer_decoder_hidden -> PointerDecoder object and the init of hidden state.\n",
    "        criterion -> Loss function\n",
    "    '''\n",
    "    \n",
    "    #is it needed to init a new hidden every batch ?\n",
    "    #Rightnow I am assuming all the models have an inti state already initialize \n",
    "    \n",
    "    \n",
    "    #zeroing all gradients\n",
    "    ques_model_optim.zero_grad()\n",
    "    para_model_optim.zero_grad()\n",
    "    match_LSTM_encoder_model_optim.zero_grad()\n",
    "    pointer_decoder_model_optim.zero_grad()\n",
    "    \n",
    "    \n",
    "    #passing the data through LSTM pre-processing layer\n",
    "\n",
    "    ques_embedded,ques_hidden = ques_model(dummy_question,ques_hidden)\n",
    "    para_embedded,para_hidden = para_model(dummy_para,para_hidden)\n",
    "    \n",
    "    #Augmenting the paragraph embedding with attentioned weighted question\n",
    "    \n",
    "    for i in range(len(para_embedded)):\n",
    "        h_ri, match_LSTM_encoder_hidden =  match_LSTM_encoder_model(para_embedded[i].view(1,BATCH_SIZE,-1),\n",
    "                                                                    h_ri, ques_embedded, match_LSTM_encoder_hidden)\n",
    "        para_embedded[i] = h_ri\n",
    "        DEBUG = False\n",
    "    DEBUG = True\n",
    "    \n",
    "    #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "    h_ak, pointer_decoder_hidden , beta_k_start = pointer_decoder_model(h_ak, para_embedded, pointer_decoder_hidden)\n",
    "    h_ak, pointer_decoder_hidden , beta_k_end = pointer_decoder_model(h_ak, para_embedded, pointer_decoder_hidden)\n",
    "    \n",
    "    #How will we manage batches for loss.\n",
    "    loss = criterion(beta_k_start,answer_batch_start)\n",
    "    loss = loss +  criterion(beta_k_end,answer_batch_end)\n",
    "    loss.backward()\n",
    "    \n",
    "    #optimization step\n",
    "    ques_model_optim.step()\n",
    "    para_model_optim.step()\n",
    "    match_LSTM_encoder_model_optim.step()\n",
    "    pointer_decoder_model_optim.step()\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
