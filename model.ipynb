{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA over unstructured data\n",
    "\n",
    "Using Match LSTM, Pointer Networks, as mentioned in paper https://arxiv.org/pdf/1608.07905.pdf\n",
    "\n",
    "We start with the pre-processing provided by https://github.com/MurtyShikhar/Question-Answering to clean up the data and make neat para, ques files.\n",
    "\n",
    "\n",
    "### @TODOs:\n",
    "\n",
    "1. [done] _Figure out how to put in real, pre-trained embeddings in embeddings layer._\n",
    "2. [done] _Explicitly provide batch size when instantiating model_\n",
    "3. is ./val.ids.* validation set or test set?: **validation**\n",
    "4. [done:em] emInstead of test loss, calculate test acc metrics\n",
    "    1. todo: new metrics like P, R, F1\n",
    "5. Update unit test codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Codeblock to pull up embeddings. Needs to run before following imports\n",
    "# import numpy as np\n",
    "\n",
    "# # Macros \n",
    "# DATA_LOC = './data/squad/'\n",
    "# EMBEDDING_FILE = 'glove.trimmed.300.npz'\n",
    "# VOCAB_FILE = 'vocab.dat'\n",
    "\n",
    "# file_loc = DATA_LOC + EMBEDDING_FILE\n",
    "# glove_file = np.load(open(file_loc))['glove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "from io import open\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import traceback\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug Legend\n",
    "\n",
    "- 5: Print everything that goes in every tensor.\n",
    "- 4: ??\n",
    "- 3: Check every model individually\n",
    "- 2: Print things in training loops\n",
    "- 1: ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macros \n",
    "DATA_LOC = './data/squad/'\n",
    "DEBUG = 2\n",
    "\n",
    "# nn Macros\n",
    "QUES_LEN, PARA_LEN =  30, 200\n",
    "VOCAB_SIZE = 120000\n",
    "# VOCAB_SIZE = glove_file.shape[1]               # @TODO: get actual size\n",
    "HIDDEN_DIM = 150\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 20                  # Might have total 100 batches.\n",
    "EPOCHS = 300\n",
    "TEST_EVERY_ = 1\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder \n",
    "Use a simple lstm class to have encoder for question and paragraph. \n",
    "The output of these will be used in the match lstm\n",
    "\n",
    "$H^p = LSTM(P)$ \n",
    "\n",
    "\n",
    "$H^q = LSTM(Q)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputlen, macros, glove_file):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Catch dim\n",
    "        self.inputlen = inputlen\n",
    "        self.hiddendim = macros['hidden_dim']\n",
    "        self.embeddingdim =  macros['embedding_dim']\n",
    "        self.vocablen = macros['vocab_size']\n",
    "        \n",
    "        self.batch_size = macros['batch_size']\n",
    "        self.debug = macros['debug']\n",
    "        \n",
    "        # Embedding Layer\n",
    "#         self.embedding = nn.Embedding(self.vocablen, self.embeddingdim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(glove_file))\n",
    "        self.embedding.weight.requires_grad = True\n",
    "       \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(self.embeddingdim, self.hiddendim)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        # Returns a new hidden layer var for LSTM\n",
    "        return (torch.zeros((1, batch_size, self.hiddendim), device=device), \n",
    "                torch.zeros((1, batch_size, self.hiddendim), device=device))\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        \n",
    "        # Input: x (batch, len ) (current input)\n",
    "        # Hidden: h (1, batch, hiddendim) (last hidden state)\n",
    "        \n",
    "        # Batchsize: b int (inferred)\n",
    "        b = x.shape[0]\n",
    "        \n",
    "        if self.debug > 4: print(\"x:\\t\", x.shape)\n",
    "        if self.debug > 4: print(\"h:\\t\", h[0].shape, h[1].shape)\n",
    "        \n",
    "        x_emb = self.embedding(x)\n",
    "        if self.debug > 4: print(\"x_emb:\\t\", x_emb.shape)\n",
    "            \n",
    "        ycap, h = self.lstm(x_emb.view(-1, b, self.embeddingdim), h)\n",
    "        if self.debug > 4: print(\"ycap:\\t\", ycap.shape)\n",
    "        \n",
    "        return ycap, h\n",
    "    \n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     print (\"Trying out question encoder LSTM\")\n",
    "#     model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "#     dummy_x = torch.tensor([22,45,12], dtype=torch.long)\n",
    "#     hidden = model.init_hidden()\n",
    "#     ycap, h = model(dummy_x, hidden)\n",
    "    \n",
    "#     print(ycap.shape)\n",
    "#     print(h[0].shape, h[1].shape)\n",
    "\n",
    "\n",
    "if DEBUG > 2:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        dummy_para = torch.randint(0,VOCAB_SIZE-1,(PARA_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,PARA_LEN).long()\n",
    "    #     print (dummy_para.shape)\n",
    "        dummy_question = torch.randint(0,VOCAB_SIZE-1,(QUES_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,QUES_LEN).long()\n",
    "    #     print (dummy_question.shape)\n",
    "\n",
    "    #     print(\"LSTM with batches\")\n",
    "        ques_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE).cuda(device)\n",
    "        para_model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE).cuda(device)\n",
    "        ques_hidden = ques_model.init_hidden()\n",
    "        para_hidden = para_model.init_hidden()\n",
    "        ques_embedded,hidden_ques = ques_model(dummy_question,ques_hidden)\n",
    "        para_embedded,hidden_para = para_model(dummy_para,para_hidden)\n",
    "        \n",
    "        print (ques_embedded.shape) # question_length,batch,embedding_dim\n",
    "        print (para_embedded.shape) # para_length,batch,embedding_dim\n",
    "        print (hidden_para[0].shape,hidden_para[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match LSTM\n",
    "\n",
    "Use a match LSTM to compute a **summarized sequential vector** for the paragraph w.r.t the question.\n",
    "\n",
    "Consider the summarized vector ($H^r$) as the output of a new decoder, where the inputs are $H^p, H^q$ computed above. \n",
    "\n",
    "1. Attend the para word $i$ with the entire question ($H^q$)\n",
    "  \n",
    "    1. $\\vec{G}_i = tanh(W^qH^q + repeat(W^ph^p_i + W^r\\vec{h^r_{i-1} + b^p}))$\n",
    "    \n",
    "    2. *Computing it*: Here, $\\vec{G}_i$ is equivalent to `energy`, computed differently.\n",
    "    \n",
    "    3. Use a linear layer to compute the content within the $repeat$ fn.\n",
    "    \n",
    "    4. Add with another linear (without bias) with $H_q$\n",
    "    \n",
    "    5. $tanh$ the bloody thing\n",
    "  \n",
    "  \n",
    "2. Softmax over it to get $\\alpha$ weights.\n",
    "\n",
    "    1. $\\vec{\\alpha_i} = softmax(w^t\\vec{G}_i + repeat(b))$\n",
    "    \n",
    "3. Use the attention weight vector $\\vec{\\alpha_i}$ to obtain a weighted version of the question and concat it with the current token of the passage to form a vector $\\vec{z_i}$\n",
    "\n",
    "4. Use $\\vec{z_i}$ to compute the desired $h^r_i$:\n",
    "\n",
    "    1. $ h^r_i = LSTM(\\vec{z_i}, h^r_{i-1}) $\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchLSTMEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, macros):\n",
    "        \n",
    "        super(MatchLSTMEncoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = macros['hidden_dim']\n",
    "        self.ques_len = macros['ques_len']\n",
    "        self.batch_size = macros['batch_size']\n",
    "        self.debug = macros['debug']    \n",
    "        \n",
    "        # Catch lens and params\n",
    "        self.lin_g_repeat = nn.Linear(2*self.hidden_dim, self.hidden_dim)\n",
    "        self.lin_g_nobias = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        self.alpha_i_w = nn.Parameter(torch.FloatTensor(self.hidden_dim, 1))\n",
    "        self.alpha_i_b = nn.Parameter(torch.FloatTensor((1)))\n",
    "        \n",
    "        self.lstm_summary = nn.LSTM(self.hidden_dim*(self.ques_len+2), self.hidden_dim)\n",
    "                                      \n",
    "    \n",
    "    def forward(self, H_p, h_ri, H_q, hidden):\n",
    "        \"\"\"\n",
    "            Ideally, we would have manually unrolled the lstm \n",
    "            but due to memory constraints, we do it in the module.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find the batchsize\n",
    "        batch_size = H_p.shape[1]\n",
    "        \n",
    "        H_r = torch.empty((0, batch_size, self.hidden_dim), device=device, dtype=torch.float)\n",
    "        H_r = torch.cat((H_r, h_ri), dim=0)\n",
    "        \n",
    "        if self.debug > 4:\n",
    "            print( \"H_p:\\t\\t\\t\", H_p.shape)\n",
    "            print( \"h_ri:\\t\\t\\t\", h_ri.shape)\n",
    "            print( \"H_q:\\t\\t\\t\", H_q.shape)\n",
    "        \n",
    "        for i in range(H_p.shape[0]):\n",
    "            \n",
    "            lin_repeat_input = torch.cat((H_p[i].view(1, batch_size, -1), H_r[i].view(1, batch_size, -1)), dim=2)\n",
    "            if self.debug > 4: print(\"lin_repeat_input:\\t\", lin_repeat_input.shape)\n",
    "\n",
    "            lin_g_input_b = self.lin_g_repeat(lin_repeat_input)\n",
    "            if self.debug > 4: print(\"lin_g_input_b unrepeated:\", lin_g_input_b.shape)\n",
    "\n",
    "            lin_g_input_b = lin_g_input_b.repeat(H_q.shape[0], 1, 1)\n",
    "            if self.debug > 4: print(\"lin_g_input_b:\\t\\t\", lin_g_input_b.shape)\n",
    "\n",
    "            # lin_g_input_a = self.lin_g_nobias.matmul(H_q.view(-1, self.ques_len, self.hidden_dim)) #self.lin_g_nobias(H_q)\n",
    "            lin_g_input_a =  self.lin_g_nobias(H_q)\n",
    "            if self.debug > 4: print(\"lin_g_input_a:\\t\\t\", lin_g_input_a.shape)\n",
    "\n",
    "            G_i = F.tanh(lin_g_input_a + lin_g_input_b)\n",
    "            if self.debug > 4: print(\"G_i:\\t\\t\\t\", G_i.shape)\n",
    "            # Note; G_i should be a 1D vector over ques_len\n",
    "\n",
    "            # Attention weights\n",
    "            alpha_i_input_a = G_i.view(batch_size, -1, self.hidden_dim).matmul(self.alpha_i_w).view(batch_size, 1, -1)\n",
    "            if self.debug > 4: print(\"alpha_i_input_a:\\t\", alpha_i_input_a.shape)\n",
    "\n",
    "            alpha_i_input = alpha_i_input_a.add_(self.alpha_i_b.view(-1,1,1).repeat(1,1,self.ques_len))\n",
    "            if self.debug > 4: print(\"alpha_i_input:\\t\\t\", alpha_i_input.shape)\n",
    "\n",
    "            # Softmax over alpha inputs\n",
    "            alpha_i = F.softmax(alpha_i_input, dim=-1)\n",
    "            if self.debug > 4: print(\"alpha_i:\\t\\t\", alpha_i.shape)\n",
    "\n",
    "            # Weighted summary of question with alpha    \n",
    "            z_i_input_b = (\n",
    "                            H_q.view(batch_size, self.ques_len, -1) *\n",
    "                           (alpha_i.view(batch_size, self.ques_len, -1).repeat(1, 1, self.hidden_dim))\n",
    "                          ).view(self.ques_len,batch_size, -1)\n",
    "            if self.debug > 4: print(\"z_i_input_b:\\t\\t\", z_i_input_b.shape)\n",
    "\n",
    "            z_i = torch.cat((H_p[i].view(1, batch_size, -1), z_i_input_b), dim=0)\n",
    "            if self.debug > 4: print(\"z_i:\\t\\t\\t\", z_i.shape)\n",
    "\n",
    "            # Pass z_i, h_ri to the LSTM \n",
    "            lstm_input = torch.cat((z_i.view(1, batch_size,-1), H_r[i].view(1, batch_size, -1)), dim=2)\n",
    "            if self.debug > 4: print(\"lstm_input:\\t\\t\", lstm_input.shape)\n",
    "\n",
    "            # Take input from LSTM, concat in H_r and nullify the temp var.\n",
    "            h_ri, hidden = self.lstm_summary(lstm_input, hidden)\n",
    "            H_r = torch.cat((H_r, h_ri), dim=0)\n",
    "            h_ri = None\n",
    "            \n",
    "            if self.debug > 4:\n",
    "                print(\"\\tH_r:\\t\\t\\t\", H_r.shape)\n",
    "#                 print(\"hidden new:\\t\\t\", hidden[0].shape, hidden[1].shape)\n",
    "\n",
    "        return H_r[1:]\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros((1, batch_size, self.hidden_dim), device=device),\n",
    "                torch.zeros((1, batch_size, self.hidden_dim), device=device))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN)\n",
    "#     h_pi = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "#     h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "#     hidden = model.init_hidden()\n",
    "#     H_q = torch.randn(QUES_LEN, BATCH_SIZE, HIDDEN_DIM)\n",
    "    \n",
    "#     op, hid = model(h_pi, h_ri, H_q, hidden)\n",
    "    \n",
    "#     print(\"\\nDone:op\", op.shape)\n",
    "#     print(\"Done:hid\", hid[0].shape, hid[1].shape)\n",
    "\n",
    "if DEBUG > 2:\n",
    "    with torch.no_grad():\n",
    "        matchLSTMEncoder = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN).cuda(device)\n",
    "        hidden = matchLSTMEncoder.init_hidden()\n",
    "        para_embedded = torch.rand((PARA_LEN, BATCH_SIZE, HIDDEN_DIM), device=device)\n",
    "        ques_embedded = torch.rand((QUES_LEN, BATCH_SIZE, HIDDEN_DIM), device=device)\n",
    "        h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "    #     if DEBUG:\n",
    "    #         print (\"init h_ri shape is: \", h_ri.shape)\n",
    "    #         print (\"the para length is \", len(para_embedded))\n",
    "        H_r = matchLSTMEncoder(para_embedded.view(-1,BATCH_SIZE,HIDDEN_DIM),\n",
    "                               h_ri, \n",
    "                               ques_embedded, \n",
    "                               hidden)\n",
    "        print(\"H_r: \", H_r.shape)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network\n",
    "\n",
    "Using a ptrnet over $H_r$ to unfold and get most probable spans.\n",
    "We use the **boundry model** to do that (predict start and end of seq).\n",
    "\n",
    "A simple energy -> softmax -> decoder. Where softmaxed energy is supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PointerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, macros):\n",
    "        super(PointerDecoder, self).__init__()\n",
    "        \n",
    "        # Keep args\n",
    "        self.hidden_dim = macros['hidden_dim']\n",
    "        self.batch_size = macros['batch_size']\n",
    "        self.para_len = macros['para_len']\n",
    "        self.debug = macros['debug']\n",
    "        \n",
    "        self.lin_f_repeat = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.lin_f_nobias = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        \n",
    "        self.beta_k_w = nn.Parameter(torch.FloatTensor(self.hidden_dim, 1))\n",
    "        self.beta_k_b = nn.Parameter(torch.FloatTensor(1))\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.hidden_dim*(PARA_LEN+1), self.hidden_dim)\n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros((1, batch_size, self.hidden_dim), device=device),\n",
    "                torch.zeros((1, batch_size, self.hidden_dim), device=device))\n",
    "    \n",
    "    def forward(self, h_ak, H_r, hidden):\n",
    "        \n",
    "        # h_ak (current decoder's last op) (1,batch,hiddendim)\n",
    "        # H_r (weighted summary of para) (P, batch, hiddendim)\n",
    "        batch_size = H_r.shape[1]\n",
    "        \n",
    "        if self.debug > 4:\n",
    "            print(\"h_ak:\\t\\t\\t\", h_ak.shape)\n",
    "            print(\"H_r:\\t\\t\\t\", H_r.shape)\n",
    "            print(\"hidden:\\t\\t\\t\", hidden[0].shape, hidden[1].shape)\n",
    "            \n",
    "        # Prepare inputs for the tanh used to compute energy\n",
    "        f_input_b = self.lin_f_repeat(h_ak)\n",
    "        if self.debug > 4: print(\"f_input_b unrepeated:  \", f_input_b.shape)\n",
    "        \n",
    "        #H_r shape is ([PARA_LEN, BATCHSIZE, EmbeddingDIM])\n",
    "        f_input_b = f_input_b.repeat(H_r.shape[0], 1, 1)\n",
    "        if self.debug > 4: print(\"f_input_b repeated:\\t\", f_input_b.shape)\n",
    "            \n",
    "        f_input_a = self.lin_f_nobias(H_r)\n",
    "        if self.debug > 4: print(\"f_input_a:\\t\\t\", f_input_a.shape)\n",
    "            \n",
    "        # Send it off to tanh now\n",
    "        F_k = F.tanh(f_input_a+f_input_b)\n",
    "        if self.debug > 4: print(\"F_k:\\t\\t\\t\", F_k.shape) #PARA_LEN,BATCHSIZE,EmbeddingDim\n",
    "            \n",
    "        # Attention weights\n",
    "        beta_k_input_a = F_k.view(batch_size, -1, self.hidden_dim).matmul(self.beta_k_w).view(batch_size, 1, -1)\n",
    "        if self.debug > 4: print(\"beta_k_input_a:\\t\\t\", beta_k_input_a.shape)\n",
    "            \n",
    "        beta_k_input = beta_k_input_a.add_(self.beta_k_b.repeat(1,1,self.para_len))\n",
    "        if self.debug > 4: print(\"beta_k_input:\\t\\t\", beta_k_input.shape)\n",
    "            \n",
    "        beta_k = F.softmax(beta_k_input, dim=-1)\n",
    "        if self.debug > 4: print(\"beta_k:\\t\\t\\t\", beta_k.shape)\n",
    "            \n",
    "        lstm_input_a = H_r.view(batch_size, self.para_len, -1) * (beta_k.view(batch_size, self.para_len, -1).repeat(1,1,self.hidden_dim))\n",
    "        if self.debug > 4: print(\"lstm_input_a:\\t\\t\", lstm_input_a.shape)\n",
    "            \n",
    "        lstm_input = torch.cat((lstm_input_a.view(1, batch_size,-1), h_ak.view(1, batch_size, -1)), dim=2)\n",
    "        if self.debug > 4: print(\"lstm_input:\\t\\t\", lstm_input.shape)\n",
    "        \n",
    "        h_ak, hidden = self.lstm(lstm_input, hidden)\n",
    "        \n",
    "        return h_ak, hidden, beta_k\n",
    "            \n",
    "if DEBUG > 2:\n",
    "    with torch.no_grad():\n",
    "        pointerDecoder = PointerDecoder(HIDDEN_DIM).cuda(device)\n",
    "        h_ak = torch.randn(1,BATCH_SIZE,HIDDEN_DIM, device=device)\n",
    "    #     H_r = torch.randn(PARA_LEN, BATCH_SIZE, HIDDEN_DIM)\n",
    "        pointerHidden = pointerDecoder.init_hidden()\n",
    "        h_ak, hidden, beta_k = pointerDecoder(h_ak, para_embedded, hidden)\n",
    "        print (beta_k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull the real data from disk.\n",
    "\n",
    "Files stored in `./data/squad/train.ids.*`\n",
    "Pull both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_loc, macros, crop=None):\n",
    "    \"\"\"\n",
    "        Given the dataloc and the data available in a specific format, it would pick the data up, and make trainable matrices,\n",
    "        Harvest train_P, train_Q, train_Y, test_P, test_Q, test_Y matrices in this format\n",
    "        \n",
    "        If crop given, will trim the data at a certain length\n",
    "        \n",
    "        **return_type**: np matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpacking macros\n",
    "    PARA_LEN = macros['para_len']\n",
    "    QUES_LEN = macros['ques_len']\n",
    "    \n",
    "    train_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.ids.question')))])\n",
    "    train_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.ids.context')))])\n",
    "    train_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.span')))])\n",
    "\n",
    "    test_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.ids.question')))])\n",
    "    test_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.ids.context')))])\n",
    "    test_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.span')))])\n",
    "\n",
    "    if macros['debug'] > 3:\n",
    "        print(\"Train Q: \", train_q.shape)\n",
    "        print(\"Train P: \", train_p.shape)\n",
    "        print(\"Train Y: \", train_y.shape)\n",
    "        print(\"Test Q: \", test_q.shape)\n",
    "        print(\"Test P: \", test_p.shape)\n",
    "        print(\"Test Y: \", test_y.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "        Parse the semi-raw data:\n",
    "            - shuffle\n",
    "            - pad, prepare\n",
    "            - dump useless vars\n",
    "    \"\"\"\n",
    "    # Shuffle data\n",
    "    \n",
    "    if crop:\n",
    "        index_train, index_test = np.random.choice(np.arange(len(train_p)), crop), \\\n",
    "                                  np.random.choice(np.arange(len(test_p)), crop)\n",
    "    else:\n",
    "        index_train, index_test = np.arange(len(train_p)), np.arange(len(test_p))\n",
    "        np.random.shuffle(index_train)\n",
    "        np.random.shuffle(index_test)\n",
    "\n",
    "    train_p, train_q, train_y = train_p[index_train], train_q[index_train], train_y[index_train]\n",
    "    test_p, test_q, test_y = test_p[index_test], test_q[index_test], test_y[index_test]\n",
    "\n",
    "#     sanity_check(train_p, train_y)\n",
    "    \n",
    "    # Pad and prepare\n",
    "    train_P = np.zeros((len(train_p), PARA_LEN))\n",
    "    train_Q = np.zeros((len(train_q), QUES_LEN))\n",
    "    train_Y_start = np.zeros((len(train_p), PARA_LEN))\n",
    "    train_Y_end = np.zeros((len(train_p), PARA_LEN))\n",
    "\n",
    "    test_P = np.zeros((len(test_p), PARA_LEN))\n",
    "    test_Q = np.zeros((len(test_q), QUES_LEN))\n",
    "    test_Y_start = np.zeros((len(test_p), PARA_LEN))\n",
    "    test_Y_end = np.zeros((len(test_p), PARA_LEN))\n",
    "    \n",
    "#     print(train_P.shape)\n",
    "\n",
    "    crop_train = []    # Remove these rows from training\n",
    "    for i in range(len(train_p)):\n",
    "        p = train_p[i]\n",
    "        q = train_q[i]\n",
    "        y = train_y[i]\n",
    "\n",
    "        # First see if you can keep this example or not (due to size)\n",
    "        if y[0] >= PARA_LEN or y[1] >= PARA_LEN:\n",
    "            crop_train.append(i)\n",
    "            continue\n",
    "\n",
    "\n",
    "        train_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "        train_Q[i, :min(QUES_LEN, len(q))] = p[:min(QUES_LEN, len(q))]\n",
    "        train_Y_start[i, y[0]] = 1\n",
    "        train_Y_end[i, y[1]] = 1\n",
    "\n",
    "    crop_test = []\n",
    "    for i in range(len(test_p)):\n",
    "        p = test_p[i]\n",
    "        q = test_q[i]\n",
    "        y = test_y[i]\n",
    "\n",
    "        # First see if you can keep this example or not (due to size)\n",
    "        if y[0] >= PARA_LEN or y[1] >= PARA_LEN:\n",
    "            crop_test.append(i)\n",
    "            continue\n",
    "\n",
    "        test_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "        test_Q[i, :min(QUES_LEN, len(q))] = p[:min(QUES_LEN, len(q))]\n",
    "        test_Y_start[i, y[0]] = 1\n",
    "        test_Y_end[i, y[1]] = 1\n",
    "        \n",
    "        \n",
    "    # Remove the instances which are in crop_train\n",
    "    train_P = np.delete(train_P, crop_train, axis=0)\n",
    "    train_Q = np.delete(train_Q, crop_train, axis=0)\n",
    "    train_Y_start = np.delete(train_Y_start, crop_train, axis=0)\n",
    "    train_Y_end = np.delete(train_Y_end, crop_train, axis=0)\n",
    "    \n",
    "    test_P = np.delete(test_P, crop_test, axis=0)\n",
    "    test_Q = np.delete(test_Q, crop_test, axis=0)\n",
    "    test_Y_start = np.delete(test_Y_start, crop_test, axis=0)\n",
    "    test_Y_end = np.delete(test_Y_end, crop_test, axis=0)\n",
    "\n",
    "    if macros['debug'] >= 1:\n",
    "        print(\"Train Q: \", train_Q.shape)\n",
    "        print(\"Train P: \", train_P.shape)\n",
    "        print(\"Train Y: \", train_Y_start.shape)\n",
    "        print(\"Test Q: \", test_Q.shape)\n",
    "        print(\"Test P: \", test_P.shape)\n",
    "        print(\"Test Y: \", test_Y_start.shape)\n",
    "        print(\"Crop_train: \", len(crop_train))\n",
    "        print(\"Crop_test: \", len(crop_test))\n",
    "    # Let's free up some memory now\n",
    "    train_p, train_q, train_y, test_p, test_q, test_y = None, None, None, None, None, None\n",
    "    \n",
    "    # Load embedding matrics\n",
    "    vectors = np.load(os.path.join(data_loc, 'glove.trimmed.300.npz'))['glove']\n",
    "    \n",
    "    return train_P, train_Q, train_Y_start, train_Y_end, test_P, test_Q, test_Y_start, test_Y_end, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macros = {\n",
    "#     \"ques_len\": QUES_LEN,\n",
    "#     \"hidden_dim\": HIDDEN_DIM, \n",
    "#     \"vocab_size\": VOCAB_SIZE, \n",
    "#     \"batch_size\": BATCH_SIZE,\n",
    "#     \"para_len\": PARA_LEN,\n",
    "#     \"embedding_dim\": EMBEDDING_DIM,\n",
    "#     \"debug\": 4\n",
    "# } \n",
    "\n",
    "# a = prepare_data(DATA_LOC, macros=macros, crop=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, and running the model\n",
    "- Write a train fn\n",
    "- Write a training loop invoking it\n",
    "- Fill in real data\n",
    "\n",
    "----------\n",
    "\n",
    "Feats:\n",
    "- Function to test every n epochs.\n",
    "- Report train accuracy every epoch\n",
    "- Store the train, test accuracy for every instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(para_batch,\n",
    "          ques_batch,\n",
    "          answer_start_batch,\n",
    "          answer_end_batch,\n",
    "          ques_model,\n",
    "          para_model,\n",
    "          match_LSTM_encoder_model,\n",
    "          pointer_decoder_model,\n",
    "          optimizer, \n",
    "          loss_fn,\n",
    "          macros,\n",
    "          debug=2):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    :param para_batch: paragraphs (batch, max_seq_len_para) \n",
    "    :param ques_batch: questions corresponding to para (batch, max_seq_len_ques)\n",
    "    :param answer_start_batch: one-hot vector denoting pos of span start (batch, max_seq_len_para)\n",
    "    :param answer_end_batch: one-hot vector denoting pos of span end (batch, max_seq_len_para)\n",
    "    \n",
    "    # Models\n",
    "    :param ques_model: model to encode ques\n",
    "    :param para_model: model to encode para\n",
    "    :param match_LSTM_encoder_model: model to match para, ques to get para summary\n",
    "    :param pointer_decoder_model: model to get a pointer over start and end span pointer\n",
    "    \n",
    "    # Loss and Optimizer.\n",
    "    :param loss_fn: \n",
    "    :param optimizer: \n",
    "    \n",
    "    :return: \n",
    "    \n",
    "    \n",
    "    NOTE: When using MSE, \n",
    "        - target labels are one-hot\n",
    "        - target label is float tensor\n",
    "        - shape (batch, 1, len)\n",
    "        \n",
    "        When using CrossEntropy\n",
    "        - target is not onehot\n",
    "        - long\n",
    "        - shape (batch, )\n",
    "    \"\"\"\n",
    "    try:    \n",
    "    #     DEBUG = debug\n",
    "    #     BATCH_SIZE = macros['batch_size']\n",
    "    #     HIDDEN_DIM = macros['hidden_dim']\n",
    "\n",
    "        if debug >=2: \n",
    "            print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "            print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "            print(\"\\tanswer_start_batch:\\t\", answer_start_batch.shape)\n",
    "            print(\"\\tanswer_end_batch:\\t\\t\", answer_end_batch.shape)\n",
    "\n",
    "        # Wiping all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initializing all hidden states.\n",
    "        hidden_quesenc = ques_model.init_hidden(macros['batch_size'])\n",
    "        hidden_paraenc = para_model.init_hidden(macros['batch_size'])\n",
    "        hidden_mlstm = match_LSTM_encoder_model.init_hidden(macros['batch_size'])\n",
    "        hidden_ptrnet = pointer_decoder_model.init_hidden(macros['batch_size'])\n",
    "        h_ri = torch.zeros((1, macros['batch_size'], macros['hidden_dim']), dtype=torch.float, device=device)\n",
    "        h_ak = torch.zeros((1, macros['batch_size'], macros['hidden_dim']), dtype=torch.float, device=device)\n",
    "        if debug >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "\n",
    "        #passing the data through LSTM pre-processing layer\n",
    "        H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc)\n",
    "        H_p, para_model_hidden = para_model(para_batch, hidden_paraenc)\n",
    "        if debug >= 2: \n",
    "            print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "            print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "            print(\"\\tH_ri:\\t\\t\", h_ri.shape)\n",
    "    #         raw_input(\"Check memory and ye shall continue\")\n",
    "            print(\"------------Encoded hidden states------------\")\n",
    "\n",
    "        H_r = match_LSTM_encoder_model(H_p.view(-1, macros['batch_size'], macros['hidden_dim']), h_ri, H_q, hidden_mlstm)\n",
    "        if debug >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "\n",
    "        #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "        h_ak, hidden_ptrnet, beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet)\n",
    "        h_ak, hidden_ptrnet, beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet)\n",
    "        if debug >= 2: print(\"------------Passed through pointernet------------\")\n",
    "\n",
    "\n",
    "        # For crossentropy\n",
    "        _, answer_start_batch = answer_start_batch.max(dim=2)\n",
    "        _, answer_end_batch = answer_end_batch.max(dim=2)\n",
    "        answer_start_batch = answer_start_batch.view(-1).long()\n",
    "        answer_end_batch = answer_end_batch.view(-1).long()\n",
    "#         print(beta_k_start.view(-1, macros['para_len']).shape, answer_start_batch.view(-1).shape)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = loss_fn(beta_k_start.view(-1, macros['para_len']), answer_start_batch)\n",
    "        loss += loss_fn(beta_k_end.view(-1, macros['para_len']), answer_end_batch)\n",
    "#         loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "#         loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "        if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "\n",
    "        loss.backward()\n",
    "        if debug >= 2: print(\"------------Calculated Gradients------------\")\n",
    "\n",
    "        #optimization step\n",
    "        optimizer.step()\n",
    "        if debug >= 2: print(\"------------Updated weights.------------\")\n",
    "            \n",
    "        return beta_k_start, beta_k_end, loss\n",
    "    \n",
    "    except: \n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function (no grad, no eval)\n",
    "def predict(para_batch,\n",
    "            ques_batch,\n",
    "            ques_model,\n",
    "            para_model,\n",
    "            match_LSTM_encoder_model,\n",
    "            pointer_decoder_model,\n",
    "            macros,\n",
    "            loss_fn,\n",
    "            debug):\n",
    "    \"\"\"\n",
    "        Function which returns the model's output based on a given set of P&Q's. \n",
    "        Does not convert to strings, gives the direct model output.\n",
    "        \n",
    "        Expects:\n",
    "            four models\n",
    "            data\n",
    "            misc macros\n",
    "    \"\"\"\n",
    "    \n",
    "#     BATCH_SIZE = macros['batch_size']\n",
    "    BATCH_SIZE = ques_batch.shape[0]\n",
    "    HIDDEN_DIM = macros['hidden_dim']\n",
    "    DEBUG = debug\n",
    "    \n",
    "    if debug >=2: \n",
    "        print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "        print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "        \n",
    "    with torch.no_grad():    \n",
    "\n",
    "        # Initializing all hidden states.\n",
    "        hidden_quesenc = ques_model.init_hidden(BATCH_SIZE)\n",
    "        hidden_paraenc = para_model.init_hidden(BATCH_SIZE)\n",
    "        hidden_mlstm = match_LSTM_encoder_model.init_hidden(BATCH_SIZE)\n",
    "        hidden_ptrnet = pointer_decoder_model.init_hidden(BATCH_SIZE)\n",
    "        h_ri = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        h_ak = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        if DEBUG >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "            \n",
    "        #passing the data through LSTM pre-processing layer\n",
    "        H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc)\n",
    "        H_p, para_model_hidden = para_model(para_batch, hidden_paraenc)\n",
    "        if DEBUG >= 2: \n",
    "            print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "            print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "            print(\"\\tH_ri:\\t\\t\", h_ri.shape)\n",
    "#             raw_input(\"Check memory and ye shall continue\")\n",
    "            print(\"------------Encoded hidden states------------\")\n",
    "\n",
    "        H_r = match_LSTM_encoder_model(H_p.view(-1, BATCH_SIZE, HIDDEN_DIM), h_ri, H_q, hidden_mlstm)\n",
    "        if DEBUG >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "\n",
    "        #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "        h_ak, hidden_ptrnet, beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet)\n",
    "        _, _, beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet)\n",
    "        if DEBUG >= 2: print(\"------------Passed through pointernet------------\")\n",
    "            \n",
    "        loss = 0.0\n",
    "                            \n",
    "        # For crossentropy\n",
    "#         _, answer_start_batch = answer_start_batch.max(dim=2)[1]\n",
    "#         _, answer_end_batch = answer_end_batch.max(dim=2)[1]\n",
    "#         print(\"labels: \", answer_start_batch.shape)[1]\n",
    "            \n",
    "#         #How will we manage batches for loss.\n",
    "#         loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "#         loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "#         if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "            \n",
    "        return (beta_k_start, beta_k_end, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval function (no grad no eval no nothing)\n",
    "def eval(y_cap, y, metrics={'em':None}):\n",
    "    \"\"\" \n",
    "        Returns the exact-match (em) metric by default.\n",
    "        Can specifiy more in a list (TODO)\n",
    "        \n",
    "        Inputs:\n",
    "        - y_cap: list of two tensors (start, end) of dim [BATCH_SIZE, PARA_LEN] each\n",
    "        - y: list of two tensors (start, end) of dim [BATCH_SIZE, 1] each\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(y[0].shape, y[1].shape, y_cap[0].shape, y_cap[1].shape)\n",
    "    \n",
    "    y_cap= torch.argmax(y_cap[0], dim=1).float(), torch.argmax(y_cap[1], dim=1).float()\n",
    "    y = torch.argmax(y[0], dim=1).float(), torch.argmax(y[1], dim=1).float()\n",
    "    \n",
    "    if \"em\" in metrics.keys():\n",
    "        metrics['em'] = (y[0].eq(y_cap[0]) & y[1].eq(y_cap[1])).sum().item()/ float(y[0].shape[0])\n",
    "        \n",
    "    if DEBUG >= 3: \n",
    "        print(\"Test performance: \", metrics)\n",
    "        print(\"------------Evaluated------------\")\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "if DEBUG >=5:\n",
    "    # Testing this function\n",
    "    metrics = {'em':None}\n",
    "#     y = torch.tensor([[3]]).float(), torch.tensor([[4]]).float()\n",
    "    y = torch.tensor([[0,0,3,0], [0,2,0,0]]), torch.tensor([[0,0,0,3], [0,0,0,3]])\n",
    "    y_cap = torch.tensor([[0,0,3,0],[0,0,3,0]]), torch.tensor([[0,0,0,3],[0,0,0,3]])\n",
    "#     y = torch.randint(0, PARA_LEN, (BATCH_SIZE,)).float(), torch.randint(0, PARA_LEN, (BATCH_SIZE,)).float()\n",
    "#     y_cap = torch.rand((BATCH_SIZE, PARA_LEN)), torch.rand((BATCH_SIZE, PARA_LEN))\n",
    "    print(eval(y_cap, y))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(_models, _data, _macros, _epochs, _save_best=False, _test_eval=0, _train_eval=0, _debug=2):\n",
    "    \"\"\"\n",
    "        > Instantiate models\n",
    "        > Instantiate loss, optimizer\n",
    "        > Instantiate ways to store loss\n",
    "\n",
    "        > Per epoch\n",
    "            > sample batch and give to train fn\n",
    "            > get loss\n",
    "            > if epoch %k ==0: get test accuracy\n",
    "\n",
    "        > have fn to calculate test accuracy\n",
    "    \"\"\"\n",
    "    try: \n",
    "        # Unpack data\n",
    "        DEBUG = _debug\n",
    "        train_P = _data['train']['P']\n",
    "        train_Q = _data['train']['Q']\n",
    "        train_Y_start = _data['train']['Ys']\n",
    "        train_Y_end = _data['train']['Ye']\n",
    "        test_P = _data['test']['P']\n",
    "        test_Q = _data['test']['Q']\n",
    "        test_Y_start = _data['test']['Ys']\n",
    "        test_Y_end = _data['test']['Ye']\n",
    "\n",
    "        ques_model, para_model, match_LSTM_encoder_model, pointer_decoder_model = _models\n",
    "        _data = None\n",
    "\n",
    "        # Instantiate Loss\n",
    "#         loss_fn = nn.MSELoss()\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, ques_model.parameters())) + \n",
    "                                 list(filter(lambda p: p.requires_grad, para_model.parameters())) + \n",
    "                                 list(match_LSTM_encoder_model.parameters()) + \n",
    "                                 list(pointer_decoder_model.parameters()), lr=macros['lr'])\n",
    "#         optimizer = optim.Adam(list(ques_model.parameters()) + \\\n",
    "#                                list(para_model.parameters()) + \\\n",
    "#                                list(match_LSTM_encoder_model.parameters()) + \\\n",
    "#                               list(pointer_decoder_model.parameters()), lr=macros['lr'])\n",
    "\n",
    "        # Losses\n",
    "        train_losses = []\n",
    "        train_em = []\n",
    "        test_losses = []\n",
    "        test_em = []\n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(_epochs):\n",
    "            print(\"Epoch: \", epoch, \"/\", _epochs)\n",
    "\n",
    "            epoch_loss = []\n",
    "            epoch_train_em = []\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            for iter in range(int(len(train_P)/BATCH_SIZE)):\n",
    "    #         for iter in range(2):\n",
    "\n",
    "                batch_time = time.time()\n",
    "\n",
    "                # Sample batch and train on it\n",
    "                sample_index = np.random.randint(0, len(train_P), _macros['batch_size'])\n",
    "            \n",
    "#                 grad_old = sum([x.grad.sum().item() for x in params])\n",
    "\n",
    "                y_cap_start, y_cap_end, loss = train(\n",
    "                    para_batch = torch.tensor(train_P[sample_index], dtype=torch.long, device=device),\n",
    "                    ques_batch = torch.tensor(train_Q[sample_index], dtype=torch.long, device=device),\n",
    "                    answer_start_batch = torch.tensor(train_Y_start[sample_index], dtype=torch.float, device=device).view( _macros['batch_size'], 1, _macros['para_len']),\n",
    "                    answer_end_batch = torch.tensor(train_Y_end[sample_index], dtype=torch.float, device=device).view(_macros['batch_size'], 1, _macros['para_len']),\n",
    "                    ques_model = ques_model,\n",
    "                    para_model = para_model,\n",
    "                    match_LSTM_encoder_model = match_LSTM_encoder_model,\n",
    "                    pointer_decoder_model = pointer_decoder_model,\n",
    "                    optimizer = optimizer, \n",
    "                    loss_fn= loss_fn,\n",
    "                    macros=_macros,\n",
    "                    debug=_macros['debug']\n",
    "                )\n",
    "\n",
    "                if _train_eval: \n",
    "                    # Calculate train accuracy for this minibatch\n",
    "                    metrics = eval(\n",
    "                        y=(torch.tensor(train_Y_start[sample_index], dtype=torch.long, device=device).view( -1, _macros['para_len']),\n",
    "                            torch.tensor(train_Y_end[sample_index], dtype=torch.long, device=device).view(-1, _macros['para_len'])),\n",
    "                        y_cap=[y_cap_start.squeeze(), y_cap_end.squeeze()])\n",
    "\n",
    "                    epoch_train_em.append(metrics['em'])\n",
    "    \n",
    "                epoch_loss.append(loss.item())\n",
    "    \n",
    "#                 grad_new = sum([x.grad.sum().item() for x in params])\n",
    "\n",
    "                print(\"Batch:\\t%d\" % iter,\"/%d\\t: \" % (len(train_P)/_macros['batch_size']),\n",
    "                      str(\"%s\" % (time.time() - batch_time))[:8], \n",
    "                      str(\"\\t\\b%s\" % (time.time() - epoch_time))[:10], \n",
    "                      \"\\tl:%f\" % loss.item(),\n",
    "                      \"\\tem:%f\" % epoch_train_em[-1] if _train_eval else \"\")\n",
    "#                      \"\\t\\b\\b%s\" % grad_new - grad_old)\n",
    "#                      end=None if iter+1 == int(len(train_P)/BATCH_SIZE) else \"\\r\")\n",
    "\n",
    "            train_losses.append(epoch_loss)\n",
    "        \n",
    "            if _train_eval: train_em.append(epoch_train_em)\n",
    "            if _test_eval and epoch % _test_eval == 0:\n",
    "\n",
    "                y_cap_start, y_cap_end, test_loss = predict(\n",
    "                    para_batch = torch.tensor(test_P, dtype=torch.long, device=device),\n",
    "                    ques_batch = torch.tensor(test_Q, dtype=torch.long, device=device),\n",
    "                    ques_model = ques_model,\n",
    "                    para_model = para_model,\n",
    "                    match_LSTM_encoder_model = match_LSTM_encoder_model,\n",
    "                    pointer_decoder_model = pointer_decoder_model,\n",
    "                    macros = _macros,\n",
    "                    loss_fn= loss_fn,\n",
    "                    debug = _macros['debug']\n",
    "                )\n",
    "                metrics = eval(\n",
    "                    y=(torch.tensor(test_Y_start, dtype=torch.long, device=device).view( -1, _macros['para_len']),\n",
    "                        torch.tensor(test_Y_end, dtype=torch.long, device=device).view(-1, _macros['para_len'])),\n",
    "                    y_cap=[y_cap_start.squeeze(), y_cap_end.squeeze()])\n",
    "\n",
    "                test_losses.append(test_loss)\n",
    "                test_em.append(metrics['em'])\n",
    "\n",
    "            # At the end of every epoch, do print the average epoch loss, and other stat\n",
    "            print(\"\\nEpoch performance: \",\n",
    "                  \"%ssec\" % str(time.time() - epoch_time)[:6],\n",
    "                  \"Trl:%f\" % np.mean(epoch_loss, axis=0),\n",
    "                  \"\\tTrem:%f\" % np.mean(epoch_train_em) if _train_eval and epoch % _train_eval == 0 else \"\",\n",
    "                  \"\\tTeem:%f\\n\" % test_em[-1] if _test_eval and epoch % _test_eval == 0 else \"\\n\")\n",
    "\n",
    "#         return train_losses, train_em, test_losses, test_em\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        # someone called a ctrl+c on it. Let' return the things computed so far atlest.\n",
    "        print(\"Found keyboard interrup\")\n",
    "    finally:\n",
    "        return train_losses, train_em, test_losses, test_em\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "    Saving said models.\n",
    "    TODO\n",
    "\"\"\"\n",
    "# ques_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(loss, _label=\"Some label\", _only_epoch=True):\n",
    "    \"\"\"\n",
    "        Fn to visualize loss.\n",
    "        Expects either\n",
    "            - [int, int] for epoch level stuff\n",
    "            - [ [int, int], [int, int] ] for batch level data. \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = [15, 8] \n",
    "    \n",
    "    # Detect input format\n",
    "    if type(loss[0]) in [int, float, long]:\n",
    "        \n",
    "#         print(\"here\")\n",
    "        \n",
    "        plt.plot(loss)\n",
    "        plt.ylabel(_label)\n",
    "        plt.show()\n",
    "        \n",
    "    elif type(loss[0]) == list:\n",
    "        \n",
    "        if _only_epoch:\n",
    "            loss = [ sum(x) for x in loss ]\n",
    "            \n",
    "        else:\n",
    "            loss = [ y for x in loss for y in x ]\n",
    "            \n",
    "        plt.plot(loss)\n",
    "        plt.ylabel(_label)\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator\n",
    "\n",
    "One cell which instantiates and runs everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Q:  (197, 30)\n",
      "Train P:  (197, 200)\n",
      "Train Y:  (197, 200)\n",
      "Test Q:  (196, 30)\n",
      "Test P:  (196, 200)\n",
      "Test Y:  (196, 200)\n",
      "Crop_train:  3\n",
      "Crop_test:  4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Cell which pulls everything together.\n",
    "\n",
    "    > init models\n",
    "    > get data prepared\n",
    "    > pass models and data to training loop\n",
    "    > gets trained models and loss\n",
    "    > saves models\n",
    "    > visualizes loss?\n",
    "\n",
    "No other function but this one ever sees global macros!\n",
    "\"\"\"\n",
    "macros = {\n",
    "    \"ques_len\": QUES_LEN,\n",
    "    \"hidden_dim\": HIDDEN_DIM, \n",
    "    \"vocab_size\": VOCAB_SIZE, \n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"para_len\": PARA_LEN,\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"lr\": LR,\n",
    "    \"debug\":1\n",
    "} \n",
    "\n",
    "data = {'train':{}, 'test':{}}\n",
    "data['train']['P'], data['train']['Q'], data['train']['Ys'], data['train']['Ye'], \\\n",
    "data['test']['P'], data['test']['Q'], data['test']['Ys'], data['test']['Ye'], vectors = \\\n",
    "    prepare_data(DATA_LOC, macros, crop=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate modelshttp://localhost:8888/notebooks/model.ipynb#\n",
    "ques_model = Encoder(QUES_LEN, macros, vectors).cuda(device)\n",
    "para_model = Encoder(PARA_LEN, macros, vectors).cuda(device)\n",
    "# ques_model = Encoder(QUES_LEN, macros, glove_file).cuda(device)\n",
    "# para_model = Encoder(PARA_LEN, macros, glove_file).cuda(device)\n",
    "match_LSTM_encoder_model = MatchLSTMEncoder(macros).cuda(device)\n",
    "pointer_decoder_model = PointerDecoder(macros).cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 / 300\n",
      "Batch:\t0 /9\t:  2.818527 2.818559 \tl:10.596635 \tem:0.000000\n",
      "Batch:\t1 /9\t:  0.983757 3.803176 \tl:10.596634 \tem:0.000000\n",
      "Batch:\t2 /9\t:  0.872685 4.677175 \tl:10.596633 \tem:0.000000\n",
      "Batch:\t3 /9\t:  0.930788 5.608595 \tl:10.596628 \tem:0.050000\n",
      "Batch:\t4 /9\t:  0.975600 6.584432 \tl:10.596630 \tem:0.000000\n",
      "Batch:\t5 /9\t:  0.898082 7.483011 \tl:10.596640 \tem:0.000000\n",
      "Batch:\t6 /9\t:  1.006263 8.490586 \tl:10.596645 \tem:0.000000\n",
      "Batch:\t7 /9\t:  0.979976 9.470842 \tl:10.596635 \tem:0.000000\n",
      "Batch:\t8 /9\t:  0.994686 10.46610 \tl:10.596626 \tem:0.000000\n",
      "\n",
      "Epoch performance:  10.673sec Trl:10.596634 \tTrem:0.005556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  1 / 300\n",
      "Batch:\t0 /9\t:  0.783880 0.783897 \tl:10.596622 \tem:0.000000\n",
      "Batch:\t1 /9\t:  0.814975 1.599053 \tl:10.596626 \tem:0.000000\n",
      "Batch:\t2 /9\t:  0.859468 2.459730 \tl:10.596618 \tem:0.000000\n",
      "Batch:\t3 /9\t:  0.975922 3.436666 \tl:10.596630 \tem:0.000000\n",
      "Batch:\t4 /9\t:  0.855581 4.293632 \tl:10.596588 \tem:0.000000\n",
      "Batch:\t5 /9\t:  0.884494 5.179157 \tl:10.596611 \tem:0.000000\n",
      "Batch:\t6 /9\t:  0.890279 6.070332 \tl:10.596634 \tem:0.000000\n",
      "Batch:\t7 /9\t:  0.972701 7.045956 \tl:10.596653 \tem:0.000000\n",
      "Batch:\t8 /9\t:  0.860269 7.907533 \tl:10.596621 \tem:0.000000\n",
      "\n",
      "Epoch performance:  8.1190sec Trl:10.596623 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  2 / 300\n",
      "Batch:\t0 /9\t:  1.022197 1.022225 \tl:10.596596 \tem:0.000000\n",
      "Batch:\t1 /9\t:  0.919826 1.943909 \tl:10.596452 \tem:0.000000\n",
      "Batch:\t2 /9\t:  0.938533 2.883751 \tl:10.596569 \tem:0.000000\n",
      "Batch:\t3 /9\t:  0.866384 3.751356 \tl:10.596498 \tem:0.000000\n",
      "Batch:\t4 /9\t:  1.112154 4.864073 \tl:10.596880 \tem:0.000000\n",
      "Batch:\t5 /9\t:  0.921023 5.785930 \tl:10.596465 \tem:0.000000\n",
      "Batch:\t6 /9\t:  0.897286 6.683878 \tl:10.595905 \tem:0.000000\n",
      "Batch:\t7 /9\t:  1.016345 7.700898 \tl:10.595621 \tem:0.000000\n",
      "Batch:\t8 /9\t:  1.290028 8.991755 \tl:10.596081 \tem:0.000000\n",
      "\n",
      "Epoch performance:  9.2120sec Trl:10.596341 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  3 / 300\n",
      "Batch:\t0 /9\t:  1.105590 1.105607 \tl:10.596977 \tem:0.000000\n",
      "Batch:\t1 /9\t:  1.049949 2.156831 \tl:10.597912 \tem:0.000000\n",
      "Batch:\t2 /9\t:  1.076411 3.234319 \tl:10.597398 \tem:0.000000\n",
      "Batch:\t3 /9\t:  1.063939 4.299633 \tl:10.596197 \tem:0.000000\n",
      "Batch:\t4 /9\t:  0.878439 5.179123 \tl:10.595705 \tem:0.000000\n",
      "Batch:\t5 /9\t:  0.951725 6.132199 \tl:10.595465 \tem:0.050000\n",
      "Batch:\t6 /9\t:  1.159587 7.293735 \tl:10.596172 \tem:0.000000\n",
      "Batch:\t7 /9\t:  1.006175 8.300709 \tl:10.595050 \tem:0.000000\n",
      "Batch:\t8 /9\t:  0.900551 9.202604 \tl:10.593689 \tem:0.000000\n",
      "\n",
      "Epoch performance:  9.4079sec Trl:10.596063 \tTrem:0.005556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  4 / 300\n",
      "Batch:\t0 /9\t:  0.898277 0.898296 \tl:10.595043 \tem:0.000000\n",
      "Batch:\t1 /9\t:  0.978550 1.877645 \tl:10.594079 \tem:0.000000\n",
      "Batch:\t2 /9\t:  0.871392 2.749699 \tl:10.592909 \tem:0.000000\n",
      "Batch:\t3 /9\t:  1.042999 3.793696 \tl:10.594511 \tem:0.000000\n",
      "Batch:\t4 /9\t:  0.990686 4.785716 \tl:10.588682 \tem:0.000000\n",
      "Batch:\t5 /9\t:  1.073118 5.860239 \tl:10.595596 \tem:0.000000\n",
      "Batch:\t6 /9\t:  0.892191 6.752815 \tl:10.574297 \tem:0.050000\n",
      "Batch:\t7 /9\t:  0.918421 7.674296 \tl:10.555736 \tem:0.050000\n",
      "Batch:\t8 /9\t:  1.138187 8.813586 \tl:10.601023 \tem:0.000000\n",
      "\n",
      "Epoch performance:  9.0227sec Trl:10.587986 \tTrem:0.011111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  5 / 300\n",
      "Batch:\t0 /9\t:  0.977977 0.977994 \tl:10.482149 \tem:0.100000\n",
      "Batch:\t1 /9\t:  0.816156 1.794697 \tl:10.559632 \tem:0.050000\n",
      "Batch:\t2 /9\t:  1.008358 2.803812 \tl:10.589939 \tem:0.000000\n",
      "Batch:\t3 /9\t:  1.066892 3.871569 \tl:10.536501 \tem:0.050000\n",
      "Batch:\t4 /9\t:  0.950984 4.824046 \tl:10.532640 \tem:0.000000\n",
      "Batch:\t5 /9\t:  1.014543 5.839886 \tl:10.587540 \tem:0.000000\n",
      "Batch:\t6 /9\t:  0.952160 6.793255 \tl:10.544889 \tem:0.050000\n",
      "Batch:\t7 /9\t:  1.032263 7.825771 \tl:10.604645 \tem:0.000000\n",
      "Batch:\t8 /9\t:  1.087978 8.915050 \tl:10.595928 \tem:0.000000\n",
      "\n",
      "Epoch performance:  9.1540sec Trl:10.559318 \tTrem:0.027778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  6 / 300\n",
      "Batch:\t0 /9\t:  1.051983 1.052020 \tl:10.499658 \tem:0.050000\n",
      "Batch:\t1 /9\t:  0.961453 2.014996 \tl:10.563261 \tem:0.000000\n",
      "Batch:\t2 /9\t:  0.883394 2.899226 \tl:10.596914 \tem:0.000000\n",
      "Batch:\t3 /9\t:  0.936604 3.838943 \tl:10.593786 \tem:0.050000\n",
      "Batch:\t4 /9\t:  0.942396 4.781623 \tl:10.596334 \tem:0.000000\n",
      "Batch:\t5 /9\t:  0.911140 5.695659 \tl:10.593304 \tem:0.050000\n",
      "Batch:\t6 /9\t:  0.898459 6.595333 \tl:10.594552 \tem:0.000000\n",
      "Batch:\t7 /9\t:  0.928941 7.525135 \tl:10.596762 \tem:0.000000\n",
      "Batch:\t8 /9\t:  0.909750 8.436083 \tl:10.586320 \tem:0.050000\n",
      "\n",
      "Epoch performance:  8.6498sec Trl:10.580099 \tTrem:0.022222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  7 / 300\n",
      "Batch:\t0 /9\t:  1.212470 1.212491 \tl:10.593317 \tem:0.000000\n",
      "Batch:\t1 /9\t:  1.073297 2.286791 \tl:10.596647 \tem:0.000000\n",
      "Batch:\t2 /9\t:  1.086374 3.374178 \tl:10.556889 \tem:0.050000\n",
      "Batch:\t3 /9\t:  0.892662 4.268119 \tl:10.477131 \tem:0.000000\n",
      "Batch:\t4 /9\t:  0.833894 5.102511 \tl:10.553415 \tem:0.000000\n",
      "Batch:\t5 /9\t:  0.873476 5.976675 \tl:10.441676 \tem:0.050000\n",
      "Batch:\t6 /9\t:  0.947624 6.925409 \tl:10.511752 \tem:0.000000\n",
      "Batch:\t7 /9\t:  1.074951 8.001030 \tl:10.570949 \tem:0.000000\n",
      "Batch:\t8 /9\t:  1.043840 9.045862 \tl:10.589231 \tem:0.000000\n",
      "\n",
      "Epoch performance:  9.2656sec Trl:10.543445 \tTrem:0.011111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  8 / 300\n",
      "Batch:\t0 /9\t:  1.056583 1.056602 \tl:10.552389 \tem:0.000000\n",
      "Batch:\t1 /9\t:  0.943835 2.001034 \tl:10.456176 \tem:0.000000\n",
      "Batch:\t2 /9\t:  0.889112 2.891511 \tl:10.597831 \tem:0.000000\n",
      "Batch:\t3 /9\t:  1.070207 3.963124 \tl:10.597239 \tem:0.000000\n",
      "Batch:\t4 /9\t:  0.974244 4.937876 \tl:10.472731 \tem:0.050000\n",
      "Batch:\t5 /9\t:  0.884193 5.822906 \tl:10.498734 \tem:0.050000\n",
      "Batch:\t6 /9\t:  0.861379 6.685252 \tl:10.536414 \tem:0.000000\n",
      "Batch:\t7 /9\t:  0.863211 7.549031 \tl:10.475870 \tem:0.000000\n",
      "Batch:\t8 /9\t:  0.917357 8.466700 \tl:10.452261 \tem:0.050000\n",
      "\n",
      "Epoch performance:  8.6869sec Trl:10.515516 \tTrem:0.016667 \tTeem:0.005102\n",
      "\n",
      "Epoch:  9 / 300\n",
      "Batch:\t0 /9\t:  1.000592 1.000606 \tl:10.549276 \tem:0.000000\n",
      "Batch:\t1 /9\t:  0.935729 1.936899 \tl:10.535236 \tem:0.050000\n",
      "Batch:\t2 /9\t:  1.156203 3.093600 \tl:10.508226 \tem:0.000000\n",
      "Batch:\t3 /9\t:  1.074165 4.168594 \tl:10.538727 \tem:0.000000\n",
      "Batch:\t4 /9\t:  0.823412 4.992627 \tl:10.454759 \tem:0.050000\n",
      "Batch:\t5 /9\t:  0.932692 5.925856 \tl:10.580606 \tem:0.000000\n",
      "Batch:\t6 /9\t:  0.929433 6.855581 \tl:10.502716 \tem:0.000000\n",
      "Batch:\t7 /9\t:  1.048679 7.906568 \tl:10.552233 \tem:0.000000\n",
      "Batch:\t8 /9\t:  1.073556 8.981379 \tl:10.553595 \tem:0.000000\n",
      "\n",
      "Epoch performance:  9.1900sec Trl:10.530597 \tTrem:0.011111 \tTeem:0.005102\n",
      "\n",
      "Epoch:  10 / 300\n",
      "Batch:\t0 /9\t:  1.089272 1.089292 \tl:10.551702 \tem:0.000000\n",
      "Batch:\t1 /9\t:  0.909579 2.000143 \tl:10.456604 \tem:0.000000\n",
      "Batch:\t2 /9\t:  0.891180 2.892409 \tl:10.599269 \tem:0.000000\n",
      "Batch:\t3 /9\t:  1.171139 4.064189 \tl:10.407129 \tem:0.050000\n",
      "Batch:\t4 /9\t:  0.990962 5.055934 \tl:10.547525 \tem:0.000000\n",
      "Batch:\t5 /9\t:  0.891265 5.949961 \tl:10.500056 \tem:0.050000\n",
      "Batch:\t6 /9\t:  0.907845 6.858985 \tl:10.399727 \tem:0.100000\n",
      "Batch:\t7 /9\t:  0.980316 7.840508 \tl:10.352882 \tem:0.050000\n",
      "Batch:\t8 /9\t:  0.943388 8.784996 \tl:10.502068 \tem:0.000000\n",
      "\n",
      "Epoch performance:  9.0399sec Trl:10.479663 \tTrem:0.027778 \tTeem:0.005102\n",
      "\n",
      "Epoch:  11 / 300\n",
      "Batch:\t0 /9\t:  0.981100 0.981114 \tl:10.551001 \tem:0.000000\n",
      "Batch:\t1 /9\t:  1.131911 2.114006 \tl:10.550125 \tem:0.000000\n",
      "Batch:\t2 /9\t:  0.900895 3.016237 \tl:10.263325 \tem:0.050000\n",
      "Batch:\t3 /9\t:  1.172415 4.189843 \tl:10.446221 \tem:0.000000\n",
      "Batch:\t4 /9\t:  1.049298 5.240305 \tl:10.352055 \tem:0.050000\n",
      "Batch:\t5 /9\t:  0.877900 6.120048 \tl:10.452991 \tem:0.050000\n",
      "Batch:\t6 /9\t:  1.001955 7.123351 \tl:10.598045 \tem:0.000000\n",
      "Batch:\t7 /9\t:  0.816386 7.940575 \tl:10.400019 \tem:0.050000\n",
      "Batch:\t8 /9\t:  0.920825 8.862011 \tl:10.519352 \tem:0.000000\n",
      "\n",
      "Epoch performance:  9.0671sec Trl:10.459237 \tTrem:0.022222 \tTeem:0.005102\n",
      "\n",
      "Epoch:  12 / 300\n",
      "Batch:\t0 /9\t:  1.056928 1.056949 \tl:10.442752 \tem:0.050000\n",
      "Batch:\t1 /9\t:  0.938209 1.996535 \tl:10.497801 \tem:0.000000\n",
      "Batch:\t2 /9\t:  0.830269 2.827361 \tl:10.517525 \tem:0.000000\n",
      "Batch:\t3 /9\t:  0.943530 3.771559 \tl:10.544222 \tem:0.000000\n",
      "Batch:\t4 /9\t:  0.925035 4.698219 \tl:10.400999 \tem:0.050000\n",
      "Batch:\t5 /9\t:  1.316332 6.015726 \tl:10.549814 \tem:0.000000\n",
      "Batch:\t6 /9\t:  1.030922 7.047579 \tl:10.457684 \tem:0.050000\n",
      "Batch:\t7 /9\t:  0.918735 7.966897 \tl:10.499818 \tem:0.000000\n",
      "Batch:\t8 /9\t:  0.979071 8.947328 \tl:10.448126 \tem:0.000000\n",
      "\n",
      "Epoch performance:  9.1500sec Trl:10.484304 \tTrem:0.016667 \tTeem:0.005102\n",
      "\n",
      "Epoch:  13 / 300\n",
      "Batch:\t0 /9\t:  0.981313 0.981341 \tl:10.446301 \tem:0.050000\n",
      "Batch:\t1 /9\t:  0.955934 1.938472 \tl:10.548771 \tem:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t2 /9\t:  1.100679 3.040342 \tl:10.548409 \tem:0.000000\n",
      "Batch:\t3 /9\t:  1.005305 4.046427 \tl:10.593116 \tem:0.000000\n",
      "Batch:\t4 /9\t:  0.901406 4.948894 \tl:10.539362 \tem:0.000000\n",
      "Batch:\t5 /9\t:  0.867437 5.817045 \tl:10.495605 \tem:0.000000\n",
      "Batch:\t6 /9\t:  1.045692 6.864530 \tl:10.486691 \tem:0.000000\n",
      "Batch:\t7 /9\t:  0.858674 7.723875 \tl:10.542745 \tem:0.000000\n",
      "Batch:\t8 /9\t:  0.854707 8.579676 \tl:10.429316 \tem:0.000000\n",
      "\n",
      "Epoch performance:  8.7814sec Trl:10.514480 \tTrem:0.005556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  14 / 300\n",
      "Batch:\t0 /9\t:  1.052320 1.052338 \tl:10.560362 \tem:0.050000\n",
      "Batch:\t1 /9\t:  0.865715 1.918521 \tl:10.485687 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.873708 2.793241 \tl:10.483668 \tem:0.050000\n",
      "Batch:\t3 /9\t:  0.906761 3.701382 \tl:10.216731 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.932715 4.635413 \tl:10.353626 \tem:0.050000\n",
      "Batch:\t5 /9\t:  1.090069 5.726673 \tl:10.437686 \tem:0.100000\n",
      "Batch:\t6 /9\t:  0.894393 6.622340 \tl:10.543930 \tem:0.000000\n",
      "Batch:\t7 /9\t:  1.278126 7.901498 \tl:10.443743 \tem:0.000000\n",
      "Batch:\t8 /9\t:  1.107760 9.010048 \tl:10.419058 \tem:0.050000\n",
      "\n",
      "Epoch performance:  9.2184sec Trl:10.438277 \tTrem:0.061111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  15 / 300\n",
      "Batch:\t0 /9\t:  0.974940 0.974961 \tl:10.378227 \tem:0.050000\n",
      "Batch:\t1 /9\t:  0.952354 1.928438 \tl:10.347723 \tem:0.050000\n",
      "Batch:\t2 /9\t:  0.904042 2.833281 \tl:10.269682 \tem:0.100000\n",
      "Batch:\t3 /9\t:  1.055149 3.889195 \tl:10.222081 \tem:0.100000\n",
      "Batch:\t4 /9\t:  0.915426 4.804908 \tl:10.534048 \tem:0.000000\n",
      "Batch:\t5 /9\t:  1.000300 5.806251 \tl:10.429425 \tem:0.050000\n",
      "Batch:\t6 /9\t:  1.133208 6.940099 \tl:10.348478 \tem:0.050000\n",
      "Batch:\t7 /9\t:  0.819259 7.759904 \tl:10.454233 \tem:0.050000\n",
      "Batch:\t8 /9\t:  0.949140 8.709650 \tl:10.475831 \tem:0.050000\n",
      "\n",
      "Epoch performance:  8.9609sec Trl:10.384414 \tTrem:0.055556 \tTeem:0.005102\n",
      "\n",
      "Epoch:  16 / 300\n",
      "Batch:\t0 /9\t:  1.058238 1.058259 \tl:10.442651 \tem:0.000000\n",
      "Batch:\t1 /9\t:  0.915970 1.975023 \tl:10.397446 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.902065 2.878000 \tl:10.405529 \tem:0.000000\n",
      "Batch:\t3 /9\t:  0.991248 3.870636 \tl:10.311281 \tem:0.050000\n",
      "Batch:\t4 /9\t:  1.150575 5.021874 \tl:10.439425 \tem:0.000000\n",
      "Batch:\t5 /9\t:  1.060048 6.082130 \tl:10.331276 \tem:0.050000\n",
      "Batch:\t6 /9\t:  1.136052 7.220782 \tl:10.405667 \tem:0.000000\n",
      "Batch:\t7 /9\t:  1.117105 8.339478 \tl:10.135778 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.908270 9.248337 \tl:10.373476 \tem:0.150000\n",
      "\n",
      "Epoch performance:  9.4551sec Trl:10.360281 \tTrem:0.055556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  17 / 300\n",
      "Batch:\t0 /9\t:  0.966417 0.966435 \tl:10.408371 \tem:0.000000\n",
      "Batch:\t1 /9\t:  1.087106 2.054779 \tl:10.170773 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.973845 3.029606 \tl:10.470430 \tem:0.050000\n",
      "Batch:\t3 /9\t:  1.104566 4.134921 \tl:10.476560 \tem:0.000000\n",
      "Batch:\t4 /9\t:  1.174230 5.311791 \tl:10.306070 \tem:0.050000\n",
      "Batch:\t5 /9\t:  1.012583 6.324887 \tl:10.163316 \tem:0.100000\n",
      "Batch:\t6 /9\t:  0.857737 7.183400 \tl:10.331818 \tem:0.150000\n",
      "Batch:\t7 /9\t:  1.074559 8.258929 \tl:10.039059 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.900748 9.161070 \tl:10.257215 \tem:0.100000\n",
      "\n",
      "Epoch performance:  9.3889sec Trl:10.291512 \tTrem:0.088889 \tTeem:0.000000\n",
      "\n",
      "Epoch:  18 / 300\n",
      "Batch:\t0 /9\t:  1.159317 1.159338 \tl:10.098862 \tem:0.150000\n",
      "Batch:\t1 /9\t:  1.048931 2.209327 \tl:10.249058 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.925244 3.135275 \tl:10.433403 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.985352 4.121861 \tl:10.139687 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.871027 4.993422 \tl:10.283254 \tem:0.150000\n",
      "Batch:\t5 /9\t:  0.876343 5.869915 \tl:10.320681 \tem:0.100000\n",
      "Batch:\t6 /9\t:  0.936026 6.807181 \tl:10.409321 \tem:0.000000\n",
      "Batch:\t7 /9\t:  1.097749 7.906322 \tl:10.413994 \tem:0.000000\n",
      "Batch:\t8 /9\t:  0.987545 8.894425 \tl:10.401222 \tem:0.050000\n",
      "\n",
      "Epoch performance:  9.0715sec Trl:10.305498 \tTrem:0.088889 \tTeem:0.010204\n",
      "\n",
      "Epoch:  19 / 300\n",
      "Batch:\t0 /9\t:  0.983247 0.983267 \tl:10.382109 \tem:0.000000\n",
      "Batch:\t1 /9\t:  0.874181 1.858621 \tl:10.349791 \tem:0.000000\n",
      "Batch:\t2 /9\t:  0.964404 2.823225 \tl:10.070179 \tem:0.200000\n",
      "Batch:\t3 /9\t:  1.033697 3.857929 \tl:10.064184 \tem:0.150000\n",
      "Batch:\t4 /9\t:  1.028856 4.888703 \tl:10.206503 \tem:0.150000\n",
      "Batch:\t5 /9\t:  1.093029 5.982902 \tl:10.148087 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.810379 6.794631 \tl:10.335472 \tem:0.050000\n",
      "Batch:\t7 /9\t:  0.855549 7.651365 \tl:10.292150 \tem:0.100000\n",
      "Batch:\t8 /9\t:  0.864009 8.516139 \tl:10.352129 \tem:0.050000\n",
      "\n",
      "Epoch performance:  8.7532sec Trl:10.244511 \tTrem:0.094444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  20 / 300\n",
      "Batch:\t0 /9\t:  1.162134 1.162154 \tl:10.387743 \tem:0.000000\n",
      "Batch:\t1 /9\t:  1.031336 2.194798 \tl:10.153006 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.967953 3.163207 \tl:10.234379 \tem:0.050000\n",
      "Batch:\t3 /9\t:  0.925323 4.088829 \tl:10.210207 \tem:0.050000\n",
      "Batch:\t4 /9\t:  0.876409 4.965903 \tl:10.207426 \tem:0.050000\n",
      "Batch:\t5 /9\t:  0.848295 5.814509 \tl:10.171874 \tem:0.100000\n",
      "Batch:\t6 /9\t:  1.028337 6.843128 \tl:10.508375 \tem:0.000000\n",
      "Batch:\t7 /9\t:  1.053269 7.897164 \tl:10.210526 \tem:0.100000\n",
      "Batch:\t8 /9\t:  0.865777 8.764103 \tl:10.111487 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.9650sec Trl:10.243892 \tTrem:0.077778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  21 / 300\n",
      "Batch:\t0 /9\t:  1.026695 1.026715 \tl:10.274428 \tem:0.050000\n",
      "Batch:\t1 /9\t:  0.927834 1.957504 \tl:10.459028 \tem:0.000000\n",
      "Batch:\t2 /9\t:  0.890808 2.849699 \tl:10.125767 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.878302 3.729102 \tl:10.168135 \tem:0.150000\n",
      "Batch:\t4 /9\t:  1.055559 4.785569 \tl:10.193638 \tem:0.100000\n",
      "Batch:\t5 /9\t:  1.072828 5.859017 \tl:10.291256 \tem:0.050000\n",
      "Batch:\t6 /9\t:  0.823771 6.683526 \tl:10.499114 \tem:0.000000\n",
      "Batch:\t7 /9\t:  0.829568 7.513589 \tl:10.062468 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.918412 8.433001 \tl:10.246793 \tem:0.100000\n",
      "\n",
      "Epoch performance:  8.6413sec Trl:10.257847 \tTrem:0.077778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  22 / 300\n",
      "Batch:\t0 /9\t:  1.161989 1.162007 \tl:10.276063 \tem:0.000000\n",
      "Batch:\t1 /9\t:  0.984858 2.148218 \tl:10.262800 \tem:0.050000\n",
      "Batch:\t2 /9\t:  0.861782 3.010468 \tl:10.292047 \tem:0.050000\n",
      "Batch:\t3 /9\t:  0.957693 3.969546 \tl:10.116256 \tem:0.150000\n",
      "Batch:\t4 /9\t:  1.123152 5.093270 \tl:10.311175 \tem:0.100000\n",
      "Batch:\t5 /9\t:  1.016457 6.110727 \tl:10.336884 \tem:0.000000\n",
      "Batch:\t6 /9\t:  0.954458 7.068316 \tl:10.346195 \tem:0.000000\n",
      "Batch:\t7 /9\t:  1.101583 8.171208 \tl:10.016588 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.862761 9.035724 \tl:10.450972 \tem:0.000000\n",
      "\n",
      "Epoch performance:  9.2375sec Trl:10.267664 \tTrem:0.055556 \tTeem:0.005102\n",
      "\n",
      "Epoch:  23 / 300\n",
      "Batch:\t0 /9\t:  0.837604 0.837623 \tl:10.093882 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.952795 1.791364 \tl:10.091856 \tem:0.200000\n",
      "Batch:\t2 /9\t:  1.074465 2.866247 \tl:10.321467 \tem:0.050000\n",
      "Batch:\t3 /9\t:  0.855899 3.722722 \tl:10.304329 \tem:0.050000\n",
      "Batch:\t4 /9\t:  0.883151 4.606532 \tl:10.097313 \tem:0.150000\n",
      "Batch:\t5 /9\t:  1.181737 5.789628 \tl:10.180415 \tem:0.050000\n",
      "Batch:\t6 /9\t:  1.020208 6.810671 \tl:10.217290 \tem:0.100000\n",
      "Batch:\t7 /9\t:  0.878373 7.690423 \tl:9.792133 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.857328 8.548125 \tl:10.328508 \tem:0.050000\n",
      "\n",
      "Epoch performance:  8.7536sec Trl:10.158577 \tTrem:0.116667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  24 / 300\n",
      "Batch:\t0 /9\t:  1.155171 1.155190 \tl:9.919509 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.997084 2.153482 \tl:10.100484 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.902918 3.056896 \tl:10.248049 \tem:0.050000\n",
      "Batch:\t3 /9\t:  0.946281 4.004525 \tl:10.363646 \tem:0.050000\n",
      "Batch:\t4 /9\t:  0.948767 4.953958 \tl:10.129133 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.868050 5.822505 \tl:10.052053 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.896486 6.720242 \tl:10.305289 \tem:0.050000\n",
      "Batch:\t7 /9\t:  0.977624 7.698420 \tl:9.922370 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.931125 8.630352 \tl:10.066900 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.8389sec Trl:10.123048 \tTrem:0.133333 \tTeem:0.005102\n",
      "\n",
      "Epoch:  25 / 300\n",
      "Batch:\t0 /9\t:  0.994873 0.994892 \tl:10.259140 \tem:0.100000\n",
      "Batch:\t1 /9\t:  1.035773 2.031768 \tl:9.930463 \tem:0.200000\n",
      "Batch:\t2 /9\t:  1.083089 3.115873 \tl:10.212984 \tem:0.050000\n",
      "Batch:\t3 /9\t:  0.866583 3.983775 \tl:10.168261 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.855216 4.840198 \tl:10.216766 \tem:0.150000\n",
      "Batch:\t5 /9\t:  0.853739 5.695291 \tl:10.238222 \tem:0.050000\n",
      "Batch:\t6 /9\t:  0.983663 6.679563 \tl:10.250843 \tem:0.000000\n",
      "Batch:\t7 /9\t:  0.865250 7.545878 \tl:9.881504 \tem:0.200000\n",
      "Batch:\t8 /9\t:  1.213879 8.760636 \tl:10.370312 \tem:0.000000\n",
      "\n",
      "Epoch performance:  8.9970sec Trl:10.169833 \tTrem:0.100000 \tTeem:0.005102\n",
      "\n",
      "Epoch:  26 / 300\n",
      "Batch:\t0 /9\t:  1.086173 1.086192 \tl:10.221555 \tem:0.100000\n",
      "Batch:\t1 /9\t:  0.863498 1.950884 \tl:10.163015 \tem:0.050000\n",
      "Batch:\t2 /9\t:  1.024245 2.976103 \tl:10.160191 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.865662 3.842373 \tl:9.998130 \tem:0.150000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t4 /9\t:  0.988896 4.832195 \tl:9.955313 \tem:0.100000\n",
      "Batch:\t5 /9\t:  0.871320 5.704482 \tl:10.167356 \tem:0.150000\n",
      "Batch:\t6 /9\t:  1.022459 6.727951 \tl:10.066646 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.948024 7.676738 \tl:9.792419 \tem:0.150000\n",
      "Batch:\t8 /9\t:  1.025294 8.702863 \tl:9.943556 \tem:0.200000\n",
      "\n",
      "Epoch performance:  8.9101sec Trl:10.052020 \tTrem:0.133333 \tTeem:0.005102\n",
      "\n",
      "Epoch:  27 / 300\n",
      "Batch:\t0 /9\t:  1.106229 1.106266 \tl:10.051878 \tem:0.100000\n",
      "Batch:\t1 /9\t:  1.004745 2.112704 \tl:10.020861 \tem:0.100000\n",
      "Batch:\t2 /9\t:  1.102099 3.218211 \tl:9.933932 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.860085 4.079150 \tl:10.124531 \tem:0.100000\n",
      "Batch:\t4 /9\t:  0.901213 4.981445 \tl:10.105121 \tem:0.100000\n",
      "Batch:\t5 /9\t:  1.152690 6.135529 \tl:10.041957 \tem:0.050000\n",
      "Batch:\t6 /9\t:  1.050550 7.187495 \tl:10.113184 \tem:0.100000\n",
      "Batch:\t7 /9\t:  0.825277 8.013928 \tl:10.262756 \tem:0.000000\n",
      "Batch:\t8 /9\t:  0.940881 8.955470 \tl:10.262173 \tem:0.100000\n",
      "\n",
      "Epoch performance:  9.1308sec Trl:10.101821 \tTrem:0.083333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  28 / 300\n",
      "Batch:\t0 /9\t:  0.815142 0.815162 \tl:10.058227 \tem:0.100000\n",
      "Batch:\t1 /9\t:  0.975885 1.793845 \tl:10.169230 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.899839 2.693935 \tl:9.804596 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.865179 3.559692 \tl:10.063750 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.863463 4.423649 \tl:9.531748 \tem:0.350000\n",
      "Batch:\t5 /9\t:  0.888058 5.312085 \tl:10.154442 \tem:0.050000\n",
      "Batch:\t6 /9\t:  0.842301 6.155607 \tl:10.012308 \tem:0.050000\n",
      "Batch:\t7 /9\t:  0.918066 7.074009 \tl:10.016418 \tem:0.100000\n",
      "Batch:\t8 /9\t:  0.933526 8.008789 \tl:10.115807 \tem:0.050000\n",
      "\n",
      "Epoch performance:  8.2266sec Trl:9.991836 \tTrem:0.122222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  29 / 300\n",
      "Batch:\t0 /9\t:  0.977349 0.977370 \tl:9.762074 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.895280 1.874040 \tl:10.068934 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.962421 2.837840 \tl:10.158332 \tem:0.100000\n",
      "Batch:\t3 /9\t:  1.043622 3.882662 \tl:10.210247 \tem:0.050000\n",
      "Batch:\t4 /9\t:  0.868232 4.752046 \tl:9.973450 \tem:0.100000\n",
      "Batch:\t5 /9\t:  0.900449 5.653731 \tl:9.911344 \tem:0.200000\n",
      "Batch:\t6 /9\t:  1.076776 6.731248 \tl:10.112900 \tem:0.100000\n",
      "Batch:\t7 /9\t:  1.058436 7.790918 \tl:10.164198 \tem:0.100000\n",
      "Batch:\t8 /9\t:  0.907205 8.699003 \tl:10.281446 \tem:0.050000\n",
      "\n",
      "Epoch performance:  8.9108sec Trl:10.071436 \tTrem:0.122222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  30 / 300\n",
      "Batch:\t0 /9\t:  0.999583 0.999603 \tl:10.013608 \tem:0.050000\n",
      "Batch:\t1 /9\t:  1.143237 2.143661 \tl:10.106849 \tem:0.000000\n",
      "Batch:\t2 /9\t:  1.003737 3.148816 \tl:9.830954 \tem:0.150000\n",
      "Batch:\t3 /9\t:  1.092394 4.242217 \tl:10.197893 \tem:0.100000\n",
      "Batch:\t4 /9\t:  1.045235 5.287669 \tl:10.163212 \tem:0.100000\n",
      "Batch:\t5 /9\t:  0.885068 6.173799 \tl:10.025226 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.850016 7.024349 \tl:9.717914 \tem:0.200000\n",
      "Batch:\t7 /9\t:  1.080640 8.105609 \tl:10.142998 \tem:0.150000\n",
      "Batch:\t8 /9\t:  1.088917 9.195598 \tl:9.967348 \tem:0.200000\n",
      "\n",
      "Epoch performance:  9.4014sec Trl:10.018444 \tTrem:0.133333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  31 / 300\n",
      "Batch:\t0 /9\t:  0.966806 0.966828 \tl:10.008495 \tem:0.150000\n",
      "Batch:\t1 /9\t:  1.004415 1.972090 \tl:10.111801 \tem:0.200000\n",
      "Batch:\t2 /9\t:  1.046200 3.019438 \tl:9.994377 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.898699 3.919358 \tl:10.262554 \tem:0.000000\n",
      "Batch:\t4 /9\t:  1.222460 5.142668 \tl:10.046467 \tem:0.250000\n",
      "Batch:\t5 /9\t:  1.073939 6.216887 \tl:10.163666 \tem:0.100000\n",
      "Batch:\t6 /9\t:  1.075169 7.292692 \tl:10.023674 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.994379 8.288044 \tl:9.863384 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.896867 9.186481 \tl:9.614022 \tem:0.300000\n",
      "\n",
      "Epoch performance:  9.3899sec Trl:10.009827 \tTrem:0.172222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  32 / 300\n",
      "Batch:\t0 /9\t:  1.082950 1.082968 \tl:10.163876 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.874264 1.957510 \tl:10.012880 \tem:0.050000\n",
      "Batch:\t2 /9\t:  0.909341 2.867826 \tl:9.970571 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.872334 3.741340 \tl:9.963499 \tem:0.100000\n",
      "Batch:\t4 /9\t:  0.987730 4.730027 \tl:9.917280 \tem:0.200000\n",
      "Batch:\t5 /9\t:  1.236878 5.967955 \tl:10.013329 \tem:0.150000\n",
      "Batch:\t6 /9\t:  1.051290 7.020839 \tl:9.968071 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.916975 7.938868 \tl:10.012735 \tem:0.050000\n",
      "Batch:\t8 /9\t:  0.923973 8.863481 \tl:9.884180 \tem:0.100000\n",
      "\n",
      "Epoch performance:  9.0670sec Trl:9.989602 \tTrem:0.138889 \tTeem:0.000000\n",
      "\n",
      "Epoch:  33 / 300\n",
      "Batch:\t0 /9\t:  1.008086 1.008105 \tl:9.962008 \tem:0.050000\n",
      "Batch:\t1 /9\t:  0.921993 1.930943 \tl:9.714019 \tem:0.300000\n",
      "Batch:\t2 /9\t:  1.087976 3.019479 \tl:9.613546 \tem:0.350000\n",
      "Batch:\t3 /9\t:  0.866938 3.887027 \tl:10.075193 \tem:0.150000\n",
      "Batch:\t4 /9\t:  1.044414 4.932217 \tl:9.963594 \tem:0.150000\n",
      "Batch:\t5 /9\t:  0.940949 5.874264 \tl:10.013093 \tem:0.100000\n",
      "Batch:\t6 /9\t:  1.060573 6.936064 \tl:9.763228 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.877220 7.814316 \tl:9.962357 \tem:0.050000\n",
      "Batch:\t8 /9\t:  0.938859 8.754148 \tl:10.112831 \tem:0.100000\n",
      "\n",
      "Epoch performance:  8.9614sec Trl:9.908874 \tTrem:0.155556 \tTeem:0.005102\n",
      "\n",
      "Epoch:  34 / 300\n",
      "Batch:\t0 /9\t:  0.924658 0.924677 \tl:10.020406 \tem:0.100000\n",
      "Batch:\t1 /9\t:  0.864297 1.789730 \tl:9.971023 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.926540 2.716838 \tl:9.913572 \tem:0.300000\n",
      "Batch:\t3 /9\t:  1.019683 3.737625 \tl:9.763719 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.985578 4.723509 \tl:10.063200 \tem:0.100000\n",
      "Batch:\t5 /9\t:  0.850862 5.574910 \tl:9.863330 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.817894 6.393538 \tl:9.912186 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.856251 7.250776 \tl:10.015055 \tem:0.100000\n",
      "Batch:\t8 /9\t:  0.994504 8.246006 \tl:9.757406 \tem:0.200000\n",
      "\n",
      "Epoch performance:  8.4477sec Trl:9.919988 \tTrem:0.183333 \tTeem:0.005102\n",
      "\n",
      "Epoch:  35 / 300\n",
      "Batch:\t0 /9\t:  0.796893 0.796907 \tl:9.914232 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.901240 1.698966 \tl:10.126650 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.954444 2.654135 \tl:9.513706 \tem:0.350000\n",
      "Batch:\t3 /9\t:  0.941900 3.597333 \tl:9.663207 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.859158 4.457815 \tl:10.061571 \tem:0.050000\n",
      "Batch:\t5 /9\t:  0.900237 5.359166 \tl:10.007284 \tem:0.100000\n",
      "Batch:\t6 /9\t:  1.060585 6.422844 \tl:9.862450 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.923067 7.347149 \tl:9.810354 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.867795 8.216022 \tl:10.160745 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.4431sec Trl:9.902244 \tTrem:0.166667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  36 / 300\n",
      "Batch:\t0 /9\t:  1.010714 1.010741 \tl:9.713694 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.927896 1.938994 \tl:10.062517 \tem:0.050000\n",
      "Batch:\t2 /9\t:  0.811995 2.751955 \tl:9.862663 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.899636 3.652704 \tl:9.662971 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.919233 4.572555 \tl:9.862372 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.960258 5.533349 \tl:9.920628 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.859848 6.394515 \tl:9.813652 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.902691 7.298357 \tl:9.763224 \tem:0.150000\n",
      "Batch:\t8 /9\t:  1.204916 8.504611 \tl:10.012384 \tem:0.100000\n",
      "\n",
      "Epoch performance:  8.7193sec Trl:9.852678 \tTrem:0.166667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  37 / 300\n",
      "Batch:\t0 /9\t:  1.070958 1.070997 \tl:10.013452 \tem:0.150000\n",
      "Batch:\t1 /9\t:  1.033396 2.105597 \tl:10.161648 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.907596 3.014081 \tl:9.862961 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.989243 4.004036 \tl:10.008939 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.976640 4.981885 \tl:9.665305 \tem:0.400000\n",
      "Batch:\t5 /9\t:  0.856373 5.839200 \tl:10.263618 \tem:0.050000\n",
      "Batch:\t6 /9\t:  0.863831 6.704061 \tl:9.913748 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.979152 7.684568 \tl:9.763216 \tem:0.300000\n",
      "Batch:\t8 /9\t:  0.874386 8.559506 \tl:10.012422 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.7627sec Trl:9.962812 \tTrem:0.188889 \tTeem:0.000000\n",
      "\n",
      "Epoch:  38 / 300\n",
      "Batch:\t0 /9\t:  0.849761 0.849797 \tl:9.663944 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.950688 1.801753 \tl:9.949482 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.849716 2.652012 \tl:9.965181 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.863831 3.517163 \tl:10.115769 \tem:0.100000\n",
      "Batch:\t4 /9\t:  1.105061 4.623211 \tl:9.773108 \tem:0.300000\n",
      "Batch:\t5 /9\t:  1.159306 5.783193 \tl:10.346325 \tem:0.100000\n",
      "Batch:\t6 /9\t:  1.058782 6.843198 \tl:9.812809 \tem:0.300000\n",
      "Batch:\t7 /9\t:  1.038576 7.882631 \tl:9.763345 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.868106 8.751703 \tl:10.111862 \tem:0.050000\n",
      "\n",
      "Epoch performance:  8.9892sec Trl:9.944647 \tTrem:0.177778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  39 / 300\n",
      "Batch:\t0 /9\t:  1.175549 1.175569 \tl:9.862862 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.855617 2.031439 \tl:10.211270 \tem:0.100000\n",
      "Batch:\t2 /9\t:  1.087014 3.118731 \tl:9.896189 \tem:0.150000\n",
      "Batch:\t3 /9\t:  1.108829 4.228155 \tl:9.957479 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.864808 5.093611 \tl:9.697973 \tem:0.300000\n",
      "Batch:\t5 /9\t:  0.844083 5.937989 \tl:10.011089 \tem:0.150000\n",
      "Batch:\t6 /9\t:  1.035435 6.974714 \tl:9.760719 \tem:0.200000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t7 /9\t:  1.106374 8.082388 \tl:9.606195 \tem:0.300000\n",
      "Batch:\t8 /9\t:  0.867517 8.950722 \tl:10.033813 \tem:0.100000\n",
      "\n",
      "Epoch performance:  9.1622sec Trl:9.893066 \tTrem:0.177778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  40 / 300\n",
      "Batch:\t0 /9\t:  1.030234 1.030256 \tl:10.266333 \tem:0.100000\n",
      "Batch:\t1 /9\t:  1.127676 2.158873 \tl:9.719887 \tem:0.250000\n",
      "Batch:\t2 /9\t:  1.057253 3.217629 \tl:9.715950 \tem:0.350000\n",
      "Batch:\t3 /9\t:  1.037245 4.255633 \tl:9.812652 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.913045 5.169729 \tl:9.761528 \tem:0.300000\n",
      "Batch:\t5 /9\t:  0.979473 6.150536 \tl:9.863295 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.999301 7.150414 \tl:10.113524 \tem:0.050000\n",
      "Batch:\t7 /9\t:  0.914546 8.065405 \tl:10.112679 \tem:0.100000\n",
      "Batch:\t8 /9\t:  1.026365 9.092551 \tl:9.714720 \tem:0.200000\n",
      "\n",
      "Epoch performance:  9.3268sec Trl:9.897841 \tTrem:0.183333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  41 / 300\n",
      "Batch:\t0 /9\t:  1.024530 1.024548 \tl:9.560638 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.907291 1.932854 \tl:9.864292 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.914049 2.849305 \tl:9.763662 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.975278 3.824787 \tl:10.016612 \tem:0.100000\n",
      "Batch:\t4 /9\t:  0.862551 4.687871 \tl:9.764194 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.856842 5.545069 \tl:9.649442 \tem:0.300000\n",
      "Batch:\t6 /9\t:  0.986873 6.532865 \tl:10.213367 \tem:0.050000\n",
      "Batch:\t7 /9\t:  0.969666 7.503139 \tl:9.912676 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.865691 8.369607 \tl:10.013094 \tem:0.100000\n",
      "\n",
      "Epoch performance:  8.5775sec Trl:9.861997 \tTrem:0.194444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  42 / 300\n",
      "Batch:\t0 /9\t:  1.051561 1.051581 \tl:9.615084 \tem:0.250000\n",
      "Batch:\t1 /9\t:  1.090065 2.142945 \tl:9.775005 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.893272 3.037414 \tl:9.962440 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.862977 3.901193 \tl:10.012549 \tem:0.100000\n",
      "Batch:\t4 /9\t:  1.011941 4.915930 \tl:10.063025 \tem:0.050000\n",
      "Batch:\t5 /9\t:  1.090859 6.007539 \tl:9.760762 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.866947 6.875132 \tl:10.012014 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.853646 7.729057 \tl:10.011509 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.912003 8.642085 \tl:9.694518 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.8907sec Trl:9.878545 \tTrem:0.183333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  43 / 300\n",
      "Batch:\t0 /9\t:  1.036772 1.036790 \tl:10.062425 \tem:0.100000\n",
      "Batch:\t1 /9\t:  0.868659 1.906049 \tl:10.162470 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.899311 2.806046 \tl:9.844572 \tem:0.050000\n",
      "Batch:\t3 /9\t:  1.119745 3.926451 \tl:9.864781 \tem:0.100000\n",
      "Batch:\t4 /9\t:  1.005236 4.931960 \tl:9.763012 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.825815 5.758497 \tl:9.763093 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.889078 6.648572 \tl:9.962809 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.919625 7.569417 \tl:9.910809 \tem:0.200000\n",
      "Batch:\t8 /9\t:  1.071949 8.642363 \tl:9.913132 \tem:0.100000\n",
      "\n",
      "Epoch performance:  8.8478sec Trl:9.916345 \tTrem:0.150000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  44 / 300\n",
      "Batch:\t0 /9\t:  0.886901 0.886919 \tl:10.112465 \tem:0.050000\n",
      "Batch:\t1 /9\t:  1.013517 1.901347 \tl:9.811487 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.960119 2.862318 \tl:9.958549 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.818214 3.681058 \tl:9.613390 \tem:0.350000\n",
      "Batch:\t4 /9\t:  0.858566 4.540630 \tl:9.798617 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.906425 5.448471 \tl:9.862255 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.974756 6.424757 \tl:9.807232 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.775164 7.200452 \tl:9.864092 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.856167 8.056857 \tl:9.866289 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.2563sec Trl:9.854931 \tTrem:0.188889 \tTeem:0.000000\n",
      "\n",
      "Epoch:  45 / 300\n",
      "Batch:\t0 /9\t:  1.052357 1.052375 \tl:9.812959 \tem:0.200000\n",
      "Batch:\t1 /9\t:  0.923058 1.976259 \tl:9.564177 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.871358 2.848371 \tl:9.767485 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.853352 3.702578 \tl:9.763653 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.907402 4.610526 \tl:9.559622 \tem:0.350000\n",
      "Batch:\t5 /9\t:  0.824923 5.435616 \tl:10.013345 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.858779 6.295197 \tl:9.763563 \tem:0.300000\n",
      "Batch:\t7 /9\t:  0.902262 7.197701 \tl:9.813153 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.944530 8.143456 \tl:9.712438 \tem:0.300000\n",
      "\n",
      "Epoch performance:  8.3533sec Trl:9.752266 \tTrem:0.233333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  46 / 300\n",
      "Batch:\t0 /9\t:  1.063915 1.063934 \tl:9.862420 \tem:0.200000\n",
      "Batch:\t1 /9\t:  0.915665 1.980999 \tl:9.922268 \tem:0.100000\n",
      "Batch:\t2 /9\t:  1.018347 2.999985 \tl:10.062708 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.975389 3.976101 \tl:9.963138 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.806105 4.783398 \tl:9.563147 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.906498 5.690369 \tl:9.914291 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.905416 6.596451 \tl:9.916016 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.969094 7.566684 \tl:9.808334 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.856473 8.424545 \tl:9.480491 \tem:0.400000\n",
      "\n",
      "Epoch performance:  8.6312sec Trl:9.832535 \tTrem:0.194444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  47 / 300\n",
      "Batch:\t0 /9\t:  1.017770 1.017789 \tl:9.908491 \tem:0.200000\n",
      "Batch:\t1 /9\t:  0.946388 1.964733 \tl:10.066173 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.936098 2.902087 \tl:9.762987 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.834835 3.737123 \tl:9.706982 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.892761 4.630426 \tl:9.911491 \tem:0.150000\n",
      "Batch:\t5 /9\t:  1.211645 5.843083 \tl:9.983345 \tem:0.150000\n",
      "Batch:\t6 /9\t:  1.075075 6.919713 \tl:9.763447 \tem:0.300000\n",
      "Batch:\t7 /9\t:  1.019062 7.939101 \tl:9.815757 \tem:0.150000\n",
      "Batch:\t8 /9\t:  1.032214 8.971972 \tl:9.763276 \tem:0.200000\n",
      "\n",
      "Epoch performance:  9.1992sec Trl:9.853550 \tTrem:0.194444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  48 / 300\n",
      "Batch:\t0 /9\t:  1.037431 1.037450 \tl:9.925480 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.877254 1.915898 \tl:9.863413 \tem:0.150000\n",
      "Batch:\t2 /9\t:  1.049381 2.966955 \tl:9.813095 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.948555 3.916320 \tl:10.017153 \tem:0.050000\n",
      "Batch:\t4 /9\t:  1.066306 4.983377 \tl:9.235240 \tem:0.550000\n",
      "Batch:\t5 /9\t:  1.026571 6.010470 \tl:9.862679 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.893096 6.904245 \tl:9.759314 \tem:0.200000\n",
      "Batch:\t7 /9\t:  1.008501 7.914756 \tl:9.615256 \tem:0.350000\n",
      "Batch:\t8 /9\t:  0.972313 8.888063 \tl:9.767020 \tem:0.250000\n",
      "\n",
      "Epoch performance:  9.1023sec Trl:9.762072 \tTrem:0.222222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  49 / 300\n",
      "Batch:\t0 /9\t:  1.049017 1.049032 \tl:9.892582 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.857470 1.907032 \tl:9.790075 \tem:0.300000\n",
      "Batch:\t2 /9\t:  1.071214 2.979271 \tl:9.763002 \tem:0.300000\n",
      "Batch:\t3 /9\t:  0.904033 3.884231 \tl:9.838125 \tem:0.150000\n",
      "Batch:\t4 /9\t:  1.088155 4.973732 \tl:10.012781 \tem:0.100000\n",
      "Batch:\t5 /9\t:  0.949859 5.924889 \tl:9.738026 \tem:0.300000\n",
      "Batch:\t6 /9\t:  0.858728 6.784408 \tl:9.641613 \tem:0.450000\n",
      "Batch:\t7 /9\t:  0.851531 7.637002 \tl:9.818542 \tem:0.050000\n",
      "Batch:\t8 /9\t:  1.201267 8.838924 \tl:9.749476 \tem:0.250000\n",
      "\n",
      "Epoch performance:  9.0501sec Trl:9.804914 \tTrem:0.227778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  50 / 300\n",
      "Batch:\t0 /9\t:  1.062520 1.062538 \tl:9.721834 \tem:0.250000\n",
      "Batch:\t1 /9\t:  1.022586 2.086521 \tl:9.766388 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.922140 3.009335 \tl:9.964209 \tem:0.050000\n",
      "Batch:\t3 /9\t:  0.991826 4.002505 \tl:9.752803 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.869652 4.873147 \tl:9.765937 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.883740 5.757820 \tl:10.207760 \tem:0.050000\n",
      "Batch:\t6 /9\t:  0.896934 6.656083 \tl:9.960002 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.977924 7.634577 \tl:10.196728 \tem:0.000000\n",
      "Batch:\t8 /9\t:  0.853590 8.488732 \tl:9.870212 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.6920sec Trl:9.911764 \tTrem:0.133333 \tTeem:0.005102\n",
      "\n",
      "Epoch:  51 / 300\n",
      "Batch:\t0 /9\t:  1.019351 1.019372 \tl:9.594400 \tem:0.300000\n",
      "Batch:\t1 /9\t:  1.022815 2.045481 \tl:9.850901 \tem:0.150000\n",
      "Batch:\t2 /9\t:  1.069972 3.116055 \tl:9.852135 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.897094 4.014106 \tl:9.869293 \tem:0.100000\n",
      "Batch:\t4 /9\t:  0.868637 4.884096 \tl:9.712849 \tem:0.300000\n",
      "Batch:\t5 /9\t:  0.943928 5.828819 \tl:9.866005 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.892241 6.721650 \tl:9.821253 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.896607 7.619482 \tl:9.763025 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.864132 8.484498 \tl:9.713389 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.6668sec Trl:9.782583 \tTrem:0.183333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  52 / 300\n",
      "Batch:\t0 /9\t:  1.099624 1.099642 \tl:9.966507 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.998534 2.099452 \tl:9.760120 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.866378 2.966289 \tl:9.860771 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.953228 3.920812 \tl:9.664049 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.982403 4.904296 \tl:9.860345 \tem:0.250000\n",
      "Batch:\t5 /9\t:  1.039719 5.944937 \tl:9.517546 \tem:0.300000\n",
      "Batch:\t6 /9\t:  1.061001 7.009366 \tl:9.824875 \tem:0.250000\n",
      "Batch:\t7 /9\t:  1.086116 8.096804 \tl:10.175538 \tem:0.050000\n",
      "Batch:\t8 /9\t:  0.866106 8.964215 \tl:9.452993 \tem:0.350000\n",
      "\n",
      "Epoch performance:  9.1656sec Trl:9.786972 \tTrem:0.216667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  53 / 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t0 /9\t:  0.849961 0.849978 \tl:9.819954 \tem:0.150000\n",
      "Batch:\t1 /9\t:  1.023099 1.874281 \tl:9.813505 \tem:0.150000\n",
      "Batch:\t2 /9\t:  1.062875 2.940212 \tl:9.833814 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.864665 3.806195 \tl:9.663590 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.854906 4.662353 \tl:9.663122 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.911199 5.573781 \tl:10.036688 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.930143 6.505167 \tl:9.949518 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.861891 7.368335 \tl:10.093718 \tem:0.100000\n",
      "Batch:\t8 /9\t:  0.899085 8.268275 \tl:9.772867 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.5027sec Trl:9.849642 \tTrem:0.166667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  54 / 300\n",
      "Batch:\t0 /9\t:  1.033774 1.033794 \tl:9.542910 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.894207 1.929109 \tl:9.863150 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.893630 2.824171 \tl:9.766682 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.915370 3.740462 \tl:9.776108 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.908362 4.649950 \tl:9.771021 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.876154 5.528662 \tl:9.921696 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.898427 6.428042 \tl:9.871765 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.855880 7.285103 \tl:9.726830 \tem:0.300000\n",
      "Batch:\t8 /9\t:  0.911149 8.196949 \tl:9.763041 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.4144sec Trl:9.778133 \tTrem:0.222222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  55 / 300\n",
      "Batch:\t0 /9\t:  0.969537 0.969552 \tl:9.782940 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.889545 1.860407 \tl:9.867248 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.915250 2.776263 \tl:9.765597 \tem:0.150000\n",
      "Batch:\t3 /9\t:  1.261492 4.039065 \tl:9.489552 \tem:0.350000\n",
      "Batch:\t4 /9\t:  1.017987 5.058451 \tl:9.608023 \tem:0.100000\n",
      "Batch:\t5 /9\t:  0.961318 6.020442 \tl:9.713549 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.965054 6.986450 \tl:9.862827 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.866347 7.853081 \tl:9.687434 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.840935 8.694422 \tl:9.770241 \tem:0.200000\n",
      "\n",
      "Epoch performance:  8.8995sec Trl:9.727490 \tTrem:0.205556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  56 / 300\n",
      "Batch:\t0 /9\t:  0.850492 0.850515 \tl:9.812984 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.997955 1.849516 \tl:9.712936 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.876920 2.727740 \tl:9.537743 \tem:0.350000\n",
      "Batch:\t3 /9\t:  0.862295 3.590775 \tl:9.962372 \tem:0.050000\n",
      "Batch:\t4 /9\t:  0.863885 4.455368 \tl:9.913057 \tem:0.100000\n",
      "Batch:\t5 /9\t:  1.047492 5.503144 \tl:9.911880 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.990189 6.494425 \tl:9.563101 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.900823 7.396070 \tl:9.462256 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.895865 8.292238 \tl:9.466017 \tem:0.350000\n",
      "\n",
      "Epoch performance:  8.4739sec Trl:9.704705 \tTrem:0.216667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  57 / 300\n",
      "Batch:\t0 /9\t:  0.971246 0.971264 \tl:9.613085 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.816388 1.788300 \tl:9.563341 \tem:0.300000\n",
      "Batch:\t2 /9\t:  0.937772 2.726940 \tl:10.160744 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.874614 3.602116 \tl:9.756654 \tem:0.150000\n",
      "Batch:\t4 /9\t:  1.014478 4.616800 \tl:9.817255 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.887509 5.505357 \tl:9.614897 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.867158 6.373701 \tl:9.658649 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.887872 7.261782 \tl:9.713257 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.916445 8.179178 \tl:9.909060 \tem:0.100000\n",
      "\n",
      "Epoch performance:  8.3808sec Trl:9.756327 \tTrem:0.200000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  58 / 300\n",
      "Batch:\t0 /9\t:  0.996872 0.996892 \tl:9.840184 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.863217 1.861476 \tl:9.911762 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.985898 2.848499 \tl:9.712977 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.856123 3.705957 \tl:9.712550 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.886631 4.593852 \tl:9.759335 \tem:0.200000\n",
      "Batch:\t5 /9\t:  1.078318 5.673105 \tl:10.003559 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.903429 6.577220 \tl:9.823202 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.819952 7.398252 \tl:10.011934 \tem:0.100000\n",
      "Batch:\t8 /9\t:  0.914997 8.314562 \tl:9.616203 \tem:0.200000\n",
      "\n",
      "Epoch performance:  8.5558sec Trl:9.821301 \tTrem:0.166667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  59 / 300\n",
      "Batch:\t0 /9\t:  1.040751 1.040771 \tl:9.713652 \tem:0.200000\n",
      "Batch:\t1 /9\t:  0.889314 1.930659 \tl:9.863243 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.875377 2.807169 \tl:9.962961 \tem:0.050000\n",
      "Batch:\t3 /9\t:  0.935111 3.743345 \tl:10.063183 \tem:0.050000\n",
      "Batch:\t4 /9\t:  0.875926 4.620177 \tl:9.713030 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.949805 5.570262 \tl:9.864584 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.825719 6.397163 \tl:9.612866 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.941347 7.339848 \tl:9.635878 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.880095 8.220746 \tl:9.565169 \tem:0.300000\n",
      "\n",
      "Epoch performance:  8.5022sec Trl:9.777174 \tTrem:0.172222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  60 / 300\n",
      "Batch:\t0 /9\t:  0.973950 0.973963 \tl:9.514441 \tem:0.250000\n",
      "Batch:\t1 /9\t:  1.063438 2.038704 \tl:9.663561 \tem:0.350000\n",
      "Batch:\t2 /9\t:  1.080955 3.120971 \tl:9.663305 \tem:0.350000\n",
      "Batch:\t3 /9\t:  0.886530 4.008740 \tl:9.912918 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.865557 4.876195 \tl:9.762728 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.855482 5.733036 \tl:9.663118 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.945523 6.679383 \tl:9.813398 \tem:0.050000\n",
      "Batch:\t7 /9\t:  1.074938 7.754624 \tl:9.563197 \tem:0.300000\n",
      "Batch:\t8 /9\t:  0.998949 8.754549 \tl:9.613277 \tem:0.300000\n",
      "\n",
      "Epoch performance:  8.9715sec Trl:9.685549 \tTrem:0.250000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  61 / 300\n",
      "Batch:\t0 /9\t:  1.009454 1.009475 \tl:9.912660 \tem:0.200000\n",
      "Batch:\t1 /9\t:  0.978226 1.987988 \tl:9.800864 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.850122 2.838418 \tl:9.317032 \tem:0.450000\n",
      "Batch:\t3 /9\t:  0.844089 3.683084 \tl:9.761370 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.858410 4.542711 \tl:9.613754 \tem:0.300000\n",
      "Batch:\t5 /9\t:  0.987410 5.531238 \tl:9.613741 \tem:0.300000\n",
      "Batch:\t6 /9\t:  0.824484 6.357123 \tl:9.762651 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.890204 7.247935 \tl:9.712605 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.863626 8.112865 \tl:9.463411 \tem:0.400000\n",
      "\n",
      "Epoch performance:  8.3523sec Trl:9.662010 \tTrem:0.272222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  62 / 300\n",
      "Batch:\t0 /9\t:  1.026583 1.026602 \tl:9.413622 \tem:0.350000\n",
      "Batch:\t1 /9\t:  0.865210 1.892904 \tl:9.819105 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.875031 2.769275 \tl:9.613173 \tem:0.300000\n",
      "Batch:\t3 /9\t:  0.970914 3.740812 \tl:9.613476 \tem:0.350000\n",
      "Batch:\t4 /9\t:  1.032958 4.774909 \tl:9.563139 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.869865 5.645653 \tl:9.663198 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.906921 6.552783 \tl:9.613348 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.906419 7.459996 \tl:9.809534 \tem:0.150000\n",
      "Batch:\t8 /9\t:  1.029203 8.489413 \tl:9.810425 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.6958sec Trl:9.657669 \tTrem:0.238889 \tTeem:0.000000\n",
      "\n",
      "Epoch:  63 / 300\n",
      "Batch:\t0 /9\t:  1.001216 1.001234 \tl:9.563456 \tem:0.300000\n",
      "Batch:\t1 /9\t:  1.031451 2.033995 \tl:9.663392 \tem:0.200000\n",
      "Batch:\t2 /9\t:  1.177575 3.213036 \tl:9.963142 \tem:0.200000\n",
      "Batch:\t3 /9\t:  1.016501 4.230664 \tl:9.813814 \tem:0.100000\n",
      "Batch:\t4 /9\t:  0.907499 5.141077 \tl:9.563602 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.926591 6.068933 \tl:9.714962 \tem:0.100000\n",
      "Batch:\t6 /9\t:  0.892776 6.962015 \tl:9.663302 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.830994 7.793879 \tl:9.713099 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.947464 8.742578 \tl:9.812998 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.9555sec Trl:9.719085 \tTrem:0.194444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  64 / 300\n",
      "Batch:\t0 /9\t:  1.071295 1.071319 \tl:9.513791 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.868497 1.940844 \tl:9.863632 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.838233 2.779777 \tl:9.912731 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.927404 3.708250 \tl:9.811303 \tem:0.150000\n",
      "Batch:\t4 /9\t:  1.075448 4.784801 \tl:9.613292 \tem:0.300000\n",
      "Batch:\t5 /9\t:  0.827898 5.613678 \tl:9.912292 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.916887 6.531305 \tl:9.763485 \tem:0.100000\n",
      "Batch:\t7 /9\t:  0.858865 7.391191 \tl:9.871170 \tem:0.050000\n",
      "Batch:\t8 /9\t:  0.981037 8.373531 \tl:9.562777 \tem:0.350000\n",
      "\n",
      "Epoch performance:  8.5776sec Trl:9.758275 \tTrem:0.194444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  65 / 300\n",
      "Batch:\t0 /9\t:  1.017884 1.017902 \tl:9.563367 \tem:0.300000\n",
      "Batch:\t1 /9\t:  1.119602 2.138797 \tl:9.863489 \tem:0.150000\n",
      "Batch:\t2 /9\t:  1.048411 3.187461 \tl:9.862768 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.863464 4.052083 \tl:10.012617 \tem:0.050000\n",
      "Batch:\t4 /9\t:  1.036307 5.089640 \tl:9.862627 \tem:0.250000\n",
      "Batch:\t5 /9\t:  1.094365 6.185575 \tl:9.862856 \tem:0.100000\n",
      "Batch:\t6 /9\t:  1.072918 7.258754 \tl:9.463320 \tem:0.350000\n",
      "Batch:\t7 /9\t:  0.873172 8.132597 \tl:9.863005 \tem:0.050000\n",
      "Batch:\t8 /9\t:  0.859752 8.993531 \tl:9.513711 \tem:0.350000\n",
      "\n",
      "Epoch performance:  9.2264sec Trl:9.763084 \tTrem:0.194444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  66 / 300\n",
      "Batch:\t0 /9\t:  0.966232 0.966253 \tl:9.862526 \tem:0.100000\n",
      "Batch:\t1 /9\t:  0.924534 1.891752 \tl:9.713366 \tem:0.050000\n",
      "Batch:\t2 /9\t:  0.837216 2.729430 \tl:9.413382 \tem:0.450000\n",
      "Batch:\t3 /9\t:  1.102962 3.835294 \tl:9.862667 \tem:0.250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t4 /9\t:  1.095503 4.931404 \tl:9.512967 \tem:0.350000\n",
      "Batch:\t5 /9\t:  0.860845 5.792514 \tl:9.813012 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.845727 6.639441 \tl:9.762764 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.990859 7.630886 \tl:9.813073 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.998027 8.629689 \tl:9.732685 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.8325sec Trl:9.720716 \tTrem:0.216667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  67 / 300\n",
      "Batch:\t0 /9\t:  0.971868 0.971884 \tl:9.763006 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.890161 1.863020 \tl:9.842093 \tem:0.150000\n",
      "Batch:\t2 /9\t:  1.123925 2.987725 \tl:10.011744 \tem:0.200000\n",
      "Batch:\t3 /9\t:  1.017915 4.007287 \tl:9.913306 \tem:0.100000\n",
      "Batch:\t4 /9\t:  0.862643 4.870138 \tl:9.713616 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.908694 5.779120 \tl:9.912737 \tem:0.100000\n",
      "Batch:\t6 /9\t:  0.980276 6.760149 \tl:9.813574 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.860141 7.620570 \tl:9.662906 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.898719 8.519731 \tl:9.913090 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.7162sec Trl:9.838452 \tTrem:0.166667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  68 / 300\n",
      "Batch:\t0 /9\t:  1.040163 1.040182 \tl:9.912851 \tem:0.200000\n",
      "Batch:\t1 /9\t:  0.897028 1.938491 \tl:9.613335 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.815907 2.755262 \tl:9.413828 \tem:0.400000\n",
      "Batch:\t3 /9\t:  0.941576 3.697965 \tl:9.613539 \tem:0.300000\n",
      "Batch:\t4 /9\t:  1.136161 4.836830 \tl:9.912521 \tem:0.200000\n",
      "Batch:\t5 /9\t:  1.035506 5.873233 \tl:9.513287 \tem:0.300000\n",
      "Batch:\t6 /9\t:  0.901370 6.775820 \tl:9.712841 \tem:0.300000\n",
      "Batch:\t7 /9\t:  1.023977 7.800420 \tl:9.962753 \tem:0.050000\n",
      "Batch:\t8 /9\t:  0.980248 8.781744 \tl:9.862961 \tem:0.100000\n",
      "\n",
      "Epoch performance:  8.9875sec Trl:9.724213 \tTrem:0.233333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  69 / 300\n",
      "Batch:\t0 /9\t:  0.964391 0.964421 \tl:9.613262 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.904094 1.871021 \tl:9.862974 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.933279 2.805542 \tl:10.009497 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.918576 3.725080 \tl:9.463406 \tem:0.350000\n",
      "Batch:\t4 /9\t:  0.874563 4.600682 \tl:9.766016 \tem:0.150000\n",
      "Batch:\t5 /9\t:  0.935729 5.537764 \tl:9.613052 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.969449 6.507459 \tl:9.779945 \tem:0.300000\n",
      "Batch:\t7 /9\t:  1.076653 7.584982 \tl:9.867410 \tem:0.100000\n",
      "Batch:\t8 /9\t:  0.902213 8.488242 \tl:9.669083 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.6930sec Trl:9.738294 \tTrem:0.211111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  70 / 300\n",
      "Batch:\t0 /9\t:  0.896470 0.896492 \tl:9.613341 \tem:0.350000\n",
      "Batch:\t1 /9\t:  0.986915 1.883977 \tl:9.513433 \tem:0.300000\n",
      "Batch:\t2 /9\t:  0.823704 2.708662 \tl:9.563783 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.920898 3.630022 \tl:9.763050 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.942125 4.572974 \tl:10.016132 \tem:0.000000\n",
      "Batch:\t5 /9\t:  1.129184 5.702451 \tl:9.562929 \tem:0.350000\n",
      "Batch:\t6 /9\t:  1.014364 6.718214 \tl:9.469398 \tem:0.300000\n",
      "Batch:\t7 /9\t:  1.050862 7.770758 \tl:9.664494 \tem:0.150000\n",
      "Batch:\t8 /9\t:  1.079726 8.851512 \tl:9.520584 \tem:0.350000\n",
      "\n",
      "Epoch performance:  9.0593sec Trl:9.631905 \tTrem:0.255556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  71 / 300\n",
      "Batch:\t0 /9\t:  0.960952 0.960966 \tl:10.062719 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.905549 1.867506 \tl:10.012396 \tem:0.150000\n",
      "Batch:\t2 /9\t:  1.068470 2.937180 \tl:9.763006 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.932741 3.870676 \tl:9.763083 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.870104 4.742213 \tl:9.713573 \tem:0.100000\n",
      "Batch:\t5 /9\t:  1.025544 5.768275 \tl:9.613415 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.861213 6.630013 \tl:9.962481 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.915472 7.546738 \tl:9.812540 \tem:0.200000\n",
      "Batch:\t8 /9\t:  1.034439 8.582073 \tl:9.811815 \tem:0.100000\n",
      "\n",
      "Epoch performance:  8.7889sec Trl:9.835003 \tTrem:0.166667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  72 / 300\n",
      "Batch:\t0 /9\t:  0.999433 0.999454 \tl:9.662714 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.986243 1.986340 \tl:9.615407 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.866294 2.853538 \tl:9.762827 \tem:0.200000\n",
      "Batch:\t3 /9\t:  1.037588 3.891947 \tl:10.012665 \tem:0.100000\n",
      "Batch:\t4 /9\t:  0.953072 4.845844 \tl:9.812999 \tem:0.150000\n",
      "Batch:\t5 /9\t:  0.931975 5.778082 \tl:9.731783 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.874386 6.652750 \tl:9.813116 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.858861 7.512948 \tl:9.912620 \tem:0.250000\n",
      "Batch:\t8 /9\t:  1.086832 8.601130 \tl:9.905746 \tem:0.100000\n",
      "\n",
      "Epoch performance:  8.8584sec Trl:9.803320 \tTrem:0.194444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  73 / 300\n",
      "Batch:\t0 /9\t:  1.026829 1.026853 \tl:9.963343 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.863500 1.892043 \tl:9.696852 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.915922 2.808522 \tl:9.613495 \tem:0.300000\n",
      "Batch:\t3 /9\t:  1.096023 3.905860 \tl:9.862370 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.868453 4.775456 \tl:10.061781 \tem:0.100000\n",
      "Batch:\t5 /9\t:  0.901499 5.678092 \tl:9.805109 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.861396 6.540763 \tl:9.814097 \tem:0.100000\n",
      "Batch:\t7 /9\t:  0.986587 7.528090 \tl:9.763849 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.865095 8.393572 \tl:9.524769 \tem:0.450000\n",
      "\n",
      "Epoch performance:  8.6067sec Trl:9.789518 \tTrem:0.222222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  74 / 300\n",
      "Batch:\t0 /9\t:  1.032824 1.032843 \tl:9.928756 \tem:0.100000\n",
      "Batch:\t1 /9\t:  1.048642 2.082710 \tl:9.713120 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.909752 2.993393 \tl:9.862842 \tem:0.150000\n",
      "Batch:\t3 /9\t:  1.020745 4.014703 \tl:9.419680 \tem:0.400000\n",
      "Batch:\t4 /9\t:  1.195197 5.211215 \tl:9.771467 \tem:0.200000\n",
      "Batch:\t5 /9\t:  1.112109 6.324245 \tl:10.034632 \tem:0.050000\n",
      "Batch:\t6 /9\t:  0.988234 7.314080 \tl:9.814035 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.938381 8.253146 \tl:9.716307 \tem:0.150000\n",
      "Batch:\t8 /9\t:  1.177007 9.431473 \tl:9.745455 \tem:0.200000\n",
      "\n",
      "Epoch performance:  9.6385sec Trl:9.778477 \tTrem:0.183333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  75 / 300\n",
      "Batch:\t0 /9\t:  0.992168 0.992182 \tl:9.812927 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.891284 1.884539 \tl:9.764997 \tem:0.050000\n",
      "Batch:\t2 /9\t:  0.966816 2.852041 \tl:9.565340 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.929105 3.782186 \tl:9.615255 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.818386 4.601405 \tl:9.913391 \tem:0.150000\n",
      "Batch:\t5 /9\t:  0.902036 5.506438 \tl:9.413319 \tem:0.450000\n",
      "Batch:\t6 /9\t:  1.057039 6.564378 \tl:10.013712 \tem:0.100000\n",
      "Batch:\t7 /9\t:  1.069473 7.635143 \tl:9.528135 \tem:0.300000\n",
      "Batch:\t8 /9\t:  0.887213 8.523750 \tl:9.663639 \tem:0.200000\n",
      "\n",
      "Epoch performance:  8.7370sec Trl:9.698968 \tTrem:0.211111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  76 / 300\n",
      "Batch:\t0 /9\t:  1.008414 1.008433 \tl:9.413551 \tem:0.350000\n",
      "Batch:\t1 /9\t:  0.983427 1.992619 \tl:9.771764 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.815300 2.809252 \tl:9.613331 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.829045 3.639428 \tl:9.813332 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.907582 4.548205 \tl:9.812995 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.984325 5.533548 \tl:9.467234 \tem:0.400000\n",
      "Batch:\t6 /9\t:  0.869803 6.404169 \tl:10.154676 \tem:0.100000\n",
      "Batch:\t7 /9\t:  0.907586 7.312760 \tl:9.663056 \tem:0.350000\n",
      "Batch:\t8 /9\t:  0.909107 8.223015 \tl:9.813087 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.4522sec Trl:9.724781 \tTrem:0.250000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  77 / 300\n",
      "Batch:\t0 /9\t:  1.022300 1.022319 \tl:9.614082 \tem:0.200000\n",
      "Batch:\t1 /9\t:  0.876641 1.900098 \tl:9.913131 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.931021 2.832432 \tl:9.713039 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.911237 3.744644 \tl:9.663650 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.891976 4.638012 \tl:9.812376 \tem:0.150000\n",
      "Batch:\t5 /9\t:  0.837166 5.475373 \tl:9.513639 \tem:0.350000\n",
      "Batch:\t6 /9\t:  0.930781 6.407474 \tl:9.763024 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.936177 7.344700 \tl:9.563501 \tem:0.300000\n",
      "Batch:\t8 /9\t:  1.071248 8.416234 \tl:9.713585 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.6195sec Trl:9.696670 \tTrem:0.211111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  78 / 300\n",
      "Batch:\t0 /9\t:  1.024645 1.024663 \tl:9.663153 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.905521 1.931414 \tl:9.713598 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.988346 2.921106 \tl:9.712799 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.873135 3.795484 \tl:9.712771 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.920201 4.716592 \tl:9.712510 \tem:0.250000\n",
      "Batch:\t5 /9\t:  1.145973 5.863343 \tl:9.513058 \tem:0.300000\n",
      "Batch:\t6 /9\t:  1.051077 6.914719 \tl:10.061953 \tem:0.100000\n",
      "Batch:\t7 /9\t:  0.918730 7.834547 \tl:10.013032 \tem:0.000000\n",
      "Batch:\t8 /9\t:  0.836069 8.671232 \tl:9.862194 \tem:0.200000\n",
      "\n",
      "Epoch performance:  8.8733sec Trl:9.773896 \tTrem:0.194444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  79 / 300\n",
      "Batch:\t0 /9\t:  1.069345 1.069365 \tl:9.884653 \tem:0.100000\n",
      "Batch:\t1 /9\t:  0.874875 1.945339 \tl:9.563890 \tem:0.350000\n",
      "Batch:\t2 /9\t:  0.989402 2.935079 \tl:9.618633 \tem:0.400000\n",
      "Batch:\t3 /9\t:  1.146615 4.082398 \tl:9.863029 \tem:0.050000\n",
      "Batch:\t4 /9\t:  1.053412 5.136590 \tl:9.763185 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.877596 6.015052 \tl:9.813019 \tem:0.200000\n",
      "Batch:\t6 /9\t:  1.057369 7.073045 \tl:9.812805 \tem:0.100000\n",
      "Batch:\t7 /9\t:  0.936671 8.010351 \tl:9.812852 \tem:0.150000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t8 /9\t:  0.898456 8.910141 \tl:9.864877 \tem:0.100000\n",
      "\n",
      "Epoch performance:  9.1185sec Trl:9.777438 \tTrem:0.183333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  80 / 300\n",
      "Batch:\t0 /9\t:  1.006324 1.006343 \tl:10.062418 \tem:0.100000\n",
      "Batch:\t1 /9\t:  1.051811 2.058564 \tl:9.762848 \tem:0.300000\n",
      "Batch:\t2 /9\t:  1.085601 3.144469 \tl:9.862690 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.892783 4.039111 \tl:9.663502 \tem:0.100000\n",
      "Batch:\t4 /9\t:  0.942768 4.982861 \tl:9.863153 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.886471 5.870352 \tl:9.811838 \tem:0.100000\n",
      "Batch:\t6 /9\t:  0.943176 6.814800 \tl:10.012620 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.828727 7.644549 \tl:9.762413 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.953428 8.599100 \tl:9.613988 \tem:0.200000\n",
      "\n",
      "Epoch performance:  8.7743sec Trl:9.823941 \tTrem:0.183333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  81 / 300\n",
      "Batch:\t0 /9\t:  0.815773 0.815798 \tl:9.826317 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.957185 1.774363 \tl:9.664312 \tem:0.150000\n",
      "Batch:\t2 /9\t:  1.037051 2.812227 \tl:9.863078 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.920560 3.734261 \tl:9.563349 \tem:0.300000\n",
      "Batch:\t4 /9\t:  0.935543 4.670938 \tl:9.713166 \tem:0.150000\n",
      "Batch:\t5 /9\t:  0.875058 5.547406 \tl:9.613163 \tem:0.300000\n",
      "Batch:\t6 /9\t:  0.906855 6.455510 \tl:9.812925 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.907673 7.364308 \tl:9.713635 \tem:0.200000\n",
      "Batch:\t8 /9\t:  1.127581 8.493093 \tl:9.613466 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.7016sec Trl:9.709268 \tTrem:0.200000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  82 / 300\n",
      "Batch:\t0 /9\t:  0.862580 0.862607 \tl:9.663147 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.864911 1.728734 \tl:9.519807 \tem:0.400000\n",
      "Batch:\t2 /9\t:  0.939425 2.668841 \tl:9.562979 \tem:0.400000\n",
      "Batch:\t3 /9\t:  0.934404 3.604214 \tl:9.763183 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.882592 4.487715 \tl:9.612820 \tem:0.300000\n",
      "Batch:\t5 /9\t:  0.871907 5.361192 \tl:9.663579 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.997572 6.359743 \tl:9.662873 \tem:0.300000\n",
      "Batch:\t7 /9\t:  1.056752 7.417913 \tl:9.763037 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.886329 8.305270 \tl:9.713384 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.5181sec Trl:9.658312 \tTrem:0.272222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  83 / 300\n",
      "Batch:\t0 /9\t:  1.009880 1.009899 \tl:9.613657 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.990489 2.001768 \tl:9.563354 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.871954 2.874534 \tl:9.413872 \tem:0.400000\n",
      "Batch:\t3 /9\t:  0.836405 3.712038 \tl:9.713436 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.874277 4.587344 \tl:9.712999 \tem:0.250000\n",
      "Batch:\t5 /9\t:  1.001555 5.590304 \tl:9.663498 \tem:0.300000\n",
      "Batch:\t6 /9\t:  0.773211 6.364537 \tl:9.663591 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.854511 7.219180 \tl:9.513464 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.918705 8.138466 \tl:9.762914 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.3805sec Trl:9.624532 \tTrem:0.266667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  84 / 300\n",
      "Batch:\t0 /9\t:  1.059867 1.059885 \tl:9.713276 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.886454 1.947661 \tl:9.763475 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.838819 2.787730 \tl:9.663311 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.951902 3.741045 \tl:9.763094 \tem:0.100000\n",
      "Batch:\t4 /9\t:  0.930626 4.672628 \tl:9.813168 \tem:0.150000\n",
      "Batch:\t5 /9\t:  0.825994 5.499938 \tl:9.563454 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.832971 6.333483 \tl:9.463739 \tem:0.350000\n",
      "Batch:\t7 /9\t:  1.125231 7.459833 \tl:9.613625 \tem:0.200000\n",
      "Batch:\t8 /9\t:  1.068556 8.529315 \tl:9.563367 \tem:0.300000\n",
      "\n",
      "Epoch performance:  8.7368sec Trl:9.657834 \tTrem:0.211111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  85 / 300\n",
      "Batch:\t0 /9\t:  1.008486 1.008506 \tl:10.012857 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.863252 1.872348 \tl:9.613663 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.992891 2.865879 \tl:9.752623 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.817926 3.684524 \tl:9.464272 \tem:0.450000\n",
      "Batch:\t4 /9\t:  0.860079 4.544938 \tl:9.812549 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.862133 5.408406 \tl:9.562916 \tem:0.400000\n",
      "Batch:\t6 /9\t:  0.986539 6.395992 \tl:9.663161 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.864866 7.261435 \tl:9.762863 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.832155 8.094058 \tl:9.613102 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.2980sec Trl:9.695334 \tTrem:0.255556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  86 / 300\n",
      "Batch:\t0 /9\t:  0.890520 0.890542 \tl:9.913674 \tem:0.100000\n",
      "Batch:\t1 /9\t:  0.957701 1.849102 \tl:9.613275 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.875013 2.724781 \tl:9.614254 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.876829 3.603280 \tl:9.563201 \tem:0.300000\n",
      "Batch:\t4 /9\t:  0.954099 4.558264 \tl:9.663107 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.903862 5.462424 \tl:9.463518 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.868438 6.331982 \tl:9.763128 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.904865 7.237828 \tl:9.513563 \tem:0.300000\n",
      "Batch:\t8 /9\t:  0.964735 8.203165 \tl:9.562681 \tem:0.400000\n",
      "\n",
      "Epoch performance:  8.4121sec Trl:9.630045 \tTrem:0.250000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  87 / 300\n",
      "Batch:\t0 /9\t:  1.064779 1.064796 \tl:9.613361 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.912776 1.978572 \tl:9.413431 \tem:0.400000\n",
      "Batch:\t2 /9\t:  1.019972 3.000030 \tl:9.912146 \tem:0.150000\n",
      "Batch:\t3 /9\t:  1.073683 4.074026 \tl:9.613695 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.859522 4.934594 \tl:9.712984 \tem:0.150000\n",
      "Batch:\t5 /9\t:  1.015836 5.950688 \tl:9.563257 \tem:0.350000\n",
      "Batch:\t6 /9\t:  1.096627 7.048020 \tl:9.613100 \tem:0.350000\n",
      "Batch:\t7 /9\t:  0.940914 7.989748 \tl:9.713079 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.871029 8.861567 \tl:9.812760 \tem:0.200000\n",
      "\n",
      "Epoch performance:  9.0392sec Trl:9.663090 \tTrem:0.244444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  88 / 300\n",
      "Batch:\t0 /9\t:  0.997914 0.997931 \tl:9.613680 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.898183 1.896723 \tl:9.862407 \tem:0.200000\n",
      "Batch:\t2 /9\t:  1.070669 2.968131 \tl:9.713449 \tem:0.150000\n",
      "Batch:\t3 /9\t:  1.023769 3.992898 \tl:9.662868 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.866552 4.860186 \tl:9.712785 \tem:0.150000\n",
      "Batch:\t5 /9\t:  1.170503 6.031392 \tl:9.912802 \tem:0.100000\n",
      "Batch:\t6 /9\t:  1.033438 7.066126 \tl:9.613319 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.939874 8.007023 \tl:9.663282 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.893887 8.901994 \tl:9.512938 \tem:0.350000\n",
      "\n",
      "Epoch performance:  9.1887sec Trl:9.696392 \tTrem:0.205556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  89 / 300\n",
      "Batch:\t0 /9\t:  0.978080 0.978094 \tl:9.912974 \tem:0.050000\n",
      "Batch:\t1 /9\t:  0.924706 1.903987 \tl:9.712945 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.866406 2.771265 \tl:9.563059 \tem:0.300000\n",
      "Batch:\t3 /9\t:  0.995862 3.768510 \tl:9.912356 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.871397 4.640905 \tl:9.513552 \tem:0.300000\n",
      "Batch:\t5 /9\t:  0.915287 5.556821 \tl:9.663036 \tem:0.300000\n",
      "Batch:\t6 /9\t:  0.928671 6.486665 \tl:9.707950 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.973073 7.460581 \tl:9.513245 \tem:0.300000\n",
      "Batch:\t8 /9\t:  0.872447 8.334304 \tl:9.563575 \tem:0.300000\n",
      "\n",
      "Epoch performance:  8.5462sec Trl:9.673632 \tTrem:0.238889 \tTeem:0.000000\n",
      "\n",
      "Epoch:  90 / 300\n",
      "Batch:\t0 /9\t:  1.122902 1.122924 \tl:9.812819 \tem:0.200000\n",
      "Batch:\t1 /9\t:  1.121909 2.246289 \tl:9.563354 \tem:0.350000\n",
      "Batch:\t2 /9\t:  1.087549 3.335123 \tl:9.912182 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.995285 4.330848 \tl:9.613582 \tem:0.300000\n",
      "Batch:\t4 /9\t:  1.012786 5.344160 \tl:9.862928 \tem:0.150000\n",
      "Batch:\t5 /9\t:  0.941315 6.285757 \tl:9.613564 \tem:0.300000\n",
      "Batch:\t6 /9\t:  0.839762 7.125986 \tl:9.563164 \tem:0.300000\n",
      "Batch:\t7 /9\t:  0.939636 8.066669 \tl:9.964527 \tem:0.100000\n",
      "Batch:\t8 /9\t:  0.944727 9.011703 \tl:9.614262 \tem:0.350000\n",
      "\n",
      "Epoch performance:  9.2450sec Trl:9.724487 \tTrem:0.250000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  91 / 300\n",
      "Batch:\t0 /9\t:  1.048290 1.048307 \tl:9.263617 \tem:0.450000\n",
      "Batch:\t1 /9\t:  0.907160 1.955981 \tl:9.715015 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.933617 2.890156 \tl:9.674839 \tem:0.200000\n",
      "Batch:\t3 /9\t:  1.155817 4.046288 \tl:9.463333 \tem:0.350000\n",
      "Batch:\t4 /9\t:  1.009180 5.056285 \tl:9.562094 \tem:0.350000\n",
      "Batch:\t5 /9\t:  1.070419 6.130370 \tl:9.612413 \tem:0.250000\n",
      "Batch:\t6 /9\t:  1.077359 7.208299 \tl:9.812231 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.901126 8.112138 \tl:9.912562 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.911987 9.024794 \tl:9.613388 \tem:0.250000\n",
      "\n",
      "Epoch performance:  9.2269sec Trl:9.625499 \tTrem:0.261111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  92 / 300\n",
      "Batch:\t0 /9\t:  0.940343 0.940368 \tl:9.513056 \tem:0.350000\n",
      "Batch:\t1 /9\t:  1.115231 2.056850 \tl:9.862989 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.875160 2.933127 \tl:9.712704 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.916163 3.850544 \tl:9.563164 \tem:0.250000\n",
      "Batch:\t4 /9\t:  1.037806 4.889542 \tl:9.912994 \tem:0.100000\n",
      "Batch:\t5 /9\t:  0.928323 5.818784 \tl:9.812975 \tem:0.100000\n",
      "Batch:\t6 /9\t:  0.867081 6.686655 \tl:9.763021 \tem:0.200000\n",
      "Batch:\t7 /9\t:  1.063719 7.750804 \tl:9.662815 \tem:0.200000\n",
      "Batch:\t8 /9\t:  1.093070 8.845141 \tl:9.463327 \tem:0.300000\n",
      "\n",
      "Epoch performance:  9.0610sec Trl:9.696338 \tTrem:0.205556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  93 / 300\n",
      "Batch:\t0 /9\t:  0.996536 0.996558 \tl:9.813474 \tem:0.150000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t1 /9\t:  0.917412 1.915112 \tl:9.613243 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.945040 2.861433 \tl:9.513802 \tem:0.350000\n",
      "Batch:\t3 /9\t:  1.075495 3.938235 \tl:9.763649 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.828039 4.767361 \tl:9.662563 \tem:0.350000\n",
      "Batch:\t5 /9\t:  0.855439 5.623368 \tl:9.613750 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.962767 6.587131 \tl:9.913202 \tem:0.100000\n",
      "Batch:\t7 /9\t:  0.944153 7.532160 \tl:9.863078 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.822201 8.355576 \tl:9.413723 \tem:0.350000\n",
      "\n",
      "Epoch performance:  8.5612sec Trl:9.685609 \tTrem:0.233333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  94 / 300\n",
      "Batch:\t0 /9\t:  1.046506 1.046526 \tl:9.716511 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.962167 2.009742 \tl:9.862833 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.894515 2.904850 \tl:9.962805 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.918184 3.824048 \tl:9.513551 \tem:0.300000\n",
      "Batch:\t4 /9\t:  1.143971 4.968771 \tl:9.862774 \tem:0.150000\n",
      "Batch:\t5 /9\t:  1.184428 6.154007 \tl:9.713318 \tem:0.200000\n",
      "Batch:\t6 /9\t:  1.018018 7.173410 \tl:9.313643 \tem:0.400000\n",
      "Batch:\t7 /9\t:  0.912127 8.086539 \tl:9.713730 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.934692 9.022439 \tl:9.563351 \tem:0.250000\n",
      "\n",
      "Epoch performance:  9.3055sec Trl:9.691390 \tTrem:0.233333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  95 / 300\n",
      "Batch:\t0 /9\t:  1.003607 1.003623 \tl:9.812792 \tem:0.200000\n",
      "Batch:\t1 /9\t:  0.920030 1.924926 \tl:9.813015 \tem:0.200000\n",
      "Batch:\t2 /9\t:  1.064724 2.990348 \tl:9.513546 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.912734 3.904249 \tl:9.813086 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.864718 4.770327 \tl:9.912879 \tem:0.150000\n",
      "Batch:\t5 /9\t:  1.097451 5.868319 \tl:9.513389 \tem:0.300000\n",
      "Batch:\t6 /9\t:  1.127330 6.996324 \tl:9.713039 \tem:0.200000\n",
      "Batch:\t7 /9\t:  1.049540 8.049252 \tl:9.663666 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.919579 8.970192 \tl:9.463334 \tem:0.400000\n",
      "\n",
      "Epoch performance:  9.1778sec Trl:9.690972 \tTrem:0.227778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  96 / 300\n",
      "Batch:\t0 /9\t:  1.062966 1.062993 \tl:9.663279 \tem:0.200000\n",
      "Batch:\t1 /9\t:  0.948576 2.013062 \tl:9.663210 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.904406 2.918500 \tl:9.763515 \tem:0.300000\n",
      "Batch:\t3 /9\t:  1.064470 3.984863 \tl:9.713306 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.927379 4.913002 \tl:9.813593 \tem:0.100000\n",
      "Batch:\t5 /9\t:  1.090886 6.004179 \tl:9.263678 \tem:0.450000\n",
      "Batch:\t6 /9\t:  0.914337 6.918942 \tl:9.863298 \tem:0.100000\n",
      "Batch:\t7 /9\t:  1.016075 7.936176 \tl:9.563469 \tem:0.250000\n",
      "Batch:\t8 /9\t:  1.106889 9.043349 \tl:9.550257 \tem:0.250000\n",
      "\n",
      "Epoch performance:  9.2469sec Trl:9.650845 \tTrem:0.227778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  97 / 300\n",
      "Batch:\t0 /9\t:  1.013036 1.013057 \tl:9.613369 \tem:0.150000\n",
      "Batch:\t1 /9\t:  1.062526 2.076236 \tl:9.563653 \tem:0.250000\n",
      "Batch:\t2 /9\t:  1.086982 3.164025 \tl:9.563749 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.878515 4.043069 \tl:9.513222 \tem:0.300000\n",
      "Batch:\t4 /9\t:  0.974554 5.018637 \tl:9.513239 \tem:0.300000\n",
      "Batch:\t5 /9\t:  1.075024 6.093969 \tl:9.763348 \tem:0.250000\n",
      "Batch:\t6 /9\t:  1.129019 7.223678 \tl:9.862928 \tem:0.250000\n",
      "Batch:\t7 /9\t:  1.045428 8.270123 \tl:9.813159 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.878916 9.150300 \tl:9.362940 \tem:0.450000\n",
      "\n",
      "Epoch performance:  9.3554sec Trl:9.618845 \tTrem:0.261111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  98 / 300\n",
      "Batch:\t0 /9\t:  1.089962 1.089983 \tl:9.563563 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.876236 1.966524 \tl:9.413438 \tem:0.350000\n",
      "Batch:\t2 /9\t:  0.922797 2.891156 \tl:9.613442 \tem:0.250000\n",
      "Batch:\t3 /9\t:  1.098831 3.991092 \tl:9.712919 \tem:0.250000\n",
      "Batch:\t4 /9\t:  1.016659 5.008770 \tl:9.863245 \tem:0.050000\n",
      "Batch:\t5 /9\t:  0.886751 5.896037 \tl:9.663606 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.900040 6.796622 \tl:9.663714 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.822939 7.620119 \tl:9.813036 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.956660 8.577600 \tl:9.713604 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.7793sec Trl:9.668952 \tTrem:0.211111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  99 / 300\n",
      "Batch:\t0 /9\t:  0.847275 0.847295 \tl:9.662994 \tem:0.200000\n",
      "Batch:\t1 /9\t:  0.892130 1.740480 \tl:9.563108 \tem:0.350000\n",
      "Batch:\t2 /9\t:  1.143887 2.885048 \tl:9.912714 \tem:0.050000\n",
      "Batch:\t3 /9\t:  1.007463 3.893096 \tl:9.713694 \tem:0.300000\n",
      "Batch:\t4 /9\t:  0.909202 4.803437 \tl:9.812675 \tem:0.200000\n",
      "Batch:\t5 /9\t:  1.102622 5.906384 \tl:9.613121 \tem:0.300000\n",
      "Batch:\t6 /9\t:  1.091140 6.998373 \tl:9.912788 \tem:0.150000\n",
      "Batch:\t7 /9\t:  1.037213 8.035872 \tl:9.613173 \tem:0.350000\n",
      "Batch:\t8 /9\t:  0.964143 9.001102 \tl:9.563282 \tem:0.300000\n",
      "\n",
      "Epoch performance:  9.2131sec Trl:9.707506 \tTrem:0.244444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  100 / 300\n",
      "Batch:\t0 /9\t:  1.080528 1.080548 \tl:9.763359 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.874507 1.955968 \tl:9.912868 \tem:0.150000\n",
      "Batch:\t2 /9\t:  1.051154 3.008512 \tl:9.513345 \tem:0.400000\n",
      "Batch:\t3 /9\t:  0.898263 3.907405 \tl:9.762924 \tem:0.250000\n",
      "Batch:\t4 /9\t:  1.111005 5.019693 \tl:9.612943 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.908217 5.928218 \tl:9.763067 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.877805 6.807198 \tl:9.613626 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.944307 7.752312 \tl:9.763359 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.913127 8.666024 \tl:9.912903 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.8683sec Trl:9.735377 \tTrem:0.205556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  101 / 300\n",
      "Batch:\t0 /9\t:  1.026612 1.026631 \tl:9.413481 \tem:0.400000\n",
      "Batch:\t1 /9\t:  1.042118 2.069058 \tl:9.763512 \tem:0.150000\n",
      "Batch:\t2 /9\t:  1.067866 3.138620 \tl:9.613364 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.829001 3.968696 \tl:9.513556 \tem:0.300000\n",
      "Batch:\t4 /9\t:  0.927175 4.897145 \tl:9.713266 \tem:0.200000\n",
      "Batch:\t5 /9\t:  1.008022 5.906439 \tl:9.863192 \tem:0.050000\n",
      "Batch:\t6 /9\t:  1.090640 6.998177 \tl:9.911873 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.978132 7.976805 \tl:9.812702 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.891018 8.868395 \tl:9.513453 \tem:0.350000\n",
      "\n",
      "Epoch performance:  9.0763sec Trl:9.679822 \tTrem:0.233333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  102 / 300\n",
      "Batch:\t0 /9\t:  1.048895 1.048914 \tl:9.663025 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.816817 1.866396 \tl:9.962927 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.904372 2.771018 \tl:10.012518 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.906009 3.677859 \tl:9.712973 \tem:0.200000\n",
      "Batch:\t4 /9\t:  1.152617 4.831007 \tl:9.713377 \tem:0.250000\n",
      "Batch:\t5 /9\t:  1.011151 5.843388 \tl:9.763069 \tem:0.200000\n",
      "Batch:\t6 /9\t:  1.058295 6.902698 \tl:9.713024 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.966246 7.870239 \tl:9.862937 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.880335 8.751374 \tl:9.513481 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.9539sec Trl:9.768592 \tTrem:0.200000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  103 / 300\n",
      "Batch:\t0 /9\t:  1.005760 1.005779 \tl:9.412977 \tem:0.400000\n",
      "Batch:\t1 /9\t:  1.072388 2.078768 \tl:9.462746 \tem:0.400000\n",
      "Batch:\t2 /9\t:  0.893501 2.973392 \tl:9.613269 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.856045 3.830601 \tl:9.463200 \tem:0.350000\n",
      "Batch:\t4 /9\t:  0.913416 4.745113 \tl:9.513228 \tem:0.300000\n",
      "Batch:\t5 /9\t:  0.966132 5.712332 \tl:9.962543 \tem:0.100000\n",
      "Batch:\t6 /9\t:  0.991436 6.705065 \tl:9.313498 \tem:0.450000\n",
      "Batch:\t7 /9\t:  0.999657 7.706160 \tl:9.763022 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.936763 8.643567 \tl:9.762905 \tem:0.200000\n",
      "\n",
      "Epoch performance:  8.8453sec Trl:9.585265 \tTrem:0.288889 \tTeem:0.000000\n",
      "\n",
      "Epoch:  104 / 300\n",
      "Batch:\t0 /9\t:  1.082235 1.082252 \tl:9.663479 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.871943 1.955237 \tl:9.563530 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.900285 2.856525 \tl:9.613292 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.911300 3.768740 \tl:9.662945 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.985961 4.755984 \tl:9.862853 \tem:0.150000\n",
      "Batch:\t5 /9\t:  0.873253 5.629519 \tl:9.712994 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.844361 6.474337 \tl:9.912958 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.922311 7.397785 \tl:9.813431 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.965496 8.363917 \tl:9.713320 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.5722sec Trl:9.724311 \tTrem:0.216667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  105 / 300\n",
      "Batch:\t0 /9\t:  0.980539 0.980558 \tl:9.463351 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.941709 1.922820 \tl:9.613111 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.922460 2.845881 \tl:9.663190 \tem:0.250000\n",
      "Batch:\t3 /9\t:  1.083612 3.930003 \tl:9.513329 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.913582 4.844377 \tl:9.663446 \tem:0.150000\n",
      "Batch:\t5 /9\t:  1.080509 5.926128 \tl:9.513081 \tem:0.450000\n",
      "Batch:\t6 /9\t:  1.113615 7.040343 \tl:9.613146 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.871703 7.912884 \tl:9.613482 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.914138 8.828118 \tl:9.662590 \tem:0.250000\n",
      "\n",
      "Epoch performance:  9.0669sec Trl:9.590970 \tTrem:0.261111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  106 / 300\n",
      "Batch:\t0 /9\t:  1.035634 1.035657 \tl:9.763486 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.907264 1.943835 \tl:9.563374 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.910191 2.855221 \tl:9.663558 \tem:0.200000\n",
      "Batch:\t3 /9\t:  1.056191 3.912551 \tl:9.912565 \tem:0.050000\n",
      "Batch:\t4 /9\t:  1.100604 5.014225 \tl:9.463464 \tem:0.350000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t5 /9\t:  0.874773 5.890147 \tl:9.862997 \tem:0.100000\n",
      "Batch:\t6 /9\t:  0.913326 6.804672 \tl:9.762305 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.980067 7.785429 \tl:9.763287 \tem:0.100000\n",
      "Batch:\t8 /9\t:  1.135389 8.921422 \tl:9.763074 \tem:0.250000\n",
      "\n",
      "Epoch performance:  9.1292sec Trl:9.724234 \tTrem:0.183333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  107 / 300\n",
      "Batch:\t0 /9\t:  1.004570 1.004589 \tl:9.762892 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.927675 1.933098 \tl:9.762607 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.985599 2.919764 \tl:9.712886 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.870497 3.791372 \tl:9.562945 \tem:0.300000\n",
      "Batch:\t4 /9\t:  0.838988 4.630970 \tl:9.812641 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.964239 5.595808 \tl:9.813173 \tem:0.100000\n",
      "Batch:\t6 /9\t:  1.176316 6.773092 \tl:9.413108 \tem:0.350000\n",
      "Batch:\t7 /9\t:  1.018527 7.793040 \tl:9.912504 \tem:0.050000\n",
      "Batch:\t8 /9\t:  0.939591 8.733213 \tl:9.613043 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.9136sec Trl:9.707311 \tTrem:0.211111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  108 / 300\n",
      "Batch:\t0 /9\t:  0.816971 0.816991 \tl:9.812981 \tem:0.100000\n",
      "Batch:\t1 /9\t:  0.941557 1.759531 \tl:9.813115 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.867902 2.628478 \tl:9.812586 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.923313 3.552464 \tl:9.413422 \tem:0.350000\n",
      "Batch:\t4 /9\t:  1.237473 4.790937 \tl:9.812925 \tem:0.150000\n",
      "Batch:\t5 /9\t:  1.007601 5.800150 \tl:9.463537 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.914896 6.716278 \tl:9.713105 \tem:0.200000\n",
      "Batch:\t7 /9\t:  1.082344 7.799453 \tl:9.712709 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.898200 8.698693 \tl:9.612999 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.9016sec Trl:9.685264 \tTrem:0.200000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  109 / 300\n",
      "Batch:\t0 /9\t:  0.999389 0.999409 \tl:9.612804 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.893270 1.893809 \tl:9.812483 \tem:0.200000\n",
      "Batch:\t2 /9\t:  1.088057 2.982737 \tl:9.811576 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.972651 3.956268 \tl:9.663295 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.820722 4.777425 \tl:9.613467 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.913911 5.692521 \tl:10.062288 \tem:0.000000\n",
      "Batch:\t6 /9\t:  1.001958 6.695244 \tl:9.712713 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.862603 7.559617 \tl:9.863185 \tem:0.050000\n",
      "Batch:\t8 /9\t:  0.858546 8.418748 \tl:9.413071 \tem:0.400000\n",
      "\n",
      "Epoch performance:  8.6156sec Trl:9.729431 \tTrem:0.200000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  110 / 300\n",
      "Batch:\t0 /9\t:  1.037118 1.037137 \tl:9.912340 \tem:0.100000\n",
      "Batch:\t1 /9\t:  0.915593 1.953181 \tl:9.763003 \tem:0.300000\n",
      "Batch:\t2 /9\t:  0.851166 2.804621 \tl:9.712862 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.839428 3.644728 \tl:9.613415 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.970904 4.616631 \tl:9.512383 \tem:0.350000\n",
      "Batch:\t5 /9\t:  1.062973 5.680855 \tl:10.012870 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.877804 6.559642 \tl:9.312609 \tem:0.450000\n",
      "Batch:\t7 /9\t:  0.950496 7.510952 \tl:9.813507 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.925036 8.436999 \tl:9.463066 \tem:0.350000\n",
      "\n",
      "Epoch performance:  8.7162sec Trl:9.679562 \tTrem:0.255556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  111 / 300\n",
      "Batch:\t0 /9\t:  0.973388 0.973400 \tl:9.363613 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.913563 1.888180 \tl:9.713680 \tem:0.100000\n",
      "Batch:\t2 /9\t:  1.053471 2.942750 \tl:9.912457 \tem:0.200000\n",
      "Batch:\t3 /9\t:  1.073091 4.019023 \tl:9.513083 \tem:0.300000\n",
      "Batch:\t4 /9\t:  0.881137 4.901556 \tl:9.513073 \tem:0.350000\n",
      "Batch:\t5 /9\t:  0.925446 5.828187 \tl:9.812920 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.931308 6.760767 \tl:9.962134 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.942128 7.704336 \tl:9.613125 \tem:0.300000\n",
      "Batch:\t8 /9\t:  0.885876 8.591064 \tl:10.011982 \tem:0.100000\n",
      "\n",
      "Epoch performance:  8.7940sec Trl:9.712896 \tTrem:0.227778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  112 / 300\n",
      "Batch:\t0 /9\t:  0.818068 0.818087 \tl:9.763165 \tem:0.150000\n",
      "Batch:\t1 /9\t:  1.065765 1.885161 \tl:9.562492 \tem:0.300000\n",
      "Batch:\t2 /9\t:  1.019921 2.908579 \tl:9.662718 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.914966 3.824578 \tl:9.513138 \tem:0.300000\n",
      "Batch:\t4 /9\t:  0.914354 4.740092 \tl:9.712763 \tem:0.300000\n",
      "Batch:\t5 /9\t:  0.990318 5.731031 \tl:9.712858 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.861692 6.593350 \tl:9.712681 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.868658 7.463360 \tl:9.413026 \tem:0.400000\n",
      "Batch:\t8 /9\t:  1.072737 8.537768 \tl:9.763041 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.7494sec Trl:9.646209 \tTrem:0.261111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  113 / 300\n",
      "Batch:\t0 /9\t:  1.056171 1.056191 \tl:9.912058 \tem:0.200000\n",
      "Batch:\t1 /9\t:  0.883082 1.940244 \tl:9.702952 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.940714 2.881979 \tl:9.853828 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.946569 3.829860 \tl:9.862600 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.895636 4.728111 \tl:9.743406 \tem:0.300000\n",
      "Batch:\t5 /9\t:  0.908527 5.637808 \tl:9.563164 \tem:0.350000\n",
      "Batch:\t6 /9\t:  0.895374 6.534147 \tl:9.413672 \tem:0.350000\n",
      "Batch:\t7 /9\t:  0.944997 7.480246 \tl:9.463112 \tem:0.350000\n",
      "Batch:\t8 /9\t:  0.868566 8.350147 \tl:9.663376 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.5263sec Trl:9.686463 \tTrem:0.261111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  114 / 300\n",
      "Batch:\t0 /9\t:  1.144250 1.144268 \tl:9.463375 \tem:0.300000\n",
      "Batch:\t1 /9\t:  1.145818 2.290705 \tl:9.513512 \tem:0.300000\n",
      "Batch:\t2 /9\t:  1.110269 3.401608 \tl:9.813089 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.905425 4.308861 \tl:9.812746 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.989495 5.298989 \tl:9.763062 \tem:0.250000\n",
      "Batch:\t5 /9\t:  1.068608 6.368520 \tl:9.563448 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.871031 7.239813 \tl:9.513330 \tem:0.300000\n",
      "Batch:\t7 /9\t:  0.828562 8.068598 \tl:9.513517 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.919171 8.988631 \tl:9.763247 \tem:0.100000\n",
      "\n",
      "Epoch performance:  9.2271sec Trl:9.635481 \tTrem:0.233333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  115 / 300\n",
      "Batch:\t0 /9\t:  1.038916 1.038935 \tl:9.863070 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.883625 1.923590 \tl:10.013422 \tem:0.050000\n",
      "Batch:\t2 /9\t:  1.205591 3.129884 \tl:9.413589 \tem:0.400000\n",
      "Batch:\t3 /9\t:  1.151156 4.281922 \tl:9.813455 \tem:0.150000\n",
      "Batch:\t4 /9\t:  1.003031 5.286022 \tl:9.463653 \tem:0.350000\n",
      "Batch:\t5 /9\t:  0.901176 6.188095 \tl:9.563256 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.944526 7.133965 \tl:9.664111 \tem:0.200000\n",
      "Batch:\t7 /9\t:  1.063652 8.198206 \tl:9.663363 \tem:0.150000\n",
      "Batch:\t8 /9\t:  1.010248 9.209913 \tl:9.565505 \tem:0.250000\n",
      "\n",
      "Epoch performance:  9.4053sec Trl:9.669269 \tTrem:0.205556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  116 / 300\n",
      "Batch:\t0 /9\t:  0.824388 0.824408 \tl:9.912579 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.944840 1.769577 \tl:9.812986 \tem:0.200000\n",
      "Batch:\t2 /9\t:  1.051327 2.821228 \tl:10.012266 \tem:0.100000\n",
      "Batch:\t3 /9\t:  1.062060 3.884444 \tl:9.613434 \tem:0.300000\n",
      "Batch:\t4 /9\t:  0.826382 4.711358 \tl:9.514866 \tem:0.300000\n",
      "Batch:\t5 /9\t:  1.011986 5.724818 \tl:9.463204 \tem:0.350000\n",
      "Batch:\t6 /9\t:  0.855617 6.581471 \tl:9.463453 \tem:0.300000\n",
      "Batch:\t7 /9\t:  0.863016 7.445583 \tl:9.862869 \tem:0.100000\n",
      "Batch:\t8 /9\t:  0.869102 8.315382 \tl:9.313400 \tem:0.500000\n",
      "\n",
      "Epoch performance:  8.5522sec Trl:9.663229 \tTrem:0.255556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  117 / 300\n",
      "Batch:\t0 /9\t:  0.926828 0.926844 \tl:9.463247 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.790807 1.718109 \tl:9.612762 \tem:0.300000\n",
      "Batch:\t2 /9\t:  0.907747 2.627140 \tl:10.062504 \tem:0.050000\n",
      "Batch:\t3 /9\t:  0.947516 3.575690 \tl:9.812568 \tem:0.200000\n",
      "Batch:\t4 /9\t:  1.105116 4.682124 \tl:9.563072 \tem:0.400000\n",
      "Batch:\t5 /9\t:  0.948379 5.631723 \tl:9.363647 \tem:0.400000\n",
      "Batch:\t6 /9\t:  1.000232 6.632151 \tl:9.712808 \tem:0.250000\n",
      "Batch:\t7 /9\t:  1.393219 8.026793 \tl:9.762869 \tem:0.200000\n",
      "Batch:\t8 /9\t:  1.005922 9.034170 \tl:9.612447 \tem:0.300000\n",
      "\n",
      "Epoch performance:  9.2465sec Trl:9.662880 \tTrem:0.266667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  118 / 300\n",
      "Batch:\t0 /9\t:  1.159815 1.159836 \tl:9.662989 \tem:0.250000\n",
      "Batch:\t1 /9\t:  1.302952 2.463607 \tl:9.663262 \tem:0.200000\n",
      "Batch:\t2 /9\t:  1.071792 3.536794 \tl:9.813398 \tem:0.050000\n",
      "Batch:\t3 /9\t:  1.062449 4.600010 \tl:9.562955 \tem:0.300000\n",
      "Batch:\t4 /9\t:  1.167783 5.768905 \tl:9.612919 \tem:0.300000\n",
      "Batch:\t5 /9\t:  0.997380 6.767055 \tl:9.712885 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.885508 7.653856 \tl:9.563241 \tem:0.350000\n",
      "Batch:\t7 /9\t:  0.864359 8.519155 \tl:9.563166 \tem:0.200000\n",
      "Batch:\t8 /9\t:  1.051326 9.571781 \tl:9.863016 \tem:0.050000\n",
      "\n",
      "Epoch performance:  9.7308sec Trl:9.668648 \tTrem:0.216667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  119 / 300\n",
      "Batch:\t0 /9\t:  0.746722 0.746735 \tl:9.712965 \tem:0.200000\n",
      "Batch:\t1 /9\t:  0.814208 1.561090 \tl:9.662954 \tem:0.250000\n",
      "Batch:\t2 /9\t:  1.061862 2.624222 \tl:9.562988 \tem:0.300000\n",
      "Batch:\t3 /9\t:  0.926672 3.551892 \tl:9.363693 \tem:0.400000\n",
      "Batch:\t4 /9\t:  0.856121 4.409409 \tl:9.813055 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.948457 5.358160 \tl:9.612967 \tem:0.250000\n",
      "Batch:\t6 /9\t:  1.065281 6.424247 \tl:9.613770 \tem:0.300000\n",
      "Batch:\t7 /9\t:  0.969812 7.395230 \tl:9.413551 \tem:0.350000\n",
      "Batch:\t8 /9\t:  0.878808 8.274768 \tl:9.513206 \tem:0.300000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch performance:  8.4890sec Trl:9.585461 \tTrem:0.283333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  120 / 300\n",
      "Batch:\t0 /9\t:  1.014940 1.014958 \tl:9.463761 \tem:0.400000\n",
      "Batch:\t1 /9\t:  0.984979 2.000553 \tl:9.812428 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.855098 2.856903 \tl:9.663096 \tem:0.100000\n",
      "Batch:\t3 /9\t:  0.854748 3.712774 \tl:9.463186 \tem:0.350000\n",
      "Batch:\t4 /9\t:  0.869599 4.582649 \tl:9.762732 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.954479 5.537350 \tl:9.463358 \tem:0.350000\n",
      "Batch:\t6 /9\t:  0.865898 6.404870 \tl:9.763111 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.860257 7.266157 \tl:9.613073 \tem:0.300000\n",
      "Batch:\t8 /9\t:  0.863730 8.130424 \tl:9.563135 \tem:0.300000\n",
      "\n",
      "Epoch performance:  8.3691sec Trl:9.618653 \tTrem:0.266667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  121 / 300\n",
      "Batch:\t0 /9\t:  1.015220 1.015244 \tl:9.763172 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.814673 1.831068 \tl:9.363751 \tem:0.350000\n",
      "Batch:\t2 /9\t:  0.875295 2.706912 \tl:9.413211 \tem:0.450000\n",
      "Batch:\t3 /9\t:  0.959120 3.666738 \tl:9.563142 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.934257 4.602485 \tl:9.313277 \tem:0.500000\n",
      "Batch:\t5 /9\t:  0.853359 5.457240 \tl:9.712902 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.857217 6.315744 \tl:9.563051 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.935413 7.251907 \tl:9.712849 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.926851 8.179929 \tl:9.615903 \tem:0.300000\n",
      "\n",
      "Epoch performance:  8.3868sec Trl:9.557918 \tTrem:0.294444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  122 / 300\n",
      "Batch:\t0 /9\t:  1.009170 1.009187 \tl:9.862713 \tem:0.050000\n",
      "Batch:\t1 /9\t:  1.174644 2.184387 \tl:9.612898 \tem:0.300000\n",
      "Batch:\t2 /9\t:  1.069154 3.254984 \tl:10.062977 \tem:0.050000\n",
      "Batch:\t3 /9\t:  0.852322 4.107801 \tl:9.763081 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.912188 5.020670 \tl:9.713165 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.914931 5.936919 \tl:9.713387 \tem:0.050000\n",
      "Batch:\t6 /9\t:  0.942409 6.880718 \tl:9.613285 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.835020 7.715940 \tl:9.562838 \tem:0.350000\n",
      "Batch:\t8 /9\t:  0.800462 8.517305 \tl:9.712479 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.7434sec Trl:9.735202 \tTrem:0.177778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  123 / 300\n",
      "Batch:\t0 /9\t:  1.018002 1.018025 \tl:9.613230 \tem:0.200000\n",
      "Batch:\t1 /9\t:  1.083316 2.102113 \tl:9.612789 \tem:0.450000\n",
      "Batch:\t2 /9\t:  0.915396 3.019145 \tl:9.562804 \tem:0.300000\n",
      "Batch:\t3 /9\t:  0.886943 3.906675 \tl:9.463279 \tem:0.350000\n",
      "Batch:\t4 /9\t:  0.920614 4.828423 \tl:9.662598 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.876142 5.705140 \tl:9.712696 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.885920 6.591497 \tl:9.762920 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.971127 7.563848 \tl:9.713209 \tem:0.150000\n",
      "Batch:\t8 /9\t:  1.047843 8.612746 \tl:9.912544 \tem:0.100000\n",
      "\n",
      "Epoch performance:  8.8142sec Trl:9.668452 \tTrem:0.244444 \tTeem:0.000000\n",
      "\n",
      "Epoch:  124 / 300\n",
      "Batch:\t0 /9\t:  0.972615 0.972628 \tl:9.663109 \tem:0.150000\n",
      "Batch:\t1 /9\t:  0.861572 1.835460 \tl:9.613399 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.954093 2.790695 \tl:9.613127 \tem:0.300000\n",
      "Batch:\t3 /9\t:  0.884644 3.675924 \tl:9.462982 \tem:0.400000\n",
      "Batch:\t4 /9\t:  0.813579 4.490486 \tl:9.662914 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.954702 5.446754 \tl:9.862403 \tem:0.150000\n",
      "Batch:\t6 /9\t:  1.193126 6.640965 \tl:9.663145 \tem:0.250000\n",
      "Batch:\t7 /9\t:  1.086139 7.728345 \tl:9.413628 \tem:0.300000\n",
      "Batch:\t8 /9\t:  1.031614 8.761042 \tl:9.762873 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.9591sec Trl:9.635287 \tTrem:0.255556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  125 / 300\n",
      "Batch:\t0 /9\t:  1.104907 1.104923 \tl:9.613696 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.995105 2.100600 \tl:9.662850 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.906960 3.009054 \tl:9.612673 \tem:0.350000\n",
      "Batch:\t3 /9\t:  0.953716 3.963608 \tl:9.513484 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.924177 4.888412 \tl:9.612864 \tem:0.350000\n",
      "Batch:\t5 /9\t:  0.932249 5.821691 \tl:9.662828 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.835530 6.657591 \tl:9.762482 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.944984 7.603188 \tl:9.762533 \tem:0.300000\n",
      "Batch:\t8 /9\t:  0.919887 8.523830 \tl:9.613319 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.7939sec Trl:9.646303 \tTrem:0.266667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  126 / 300\n",
      "Batch:\t0 /9\t:  0.962129 0.962142 \tl:9.413555 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.858525 1.821454 \tl:9.713108 \tem:0.100000\n",
      "Batch:\t2 /9\t:  0.898698 2.721436 \tl:9.962560 \tem:0.150000\n",
      "Batch:\t3 /9\t:  0.989711 3.712074 \tl:9.463277 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.807330 4.520005 \tl:9.713098 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.904088 5.425595 \tl:9.762807 \tem:0.250000\n",
      "Batch:\t6 /9\t:  1.028902 6.455853 \tl:9.763012 \tem:0.100000\n",
      "Batch:\t7 /9\t:  0.976104 7.432635 \tl:9.463640 \tem:0.350000\n",
      "Batch:\t8 /9\t:  0.857282 8.291102 \tl:9.613092 \tem:0.350000\n",
      "\n",
      "Epoch performance:  8.5038sec Trl:9.652017 \tTrem:0.227778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  127 / 300\n",
      "Batch:\t0 /9\t:  1.049446 1.049467 \tl:9.762352 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.919805 1.969553 \tl:9.812784 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.933017 2.905368 \tl:9.862814 \tem:0.150000\n",
      "Batch:\t3 /9\t:  1.018748 3.925384 \tl:9.662827 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.918282 4.844448 \tl:9.763193 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.969527 5.814895 \tl:9.762490 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.867997 6.683497 \tl:9.562613 \tem:0.300000\n",
      "Batch:\t7 /9\t:  0.855117 7.539829 \tl:9.762331 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.919206 8.460707 \tl:9.813124 \tem:0.200000\n",
      "\n",
      "Epoch performance:  8.6633sec Trl:9.751614 \tTrem:0.222222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  128 / 300\n",
      "Batch:\t0 /9\t:  1.069583 1.069597 \tl:9.463334 \tem:0.350000\n",
      "Batch:\t1 /9\t:  0.789007 1.859327 \tl:9.713116 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.914810 2.775224 \tl:9.762434 \tem:0.300000\n",
      "Batch:\t3 /9\t:  0.908313 3.684793 \tl:9.612874 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.963599 4.649548 \tl:9.762800 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.861045 5.511909 \tl:9.662374 \tem:0.300000\n",
      "Batch:\t6 /9\t:  0.907147 6.420064 \tl:9.563049 \tem:0.300000\n",
      "Batch:\t7 /9\t:  1.078691 7.500108 \tl:9.463671 \tem:0.350000\n",
      "Batch:\t8 /9\t:  0.898838 8.399143 \tl:9.812522 \tem:0.100000\n",
      "\n",
      "Epoch performance:  8.6004sec Trl:9.646242 \tTrem:0.261111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  129 / 300\n",
      "Batch:\t0 /9\t:  0.983872 0.983893 \tl:9.663126 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.922619 1.908837 \tl:9.513126 \tem:0.300000\n",
      "Batch:\t2 /9\t:  0.932219 2.841654 \tl:9.612568 \tem:0.300000\n",
      "Batch:\t3 /9\t:  0.885845 3.728370 \tl:9.569744 \tem:0.300000\n",
      "Batch:\t4 /9\t:  0.984695 4.713598 \tl:9.612568 \tem:0.300000\n",
      "Batch:\t5 /9\t:  0.974818 5.689307 \tl:9.313458 \tem:0.500000\n",
      "Batch:\t6 /9\t:  0.992290 6.682064 \tl:9.712406 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.848667 7.531293 \tl:9.912096 \tem:0.100000\n",
      "Batch:\t8 /9\t:  0.855562 8.387096 \tl:9.463187 \tem:0.300000\n",
      "\n",
      "Epoch performance:  8.5848sec Trl:9.596920 \tTrem:0.288889 \tTeem:0.000000\n",
      "\n",
      "Epoch:  130 / 300\n",
      "Batch:\t0 /9\t:  1.061614 1.061633 \tl:9.562872 \tem:0.250000\n",
      "Batch:\t1 /9\t:  1.050241 2.112568 \tl:9.412883 \tem:0.450000\n",
      "Batch:\t2 /9\t:  0.809759 2.922857 \tl:9.513112 \tem:0.350000\n",
      "Batch:\t3 /9\t:  0.930274 3.853860 \tl:9.663551 \tem:0.200000\n",
      "Batch:\t4 /9\t:  1.031785 4.887096 \tl:9.862615 \tem:0.150000\n",
      "Batch:\t5 /9\t:  1.050403 5.938178 \tl:9.813109 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.956343 6.895888 \tl:9.562754 \tem:0.250000\n",
      "Batch:\t7 /9\t:  1.066222 7.963528 \tl:9.612627 \tem:0.350000\n",
      "Batch:\t8 /9\t:  1.006275 8.971285 \tl:9.463439 \tem:0.350000\n",
      "\n",
      "Epoch performance:  9.1749sec Trl:9.607440 \tTrem:0.283333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  131 / 300\n",
      "Batch:\t0 /9\t:  0.945636 0.945656 \tl:9.513252 \tem:0.250000\n",
      "Batch:\t1 /9\t:  1.098558 2.044759 \tl:9.712843 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.918182 2.963749 \tl:9.862698 \tem:0.150000\n",
      "Batch:\t3 /9\t:  1.068156 4.035287 \tl:9.763298 \tem:0.100000\n",
      "Batch:\t4 /9\t:  1.019814 5.056044 \tl:9.263792 \tem:0.400000\n",
      "Batch:\t5 /9\t:  0.855221 5.913028 \tl:9.463361 \tem:0.300000\n",
      "Batch:\t6 /9\t:  1.102445 7.016504 \tl:9.862302 \tem:0.200000\n",
      "Batch:\t7 /9\t:  0.974981 7.992089 \tl:9.612440 \tem:0.300000\n",
      "Batch:\t8 /9\t:  1.117810 9.111294 \tl:9.713105 \tem:0.100000\n",
      "\n",
      "Epoch performance:  9.3168sec Trl:9.640788 \tTrem:0.216667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  132 / 300\n",
      "Batch:\t0 /9\t:  1.075196 1.075215 \tl:9.713037 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.851826 1.928204 \tl:9.912891 \tem:0.100000\n",
      "Batch:\t2 /9\t:  1.031163 2.960747 \tl:9.513384 \tem:0.350000\n",
      "Batch:\t3 /9\t:  0.859469 3.820497 \tl:9.612657 \tem:0.300000\n",
      "Batch:\t4 /9\t:  1.093381 4.914543 \tl:9.762908 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.855755 5.772228 \tl:9.413290 \tem:0.400000\n",
      "Batch:\t6 /9\t:  0.898069 6.670861 \tl:9.612938 \tem:0.250000\n",
      "Batch:\t7 /9\t:  1.067122 7.739294 \tl:9.712624 \tem:0.200000\n",
      "Batch:\t8 /9\t:  1.048798 8.789294 \tl:9.413111 \tem:0.400000\n",
      "\n",
      "Epoch performance:  8.9930sec Trl:9.629649 \tTrem:0.283333 \tTeem:0.005102\n",
      "\n",
      "Epoch:  133 / 300\n",
      "Batch:\t0 /9\t:  0.999068 0.999089 \tl:9.613258 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.881268 1.880714 \tl:9.712519 \tem:0.250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t2 /9\t:  1.073647 2.955473 \tl:9.513172 \tem:0.350000\n",
      "Batch:\t3 /9\t:  0.872341 3.828992 \tl:9.613461 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.849138 4.678964 \tl:9.713265 \tem:0.250000\n",
      "Batch:\t5 /9\t:  1.103000 5.783164 \tl:9.662983 \tem:0.300000\n",
      "Batch:\t6 /9\t:  1.066426 6.850890 \tl:9.812792 \tem:0.100000\n",
      "Batch:\t7 /9\t:  0.979954 7.832031 \tl:9.662844 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.893759 8.726369 \tl:9.563050 \tem:0.300000\n",
      "\n",
      "Epoch performance:  8.9259sec Trl:9.651927 \tTrem:0.238889 \tTeem:0.005102\n",
      "\n",
      "Epoch:  134 / 300\n",
      "Batch:\t0 /9\t:  0.845987 0.846007 \tl:9.513051 \tem:0.350000\n",
      "Batch:\t1 /9\t:  1.040123 1.887426 \tl:9.313068 \tem:0.500000\n",
      "Batch:\t2 /9\t:  0.893584 2.781278 \tl:9.662793 \tem:0.250000\n",
      "Batch:\t3 /9\t:  1.006500 3.788455 \tl:9.712775 \tem:0.250000\n",
      "Batch:\t4 /9\t:  0.963308 4.752091 \tl:9.712475 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.842173 5.594843 \tl:9.862380 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.842191 6.438354 \tl:9.663465 \tem:0.150000\n",
      "Batch:\t7 /9\t:  0.851934 7.291454 \tl:9.613250 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.921447 8.213878 \tl:9.513367 \tem:0.300000\n",
      "\n",
      "Epoch performance:  8.4110sec Trl:9.618514 \tTrem:0.272222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  135 / 300\n",
      "Batch:\t0 /9\t:  0.779545 0.779557 \tl:9.612501 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.835715 1.615510 \tl:9.512817 \tem:0.400000\n",
      "Batch:\t2 /9\t:  0.845515 2.461646 \tl:9.363513 \tem:0.500000\n",
      "Batch:\t3 /9\t:  0.961287 3.423405 \tl:9.862667 \tem:0.100000\n",
      "Batch:\t4 /9\t:  0.882730 4.307135 \tl:9.613123 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.845574 5.152957 \tl:9.513464 \tem:0.250000\n",
      "Batch:\t6 /9\t:  0.899503 6.053276 \tl:9.512960 \tem:0.350000\n",
      "Batch:\t7 /9\t:  1.160151 7.214096 \tl:9.413303 \tem:0.400000\n",
      "Batch:\t8 /9\t:  0.897888 8.112302 \tl:9.762082 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.3219sec Trl:9.574048 \tTrem:0.305556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  136 / 300\n",
      "Batch:\t0 /9\t:  0.916334 0.916352 \tl:9.562920 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.934005 1.850908 \tl:9.513084 \tem:0.300000\n",
      "Batch:\t2 /9\t:  1.092191 2.944460 \tl:9.563065 \tem:0.250000\n",
      "Batch:\t3 /9\t:  0.984922 3.930364 \tl:9.563334 \tem:0.200000\n",
      "Batch:\t4 /9\t:  1.026230 4.957739 \tl:9.562754 \tem:0.350000\n",
      "Batch:\t5 /9\t:  1.087787 6.046359 \tl:9.812377 \tem:0.150000\n",
      "Batch:\t6 /9\t:  0.854551 6.901478 \tl:9.962458 \tem:0.050000\n",
      "Batch:\t7 /9\t:  0.879835 7.782112 \tl:9.662740 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.884660 8.667577 \tl:9.663157 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.9081sec Trl:9.651765 \tTrem:0.227778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  137 / 300\n",
      "Batch:\t0 /9\t:  1.047962 1.047981 \tl:9.712323 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.801871 1.850978 \tl:9.812973 \tem:0.200000\n",
      "Batch:\t2 /9\t:  0.838407 2.689691 \tl:9.512768 \tem:0.400000\n",
      "Batch:\t3 /9\t:  0.929705 3.620357 \tl:9.563461 \tem:0.200000\n",
      "Batch:\t4 /9\t:  1.034347 4.655970 \tl:9.513546 \tem:0.400000\n",
      "Batch:\t5 /9\t:  0.985537 5.643085 \tl:9.612837 \tem:0.350000\n",
      "Batch:\t6 /9\t:  0.854607 6.498185 \tl:9.712885 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.900748 7.399497 \tl:9.712763 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.916455 8.316524 \tl:9.563433 \tem:0.200000\n",
      "\n",
      "Epoch performance:  8.4875sec Trl:9.635221 \tTrem:0.277778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  138 / 300\n",
      "Batch:\t0 /9\t:  0.955732 0.955749 \tl:9.861565 \tem:0.200000\n",
      "Batch:\t1 /9\t:  1.180872 2.137210 \tl:9.713030 \tem:0.100000\n",
      "Batch:\t2 /9\t:  1.039880 3.177754 \tl:9.512987 \tem:0.350000\n",
      "Batch:\t3 /9\t:  0.985660 4.163887 \tl:9.612902 \tem:0.250000\n",
      "Batch:\t4 /9\t:  1.017405 5.182343 \tl:9.712996 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.962594 6.145427 \tl:9.762606 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.922102 7.070383 \tl:9.662855 \tem:0.250000\n",
      "Batch:\t7 /9\t:  0.813342 7.884915 \tl:9.613026 \tem:0.250000\n",
      "Batch:\t8 /9\t:  0.918916 8.804318 \tl:9.712925 \tem:0.200000\n",
      "\n",
      "Epoch performance:  9.0056sec Trl:9.684988 \tTrem:0.227778 \tTeem:0.000000\n",
      "\n",
      "Epoch:  139 / 300\n",
      "Batch:\t0 /9\t:  0.899291 0.899320 \tl:9.563195 \tem:0.300000\n",
      "Batch:\t1 /9\t:  0.868661 1.768738 \tl:9.562737 \tem:0.350000\n",
      "Batch:\t2 /9\t:  0.892731 2.662569 \tl:9.513127 \tem:0.300000\n",
      "Batch:\t3 /9\t:  0.864156 3.527868 \tl:9.663179 \tem:0.200000\n",
      "Batch:\t4 /9\t:  0.944200 4.473415 \tl:9.768503 \tem:0.200000\n",
      "Batch:\t5 /9\t:  0.879104 5.353358 \tl:9.811927 \tem:0.200000\n",
      "Batch:\t6 /9\t:  0.926322 6.280237 \tl:9.413496 \tem:0.350000\n",
      "Batch:\t7 /9\t:  1.021691 7.302688 \tl:9.712952 \tem:0.150000\n",
      "Batch:\t8 /9\t:  0.957985 8.261625 \tl:9.761887 \tem:0.250000\n",
      "\n",
      "Epoch performance:  8.4613sec Trl:9.641223 \tTrem:0.255556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  140 / 300\n",
      "Batch:\t0 /9\t:  0.947361 0.947376 \tl:10.011915 \tem:0.100000\n",
      "Batch:\t1 /9\t:  0.800673 1.748602 \tl:9.513699 \tem:0.250000\n",
      "Batch:\t2 /9\t:  0.914735 2.664500 \tl:9.562849 \tem:0.350000\n",
      "Batch:\t3 /9\t:  0.950615 3.616577 \tl:9.813231 \tem:0.150000\n",
      "Batch:\t4 /9\t:  0.849938 4.467410 \tl:9.663324 \tem:0.250000\n",
      "Batch:\t5 /9\t:  0.891343 5.360136 \tl:9.363449 \tem:0.400000\n",
      "Batch:\t6 /9\t:  0.922733 6.284584 \tl:9.612963 \tem:0.300000\n",
      "Batch:\t7 /9\t:  0.937127 7.223052 \tl:9.712883 \tem:0.100000\n",
      "Batch:\t8 /9\t:  0.852289 8.075716 \tl:9.812416 \tem:0.150000\n",
      "\n",
      "Epoch performance:  8.2879sec Trl:9.674081 \tTrem:0.227778 \tTeem:0.005102\n",
      "\n",
      "Epoch:  141 / 300\n",
      "Batch:\t0 /9\t:  1.034587 1.034604 \tl:9.612938 \tem:0.250000\n",
      "Batch:\t1 /9\t:  0.804795 1.840182 \tl:9.763155 \tem:0.150000\n",
      "Batch:\t2 /9\t:  0.916609 2.758057 \tl:9.712883 \tem:0.200000\n",
      "Batch:\t3 /9\t:  0.860136 3.619086 \tl:9.563439 \tem:0.300000\n",
      "Batch:\t4 /9\t:  1.095731 4.716114 \tl:9.363445 \tem:0.350000\n",
      "Batch:\t5 /9\t:  1.007137 5.724686 \tl:9.761761 \tem:0.250000\n",
      "Batch:\t6 /9\t:  1.007714 6.733793 \tl:9.912911 \tem:0.200000\n",
      "Batch:\t7 /9\t:  1.017009 7.751924 \tl:9.712866 \tem:0.200000\n",
      "Batch:\t8 /9\t:  0.959895 8.712311 \tl:9.662886 \tem:0.250000\n"
     ]
    }
   ],
   "source": [
    "op = training_loop(_models=[ques_model, para_model, match_LSTM_encoder_model, pointer_decoder_model],\n",
    "                   _data=data,\n",
    "                   _debug=macros['debug'],\n",
    "                   _save_best=True,\n",
    "                   _test_eval=TEST_EVERY_,\n",
    "                       _train_eval=1,\n",
    "#                        _test_every=0,\n",
    "                      _epochs=EPOCHS,\n",
    "                      _macros=macros)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if gradients are being passed\n",
    "p = list(filter(lambda p: p.requires_grad, ques_model.parameters())) + \\\n",
    "    list(filter(lambda p: p.requires_grad, para_model.parameters())) + \\\n",
    "    list(para_model.parameters()) + \\\n",
    "    list(pointer_decoder_model.parameters())\n",
    "                       \n",
    "print([ x.grad.sum().item() for x in p])                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "\n",
    "So far, we plot the training losss. \n",
    "Shall we superimpose test loss on it too? We don't calculate test loss per batch though (fortunately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "print(\"Training Loss\")\n",
    "visualize_loss(op[0], \"train loss\", _only_epoch=False)\n",
    "\n",
    "# if len(op[1]) > 0:\n",
    "\n",
    "print(\"Training EM\")\n",
    "visualize_loss(op[1], \"train em\")\n",
    "\n",
    "print(\"Testing EM\")\n",
    "visualize_loss(op[3], \"test em\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
