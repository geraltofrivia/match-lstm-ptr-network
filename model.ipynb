{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA over unstructured data\n",
    "\n",
    "Using Match LSTM, Pointer Networks, as mentioned in paper https://arxiv.org/pdf/1608.07905.pdf\n",
    "\n",
    "We start with the pre-processing provided by https://github.com/MurtyShikhar/Question-Answering to clean up the data and make neat para, ques files.\n",
    "\n",
    "\n",
    "### @TODOs:\n",
    "\n",
    "1. [done] _Figure out how to put in real, pre-trained embeddings in embeddings layer._\n",
    "2. [done] _Explicitly provide batch size when instantiating model_\n",
    "3. [done] is ./val.ids.* validation set or test set?: **validation**\n",
    "4. [done:em] emInstead of test loss, calculate test acc metrics\n",
    "    1. todo: new metrics like P, R, F1\n",
    "5. [done] Update unit test codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "from io import open\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import traceback\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from networks import Encoder, MatchLSTMEncoder, PointerDecoder\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug Legend\n",
    "\n",
    "- 5: Print everything that goes in every tensor.\n",
    "- 4: ??\n",
    "- 3: Check every model individually\n",
    "- 2: Print things in training loops\n",
    "- 1: ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macros \n",
    "DATA_LOC = './data/domain/'\n",
    "MODEL_LOC = './models/mlstms/domain/'\n",
    "DEBUG = 1\n",
    "\n",
    "# nn Macros\n",
    "QUES_LEN, PARA_LEN =  30, 200\n",
    "VOCAB_SIZE = 120000\n",
    "# VOCAB_SIZE = glove_file.shape[1]               # @TODO: get actual size\n",
    "HIDDEN_DIM = 150\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 45                  # Might have total 100 batches.\n",
    "EPOCHS = 300\n",
    "TEST_EVERY_ = 1\n",
    "LR = 0.001\n",
    "CROP = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder \n",
    "Use a simple lstm class to have encoder for question and paragraph. \n",
    "The output of these will be used in the match lstm\n",
    "\n",
    "$H^p = LSTM(P)$ \n",
    "\n",
    "\n",
    "$H^q = LSTM(Q)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "    \n",
    "#     def __init__(self, inputlen, macros, glove_file, device):\n",
    "#         super(Encoder, self).__init__()\n",
    "        \n",
    "#         # Catch dim\n",
    "#         self.inputlen = inputlen\n",
    "#         self.hiddendim = macros['hidden_dim']\n",
    "#         self.embeddingdim =  macros['embedding_dim']\n",
    "#         self.vocablen = macros['vocab_size']\n",
    "# #         self.device = macros['device']\n",
    "#         self.batch_size = macros['batch_size']\n",
    "#         self.debug = macros['debug']\n",
    "        \n",
    "#         # Embedding Layer\n",
    "# #         self.embedding = nn.Embedding(self.vocablen, self.embeddingdim)\n",
    "#         self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(glove_file))\n",
    "#         self.embedding.weight.requires_grad = True\n",
    "       \n",
    "#         # LSTM Layer\n",
    "#         self.lstm = nn.LSTM(self.embeddingdim, self.hiddendim, bidirectional=True)\n",
    "        \n",
    "#     def init_hidden(self, batch_size, device):\n",
    "        \n",
    "#         # Returns a new hidden layer var for LSTM\n",
    "#         return (torch.zeros((2, batch_size, self.hiddendim), device=device), \n",
    "#                 torch.zeros((2, batch_size, self.hiddendim), device=device))\n",
    "    \n",
    "#     def forward(self, x, h):\n",
    "        \n",
    "#         # Input: x (batch, len ) (current input)\n",
    "#         # Hidden: h (1, batch, hiddendim) (last hidden state)\n",
    "        \n",
    "#         # Batchsize: b int (inferred)\n",
    "#         b = x.shape[0]\n",
    "        \n",
    "#         if self.debug > 4: print(\"x:\\t\", x.shape)\n",
    "#         if self.debug > 4: print(\"h:\\t\", h[0].shape, h[1].shape)\n",
    "        \n",
    "#         x_emb = self.embedding(x)\n",
    "#         if self.debug > 4: print(\"x_emb:\\t\", x_emb.shape)\n",
    "            \n",
    "#         ycap, h = self.lstm(x_emb.view(-1, b, self.embeddingdim), h)\n",
    "#         if self.debug > 4: print(\"ycap:\\t\", ycap.shape)\n",
    "        \n",
    "#         return ycap, h\n",
    "    \n",
    "    \n",
    "# # with torch.no_grad():\n",
    "# #     print (\"Trying out question encoder LSTM\")\n",
    "# #     model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "# #     dummy_x = torch.tensor([22,45,12], dtype=torch.long)\n",
    "# #     hidden = model.init_hidden()\n",
    "# #     ycap, h = model(dummy_x, hidden)\n",
    "    \n",
    "# #     print(ycap.shape)\n",
    "# #     print(h[0].shape, h[1].shape)\n",
    "\n",
    "\n",
    "# if DEBUG >= 4:\n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         macros = {\n",
    "#         \"ques_len\": QUES_LEN,\n",
    "#         \"hidden_dim\": HIDDEN_DIM, \n",
    "#         \"vocab_size\": VOCAB_SIZE, \n",
    "#         \"batch_size\": BATCH_SIZE,\n",
    "#         \"para_len\": PARA_LEN,\n",
    "#         \"embedding_dim\": EMBEDDING_DIM,\n",
    "#         \"lr\": LR,\n",
    "#         \"debug\":4,\n",
    "#         \"device\":device\n",
    "#     }\n",
    "\n",
    "#         dummy_para = torch.randint(0,VOCAB_SIZE-1,(PARA_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,PARA_LEN).long()\n",
    "#     #     print (dummy_para.shape)\n",
    "#         dummy_question = torch.randint(0,VOCAB_SIZE-1,(QUES_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,QUES_LEN).long()\n",
    "#     #     print (dummy_question.shape)\n",
    "#         glove_file = torch.randn((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "#     #     print(\"LSTM with batches\")\n",
    "#         ques_model = Encoder(QUES_LEN, macros, glove_file).cuda(device)\n",
    "#         para_model = Encoder(QUES_LEN, macros, glove_file).cuda(device)\n",
    "#         ques_hidden = ques_model.init_hidden(BATCH_SIZE)\n",
    "#         para_hidden = para_model.init_hidden(BATCH_SIZE)\n",
    "#         ques_embedded,hidden_ques = ques_model(dummy_question,ques_hidden)\n",
    "#         para_embedded,hidden_para = para_model(dummy_para,para_hidden)\n",
    "        \n",
    "#         print (ques_embedded.shape) # question_length,batch,embedding_dim\n",
    "#         print (para_embedded.shape) # para_length,batch,embedding_dim\n",
    "#         print (hidden_para[0].shape,hidden_para[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match LSTM\n",
    "\n",
    "Use a match LSTM to compute a **summarized sequential vector** for the paragraph w.r.t the question.\n",
    "\n",
    "Consider the summarized vector ($H^r$) as the output of a new decoder, where the inputs are $H^p, H^q$ computed above. \n",
    "\n",
    "1. Attend the para word $i$ with the entire question ($H^q$)\n",
    "  \n",
    "    1. $\\vec{G}_i = tanh(W^qH^q + repeat(W^ph^p_i + W^r\\vec{h^r_{i-1} + b^p}))$\n",
    "    \n",
    "    2. *Computing it*: Here, $\\vec{G}_i$ is equivalent to `energy`, computed differently.\n",
    "    \n",
    "    3. Use a linear layer to compute the content within the $repeat$ fn.\n",
    "    \n",
    "    4. Add with another linear (without bias) with $H_q$\n",
    "    \n",
    "    5. $tanh$ the bloody thing\n",
    "  \n",
    "  \n",
    "2. Softmax over it to get $\\alpha$ weights.\n",
    "\n",
    "    1. $\\vec{\\alpha_i} = softmax(w^t\\vec{G}_i + repeat(b))$\n",
    "    \n",
    "3. Use the attention weight vector $\\vec{\\alpha_i}$ to obtain a weighted version of the question and concat it with the current token of the passage to form a vector $\\vec{z_i}$\n",
    "\n",
    "4. Use $\\vec{z_i}$ to compute the desired $h^r_i$:\n",
    "\n",
    "    1. $ h^r_i = LSTM(\\vec{z_i}, h^r_{i-1}) $\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MatchLSTMEncoder(nn.Module):\n",
    "    \n",
    "#     def __init__(self, macros, device):\n",
    "        \n",
    "#         super(MatchLSTMEncoder, self).__init__()\n",
    "        \n",
    "#         self.hidden_dim = macros['hidden_dim']\n",
    "#         self.ques_len = macros['ques_len']\n",
    "#         self.batch_size = macros['batch_size']\n",
    "#         self.debug = macros['debug']    \n",
    "        \n",
    "#         # Catch lens and params\n",
    "#         self.lin_g_repeat_a_dense = nn.Linear(2*self.hidden_dim, self.hidden_dim)\n",
    "#         self.lin_g_repeat_b_dense = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "#         self.lin_g_nobias = nn.Linear(2*self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        \n",
    "#         self.alpha_i_w = nn.Parameter(torch.rand((self.hidden_dim, 1)))\n",
    "#         self.alpha_i_b = nn.Parameter(torch.rand((1)))\n",
    "        \n",
    "#         self.lstm_summary = nn.LSTM((self.ques_len+1)*2*self.hidden_dim, self.hidden_dim)\n",
    "                                      \n",
    "    \n",
    "#     def forward(self, H_p, h_ri, H_q, hidden, device):\n",
    "#         \"\"\"\n",
    "#             Ideally, we would have manually unrolled the lstm \n",
    "#             but due to memory constraints, we do it in the module.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Find the batchsize\n",
    "#         batch_size = H_p.shape[1]\n",
    "        \n",
    "#         H_r = torch.empty((0, batch_size, self.hidden_dim), device=device, dtype=torch.float)\n",
    "#         H_r = torch.cat((H_r, h_ri), dim=0)\n",
    "        \n",
    "#         if self.debug > 4:\n",
    "#             print( \"H_p:\\t\\t\\t\", H_p.shape)\n",
    "#             print( \"H_q:\\t\\t\\t\", H_q.shape)\n",
    "#             print( \"h_ri:\\t\\t\\t\", h_ri.shape)\n",
    "#             print( \"H_r:\\t\\t\\t\", H_r.shape)\n",
    "#             print( \"hid:\\t\\t\\t\", hidden.shape)\n",
    "        \n",
    "#         for i in range(H_p.shape[0]):\n",
    "            \n",
    "#             # We call the (W^P.H^P + W^rh^r_i-1 + b^P) as lin_repeat_input.\n",
    "            \n",
    "#             # We first write out its two components as\n",
    "#             lin_repeat_input_a = self.lin_g_repeat_a_dense(H_p[i].view(1, batch_size, -1))\n",
    "#             if self.debug > 4: print(\"lin_repeat_input_a:\\t\", lin_repeat_input_a.shape)\n",
    "            \n",
    "#             lin_repeat_input_b = self.lin_g_repeat_b_dense(H_r[i].view(1, batch_size, -1))\n",
    "#             if self.debug > 4: print(\"lin_repeat_input_b:\\t\", lin_repeat_input_b.shape)\n",
    "            \n",
    "#             # Add the two terms up\n",
    "#             lin_repeat_input_a.add_(lin_repeat_input_b)\n",
    "# #             if self.debug > 4: print(\"lin_g_input_b unrepeated:\", lin_g_input_b.shape)\n",
    "\n",
    "#             lin_g_input_b = lin_repeat_input_a.repeat(H_q.shape[0], 1, 1)\n",
    "#             if self.debug > 4: print(\"lin_g_input_b:\\t\\t\", lin_g_input_b.shape)\n",
    "\n",
    "#             # lin_g_input_a = self.lin_g_nobias.matmul(H_q.view(-1, self.ques_len, self.hidden_dim)) #self.lin_g_nobias(H_q)\n",
    "#             lin_g_input_a =  self.lin_g_nobias(H_q)\n",
    "#             if self.debug > 4: print(\"lin_g_input_a:\\t\\t\", lin_g_input_a.shape)\n",
    "\n",
    "#             G_i = F.tanh(lin_g_input_a + lin_g_input_b)\n",
    "#             if self.debug > 4: print(\"G_i:\\t\\t\\t\", G_i.shape)\n",
    "#             # Note; G_i should be a 1D vector over ques_len\n",
    "\n",
    "#             # Attention weights\n",
    "#             alpha_i_input_a = G_i.view(batch_size, -1, self.hidden_dim).matmul(self.alpha_i_w).view(batch_size, 1, -1)\n",
    "#             if self.debug > 4: print(\"alpha_i_input_a:\\t\", alpha_i_input_a.shape)\n",
    "\n",
    "#             alpha_i_input = alpha_i_input_a.add_(self.alpha_i_b.view(-1,1,1).repeat(1,1,self.ques_len))\n",
    "#             if self.debug > 4: print(\"alpha_i_input:\\t\\t\", alpha_i_input.shape)\n",
    "\n",
    "#             # Softmax over alpha inputs\n",
    "#             alpha_i = F.softmax(alpha_i_input, dim=-1)\n",
    "#             if self.debug > 4: print(\"alpha_i:\\t\\t\", alpha_i.shape)\n",
    "\n",
    "#             # Weighted summary of question with alpha    \n",
    "#             z_i_input_b = (\n",
    "#                             H_q.view(batch_size, self.ques_len, -1) *\n",
    "#                            (alpha_i.view(batch_size, self.ques_len, -1).repeat(1, 1, 2*self.hidden_dim))\n",
    "#                           ).view(self.ques_len,batch_size, -1)\n",
    "#             if self.debug > 4: print(\"z_i_input_b:\\t\\t\", z_i_input_b.shape)\n",
    "\n",
    "#             z_i = torch.cat((H_p[i].view(1, batch_size, -1), z_i_input_b), dim=0)\n",
    "#             if self.debug > 4: print(\"z_i:\\t\\t\\t\", z_i.shape)\n",
    "\n",
    "#             # Pass z_i, h_ri to the LSTM \n",
    "# #             lstm_input = torch.cat((z_i.view(1, batch_size,-1), H_r[i].view(1, batch_size, -1)), dim=2)\n",
    "# #             if self.debug > 4: print(\"lstm_input:\\t\\t\", lstm_input.shape)\n",
    "\n",
    "#             # Take input from LSTM, concat in H_r and nullify the temp var.\n",
    "#             h_ri, (_, hidden) = self.lstm_summary(z_i.view(1, batch_size, -1), \n",
    "#                                              (H_r[i].view(1,batch_size, -1), hidden))\n",
    "#             if self.debug > 4:\n",
    "#                 print(\"newh_ri:\\t\\t\", h_ri.shape)\n",
    "#                 print(\"newhidden:\\t\\t\", hidden.shape)\n",
    "#             H_r = torch.cat((H_r, h_ri), dim=0)\n",
    "# #             h_ri = None\n",
    "            \n",
    "#             if self.debug > 4:\n",
    "#                 print(\"\\tH_r:\\t\\t\\t\", H_r.shape)\n",
    "# #                 print(\"hidden new:\\t\\t\", hidden[0].shape, hidden[1].shape)\n",
    "\n",
    "#         return H_r[1:]\n",
    "    \n",
    "#     def init_hidden(self, batch_size, device):\n",
    "#         # Before we've done anything, we dont have any hidden state.\n",
    "#         # Refer to the Pytorch documentation to see exactly\n",
    "#         # why they have this dimensionality.\n",
    "#         # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "#         return torch.zeros((1, batch_size, self.hidden_dim), device=device)\n",
    "# #                 torch.zeros((1, batch_size, self.hidden_dim), device=device))\n",
    "\n",
    "# # with torch.no_grad():\n",
    "# #     model = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN)\n",
    "# #     h_pi = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "# #     h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "# #     hidden = model.init_hidden()\n",
    "# #     H_q = torch.randn(QUES_LEN, BATCH_SIZE, HIDDEN_DIM)\n",
    "    \n",
    "# #     op, hid = model(h_pi, h_ri, H_q, hidden)\n",
    "    \n",
    "# #     print(\"\\nDone:op\", op.shape)\n",
    "# #     print(\"Done:hid\", hid[0].shape, hid[1].shape)\n",
    "\n",
    "# if DEBUG >= 4:\n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         macros = {\n",
    "#             \"ques_len\": QUES_LEN,\n",
    "#             \"hidden_dim\": HIDDEN_DIM, \n",
    "#             \"vocab_size\": VOCAB_SIZE, \n",
    "#             \"batch_size\": BATCH_SIZE,\n",
    "#             \"para_len\": PARA_LEN,\n",
    "#             \"embedding_dim\": EMBEDDING_DIM,\n",
    "#             \"lr\": LR,\n",
    "#             \"debug\":5,\n",
    "#             \"device\":device\n",
    "#         }\n",
    "            \n",
    "#         matchLSTMEncoder = MatchLSTMEncoder(macros).cuda(device)\n",
    "#         hidden = matchLSTMEncoder.init_hidden(BATCH_SIZE)\n",
    "#         para_embedded = torch.rand((PARA_LEN, BATCH_SIZE, 2*HIDDEN_DIM), device=device)\n",
    "#         ques_embedded = torch.rand((QUES_LEN, BATCH_SIZE, 2*HIDDEN_DIM), device=device)\n",
    "#         h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM, device=self.device)\n",
    "#     #     if DEBUG:\n",
    "#     #         print (\"init h_ri shape is: \", h_ri.shape)\n",
    "#     #         print (\"the para length is \", len(para_embedded))\n",
    "#         H_r = matchLSTMEncoder(para_embedded.view(-1,BATCH_SIZE,2*HIDDEN_DIM),\n",
    "#                                h_ri, \n",
    "#                                ques_embedded, \n",
    "#                                hidden)\n",
    "#         print(\"H_r: \", H_r.shape)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network\n",
    "\n",
    "Using a ptrnet over $H_r$ to unfold and get most probable spans.\n",
    "We use the **boundry model** to do that (predict start and end of seq).\n",
    "\n",
    "A simple energy -> softmax -> decoder. Where softmaxed energy is supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# class PointerDecoder(nn.Module):\n",
    "    \n",
    "#     def __init__(self, macros, device):\n",
    "#         super(PointerDecoder, self).__init__()\n",
    "        \n",
    "#         # Keep args\n",
    "#         self.hidden_dim = macros['hidden_dim']\n",
    "#         self.batch_size = macros['batch_size']\n",
    "#         self.para_len = macros['para_len']\n",
    "#         self.debug = macros['debug']\n",
    "        \n",
    "#         self.lin_f_repeat = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "#         self.lin_f_nobias = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        \n",
    "#         self.beta_k_w = nn.Parameter(torch.randn(self.hidden_dim, 1))\n",
    "#         self.beta_k_b = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "#         self.lstm = nn.LSTM(self.hidden_dim*self.para_len, self.hidden_dim)\n",
    "\n",
    "    \n",
    "#     def init_hidden(self, batch_size, device):\n",
    "#         # Before we've done anything, we dont have any hidden state.\n",
    "#         # Refer to the Pytorch documentation to see exactly\n",
    "#         # why they have this dimensionality.\n",
    "#         # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "#         return torch.zeros((1, batch_size, self.hidden_dim), device=device)\n",
    "# #                 torch.zeros((1, batch_size, self.hidden_dim), device=device))\n",
    "    \n",
    "#     def forward(self, h_ak, H_r, hidden):\n",
    "        \n",
    "#         # h_ak (current decoder's last op) (1,batch,hiddendim)\n",
    "#         # H_r (weighted summary of para) (P, batch, hiddendim)\n",
    "#         batch_size = H_r.shape[1]\n",
    "        \n",
    "#         if self.debug > 4:\n",
    "#             print(\"h_ak:\\t\\t\\t\", h_ak.shape)\n",
    "#             print(\"H_r:\\t\\t\\t\", H_r.shape)\n",
    "#             print(\"hidden:\\t\\t\\t\", hidden.shape)\n",
    "            \n",
    "#         # Prepare inputs for the tanh used to compute energy\n",
    "#         f_input_b = self.lin_f_repeat(h_ak)\n",
    "#         if self.debug > 4: print(\"f_input_b unrepeated:  \", f_input_b.shape)\n",
    "        \n",
    "#         #H_r shape is ([PARA_LEN, BATCHSIZE, EmbeddingDIM])\n",
    "#         f_input_b = f_input_b.repeat(H_r.shape[0], 1, 1)\n",
    "#         if self.debug > 4: print(\"f_input_b repeated:\\t\", f_input_b.shape)\n",
    "            \n",
    "#         f_input_a = self.lin_f_nobias(H_r)\n",
    "#         if self.debug > 4: print(\"f_input_a:\\t\\t\", f_input_a.shape)\n",
    "            \n",
    "#         # Send it off to tanh now\n",
    "#         F_k = F.tanh(f_input_a+f_input_b)\n",
    "#         if self.debug > 4: print(\"F_k:\\t\\t\\t\", F_k.shape) #PARA_LEN,BATCHSIZE,EmbeddingDim\n",
    "            \n",
    "#         # Attention weights\n",
    "#         beta_k_input_a = F_k.view(batch_size, -1, self.hidden_dim).matmul(self.beta_k_w).view(batch_size, 1, -1)\n",
    "#         if self.debug > 4: print(\"beta_k_input_a:\\t\\t\", beta_k_input_a.shape)\n",
    "            \n",
    "#         beta_k_input = beta_k_input_a.add_(self.beta_k_b.repeat(1,1,self.para_len))\n",
    "#         if self.debug > 4: print(\"beta_k_input:\\t\\t\", beta_k_input.shape)\n",
    "            \n",
    "#         beta_k = F.softmax(beta_k_input, dim=-1)\n",
    "#         if self.debug > 4: print(\"beta_k:\\t\\t\\t\", beta_k.shape)\n",
    "            \n",
    "#         lstm_input_a = H_r.view(batch_size, self.para_len, -1) * (beta_k.view(batch_size, self.para_len, -1).repeat(1,1,self.hidden_dim))\n",
    "#         if self.debug > 4: print(\"lstm_input_a:\\t\\t\", lstm_input_a.shape)\n",
    "            \n",
    "# #         lstm_input = torch.cat((lstm_input_a.view(1, batch_size,-1), h_ak.view(1, batch_size, -1)), dim=2)\n",
    "# #         if self.debug > 4: print(\"lstm_input:\\t\\t\", lstm_input.shape)\n",
    "        \n",
    "#         h_ak, (_, hidden) = self.lstm(lstm_input_a.view(1, batch_size, -1), (h_ak, hidden))\n",
    "        \n",
    "#         return h_ak, hidden, F.log_softmax(beta_k_input, dim=-1)\n",
    "            \n",
    "# if DEBUG > 4:\n",
    "#     with torch.no_grad():\n",
    "#         macros = {\n",
    "#             \"ques_len\": QUES_LEN,\n",
    "#             \"hidden_dim\": HIDDEN_DIM, \n",
    "#             \"vocab_size\": VOCAB_SIZE, \n",
    "#             \"batch_size\": BATCH_SIZE,\n",
    "#             \"para_len\": PARA_LEN,\n",
    "#             \"embedding_dim\": EMBEDDING_DIM,\n",
    "#             \"lr\": LR,\n",
    "#             \"debug\":5,\n",
    "#             \"device\":device\n",
    "#         }\n",
    "        \n",
    "#         pointerDecoder = PointerDecoder(macros).cuda(device)\n",
    "#         h_ak = torch.randn(1,BATCH_SIZE,HIDDEN_DIM, device=device)\n",
    "#         H_r = torch.randn(PARA_LEN, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "#         hidden = pointerDecoder.init_hidden(BATCH_SIZE)\n",
    "#         h_ak, hidden, beta_k = pointerDecoder(h_ak, H_r, hidden)\n",
    "#         print (beta_k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull the real data from disk.\n",
    "\n",
    "Files stored in `./data/squad/train.ids.*`\n",
    "Pull both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_loc, macros, crop=None):\n",
    "    \"\"\"\n",
    "        Given the dataloc and the data available in a specific format, it would pick the data up, and make trainable matrices,\n",
    "        Harvest train_P, train_Q, train_Y, test_P, test_Q, test_Y matrices in this format\n",
    "        \n",
    "        If crop given, will trim the data at a certain length\n",
    "        \n",
    "        **return_type**: np matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpacking macros\n",
    "    PARA_LEN = macros['para_len']\n",
    "    QUES_LEN = macros['ques_len']\n",
    "    \n",
    "    train_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.ids.question')))])\n",
    "    train_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.ids.context')))])\n",
    "    train_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.span')))])\n",
    "\n",
    "    test_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.ids.question')))])\n",
    "    test_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.ids.context')))])\n",
    "    test_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.span')))])\n",
    "\n",
    "    if macros['debug'] > 3:\n",
    "        print(\"Train Q: \", train_q.shape)\n",
    "        print(\"Train P: \", train_p.shape)\n",
    "        print(\"Train Y: \", train_y.shape)\n",
    "        print(\"Test Q: \", test_q.shape)\n",
    "        print(\"Test P: \", test_p.shape)\n",
    "        print(\"Test Y: \", test_y.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "        Parse the semi-raw data:\n",
    "            - shuffle\n",
    "            - pad, prepare\n",
    "            - dump useless vars\n",
    "    \"\"\"\n",
    "    # Shuffle data\n",
    "    \n",
    "    if crop:\n",
    "        index_train, index_test = np.random.choice(np.arange(len(train_p)), crop), \\\n",
    "                                  np.random.choice(np.arange(len(test_p)), crop)\n",
    "    else:\n",
    "        index_train, index_test = np.arange(len(train_p)), np.arange(len(test_p))\n",
    "        np.random.shuffle(index_train)\n",
    "        np.random.shuffle(index_test)\n",
    "\n",
    "    train_p, train_q, train_y = train_p[index_train], train_q[index_train], train_y[index_train]\n",
    "    test_p, test_q, test_y = test_p[index_test], test_q[index_test], test_y[index_test]\n",
    "\n",
    "#     sanity_check(train_p, train_y)\n",
    "\n",
    "    if macros['debug'] >= 5:\n",
    "        print(\"Max q len: \", max(len(q) for q in train_q))\n",
    "        \n",
    "    \n",
    "    # Pad and prepare\n",
    "    train_P = np.zeros((len(train_p), PARA_LEN))\n",
    "    train_Q = np.zeros((len(train_q), QUES_LEN))\n",
    "    train_Y_start = np.zeros((len(train_p), PARA_LEN))\n",
    "    train_Y_end = np.zeros((len(train_p), PARA_LEN))\n",
    "\n",
    "    test_P = np.zeros((len(test_p), PARA_LEN))\n",
    "    test_Q = np.zeros((len(test_q), QUES_LEN))\n",
    "    test_Y_start = np.zeros((len(test_p), PARA_LEN))\n",
    "    test_Y_end = np.zeros((len(test_p), PARA_LEN))\n",
    "    \n",
    "#     print(train_P.shape)\n",
    "\n",
    "    crop_train = []    # Remove these rows from training\n",
    "    for i in range(len(train_p)):\n",
    "        p = train_p[i]\n",
    "        q = train_q[i]\n",
    "        y = train_y[i]\n",
    "        \n",
    "        # First see if you can keep this example or not (due to size)\n",
    "        if y[0] >= PARA_LEN or y[1] >= PARA_LEN:\n",
    "            crop_train.append(i)\n",
    "            continue\n",
    "\n",
    "\n",
    "        train_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "        train_Q[i, :min(QUES_LEN, len(q))] = q[:min(QUES_LEN, len(q))]\n",
    "        train_Y_start[i, y[0]] = 1\n",
    "        train_Y_end[i, y[1]] = 1\n",
    "\n",
    "    crop_test = []\n",
    "    for i in range(len(test_p)):\n",
    "        p = test_p[i]\n",
    "        q = test_q[i]\n",
    "        y = test_y[i]\n",
    "\n",
    "        # First see if you can keep this example or not (due to size)\n",
    "        if y[0] >= PARA_LEN or y[1] >= PARA_LEN:\n",
    "            crop_test.append(i)\n",
    "            continue\n",
    "\n",
    "        test_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "        test_Q[i, :min(QUES_LEN, len(q))] = q[:min(QUES_LEN, len(q))]\n",
    "        test_Y_start[i, y[0]] = 1\n",
    "        test_Y_end[i, y[1]] = 1\n",
    "        \n",
    "        \n",
    "    # Remove the instances which are in crop_train\n",
    "    train_P = np.delete(train_P, crop_train, axis=0)\n",
    "    train_Q = np.delete(train_Q, crop_train, axis=0)\n",
    "    train_Y_start = np.delete(train_Y_start, crop_train, axis=0)\n",
    "    train_Y_end = np.delete(train_Y_end, crop_train, axis=0)\n",
    "    \n",
    "    test_P = np.delete(test_P, crop_test, axis=0)\n",
    "    test_Q = np.delete(test_Q, crop_test, axis=0)\n",
    "    test_Y_start = np.delete(test_Y_start, crop_test, axis=0)\n",
    "    test_Y_end = np.delete(test_Y_end, crop_test, axis=0)\n",
    "\n",
    "    if macros['debug'] >= 1:\n",
    "        print(\"Train Q: \", train_Q.shape)\n",
    "        print(\"Train P: \", train_P.shape)\n",
    "        print(\"Train Y: \", train_Y_start.shape)\n",
    "        print(\"Test Q: \", test_Q.shape)\n",
    "        print(\"Test P: \", test_P.shape)\n",
    "        print(\"Test Y: \", test_Y_start.shape)\n",
    "        print(\"Crop_train: \", len(crop_train))\n",
    "        print(\"Crop_test: \", len(crop_test))\n",
    "    # Let's free up some memory now\n",
    "    train_p, train_q, train_y, test_p, test_q, test_y = None, None, None, None, None, None\n",
    "    \n",
    "    # Load embedding matrics\n",
    "    vectors = np.load(os.path.join(data_loc, 'glove.new.trimmed.300.npy'))\n",
    "    \n",
    "    return train_P, train_Q, train_Y_start, train_Y_end, test_P, test_Q, test_Y_start, test_Y_end, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macros = {\n",
    "#     \"ques_len\": QUES_LEN,\n",
    "#     \"hidden_dim\": HIDDEN_DIM, \n",
    "#     \"vocab_size\": VOCAB_SIZE, \n",
    "#     \"batch_size\": BATCH_SIZE,\n",
    "#     \"para_len\": PARA_LEN,\n",
    "#     \"embedding_dim\": EMBEDDING_DIM,\n",
    "#     \"debug\": 5\n",
    "# } \n",
    "\n",
    "# a = prepare_data(DATA_LOC, macros=macros, crop=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, and running the model\n",
    "- Write a train fn\n",
    "- Write a training loop invoking it\n",
    "- Fill in real data\n",
    "\n",
    "----------\n",
    "\n",
    "Feats:\n",
    "- Function to test every n epochs.\n",
    "- Report train accuracy every epoch\n",
    "- Store the train, test accuracy for every instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def save_model(loc, models, epochs=0, optimizer=None):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            loc: str of the folder where the models are to be saved\n",
    "            models: dict of 'model_name': model_object\n",
    "            epochs, optimizers are int, torch.optims (discarded right now).\n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(models) is dict and len(models.keys()) == 4\n",
    "    \n",
    "    # Assumes four models. Doesn't save device/epochs/optimizer right now.\n",
    "    \n",
    "    for name in models:\n",
    "        torch.save(models[name], os.path.join(loc, name+'.torch'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(para_batch,\n",
    "          ques_batch,\n",
    "          answer_start_batch,\n",
    "          answer_end_batch,\n",
    "          ques_model,\n",
    "          para_model,\n",
    "          mlstm_model,\n",
    "          pointer_decoder_model,\n",
    "          optimizer, \n",
    "          loss_fn,\n",
    "          macros,\n",
    "          debug=2):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    :param para_batch: paragraphs (batch, max_seq_len_para) \n",
    "    :param ques_batch: questions corresponding to para (batch, max_seq_len_ques)\n",
    "    :param answer_start_batch: one-hot vector denoting pos of span start (batch, max_seq_len_para)\n",
    "    :param answer_end_batch: one-hot vector denoting pos of span end (batch, max_seq_len_para)\n",
    "    \n",
    "    # Models\n",
    "    :param ques_model: model to encode ques\n",
    "    :param para_model: model to encode para\n",
    "    :param mlstm_model: model to match para, ques to get para summary\n",
    "    :param pointer_decoder_model: model to get a pointer over start and end span pointer\n",
    "    \n",
    "    # Loss and Optimizer.\n",
    "    :param loss_fn: \n",
    "    :param optimizer: \n",
    "    \n",
    "    :return: \n",
    "    \n",
    "    \n",
    "    NOTE: When using MSE, \n",
    "        - target labels are one-hot\n",
    "        - target label is float tensor\n",
    "        - shape (batch, 1, len)\n",
    "        \n",
    "        When using CrossEntropy\n",
    "        - target is not onehot\n",
    "        - long\n",
    "        - shape (batch, )\n",
    "    \"\"\"\n",
    "    try:    \n",
    "    #     DEBUG = debug\n",
    "    #     BATCH_SIZE = macros['batch_size']\n",
    "    #     HIDDEN_DIM = macros['hidden_dim']\n",
    "\n",
    "        if debug >=2: \n",
    "            print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "            print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "            print(\"\\tanswer_start_batch:\\t\", answer_start_batch.shape)\n",
    "            print(\"\\tanswer_end_batch:\\t\\t\", answer_end_batch.shape)\n",
    "\n",
    "        # Wiping all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initializing all hidden states.\n",
    "        hidden_quesenc = ques_model.init_hidden(macros['batch_size'], device)\n",
    "        hidden_paraenc = para_model.init_hidden(macros['batch_size'], device)\n",
    "        hidden_mlstm = mlstm_model.init_hidden(macros['batch_size'], device)\n",
    "        hidden_ptrnet = pointer_decoder_model.init_hidden(macros['batch_size'], device)\n",
    "        h_ri = torch.zeros((1, macros['batch_size'], macros['hidden_dim']), dtype=torch.float, device=device)\n",
    "        h_ak = torch.zeros((1, macros['batch_size'], macros['hidden_dim']), dtype=torch.float, device=device)\n",
    "        if debug >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "\n",
    "        #passing the data through LSTM pre-processing layer\n",
    "        H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc, device=device)\n",
    "        H_p, para_model_hidden = para_model(para_batch, hidden_paraenc, device=device)\n",
    "        if debug >= 2: \n",
    "            print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "            print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "            print(\"\\tH_ri:\\t\\t\", h_ri.shape)\n",
    "    #         raw_input(\"Check memory and ye shall continue\")\n",
    "            print(\"------------Encoded hidden states------------\")\n",
    "\n",
    "        H_r = mlstm_model(H_p.view(-1, macros['batch_size'], 2*macros['hidden_dim']), h_ri, H_q, hidden_mlstm, device=device)\n",
    "        if debug >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "\n",
    "        #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "        h_ak, hidden_ptrnet, beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device=device)\n",
    "        h_ak, hidden_ptrnet, beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device=device)\n",
    "        if debug >= 2: print(\"------------Passed through pointernet------------\")\n",
    "\n",
    "\n",
    "        # For crossentropy\n",
    "        _, answer_start_batch = answer_start_batch.max(dim=2)\n",
    "        _, answer_end_batch = answer_end_batch.max(dim=2)\n",
    "        answer_start_batch = answer_start_batch.view(-1).long()\n",
    "        answer_end_batch = answer_end_batch.view(-1).long()\n",
    "#         print(beta_k_start.view(-1, macros['para_len']).shape, answer_start_batch.view(-1).shape)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = loss_fn(beta_k_start.view(-1, macros['para_len']), answer_start_batch)\n",
    "        loss += loss_fn(beta_k_end.view(-1, macros['para_len']), answer_end_batch)\n",
    "#         loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "#         loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "        if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "\n",
    "        loss.backward()\n",
    "        if debug >= 2: print(\"------------Calculated Gradients------------\")\n",
    "\n",
    "        #optimization step\n",
    "        optimizer.step()\n",
    "        if debug >= 2: print(\"------------Updated weights.------------\")\n",
    "            \n",
    "        return beta_k_start, beta_k_end, loss\n",
    "    \n",
    "    except: \n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function (no grad, no eval)\n",
    "def predict(para_batch,\n",
    "            ques_batch,\n",
    "            ques_model,\n",
    "            para_model,\n",
    "            mlstm_model,\n",
    "            pointer_decoder_model,\n",
    "            macros,\n",
    "            loss_fn=None,\n",
    "            debug=DEBUG):\n",
    "    \"\"\"\n",
    "        Function which returns the model's output based on a given set of P&Q's. \n",
    "        Does not convert to strings, gives the direct model output.\n",
    "        \n",
    "        Expects:\n",
    "            four models\n",
    "            data\n",
    "            misc macros\n",
    "    \"\"\"\n",
    "    \n",
    "#     BATCH_SIZE = macros['batch_size']\n",
    "    BATCH_SIZE = ques_batch.shape[0]\n",
    "    HIDDEN_DIM = macros['hidden_dim']\n",
    "    DEBUG = debug\n",
    "    \n",
    "    if debug >=2: \n",
    "        print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "        print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "        \n",
    "    with torch.no_grad():    \n",
    "\n",
    "        # Initializing all hidden states.\n",
    "        hidden_quesenc = ques_model.init_hidden(BATCH_SIZE, device)\n",
    "        hidden_paraenc = para_model.init_hidden(BATCH_SIZE, device)\n",
    "        hidden_mlstm = mlstm_model.init_hidden(BATCH_SIZE, device)\n",
    "        hidden_ptrnet = pointer_decoder_model.init_hidden(BATCH_SIZE, device)\n",
    "        h_ri = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        h_ak = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        if DEBUG >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "            \n",
    "        #passing the data through LSTM pre-processing layer\n",
    "        H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc, device)\n",
    "        H_p, para_model_hidden = para_model(para_batch, hidden_paraenc, device)\n",
    "        if DEBUG >= 2: \n",
    "            print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "            print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "            print(\"\\tH_ri:\\t\\t\", h_ri.shape)\n",
    "#             raw_input(\"Check memory and ye shall continue\")\n",
    "            print(\"------------Encoded hidden states------------\")\n",
    "\n",
    "        H_r = mlstm_model(H_p.view(-1, BATCH_SIZE, 2*HIDDEN_DIM), h_ri, H_q, hidden_mlstm, device)\n",
    "        if DEBUG >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "\n",
    "        #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "        h_ak, hidden_ptrnet, beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device)\n",
    "        _, _, beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device)\n",
    "        if DEBUG >= 2: print(\"------------Passed through pointernet------------\")\n",
    "                            \n",
    "        # For crossentropy\n",
    "#         _, answer_start_batch = answer_start_batch.max(dim=2)[1]\n",
    "#         _, answer_end_batch = answer_end_batch.max(dim=2)[1]\n",
    "#         print(\"labels: \", answer_start_batch.shape)[1]\n",
    "            \n",
    "#         #How will we manage batches for loss.\n",
    "#         loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "#         loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "#         if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "            \n",
    "        return (beta_k_start, beta_k_end, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval function (no grad no eval no nothing)\n",
    "def eval(y_cap, y, metrics={'em':None}):\n",
    "    \"\"\" \n",
    "        Returns the exact-match (em) metric by default.\n",
    "        Can specifiy more in a list (TODO)\n",
    "        \n",
    "        Inputs:\n",
    "        - y_cap: list of two tensors (start, end) of dim [BATCH_SIZE, PARA_LEN] each\n",
    "        - y: list of two tensors (start, end) of dim [BATCH_SIZE, 1] each\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(y[0].shape, y[1].shape, y_cap[0].shape, y_cap[1].shape)\n",
    "    \n",
    "    y_cap= torch.argmax(y_cap[0], dim=1).float(), torch.argmax(y_cap[1], dim=1).float()\n",
    "    y = torch.argmax(y[0], dim=1).float(), torch.argmax(y[1], dim=1).float()\n",
    "    \n",
    "    if \"em\" in metrics.keys():\n",
    "        metrics['em'] = (y[0].eq(y_cap[0]) & y[1].eq(y_cap[1])).sum().item()/ float(y[0].shape[0])\n",
    "        \n",
    "    if DEBUG >= 3: \n",
    "        print(\"Test performance: \", metrics)\n",
    "        print(\"------------Evaluated------------\")\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "if DEBUG >=5:\n",
    "    # Testing this function\n",
    "    metrics = {'em':None}\n",
    "#     y = torch.tensor([[3]]).float(), torch.tensor([[4]]).float()\n",
    "    y = torch.tensor([[0,0,3,0], [0,2,0,0]]), torch.tensor([[0,0,0,3], [0,0,0,3]])\n",
    "    y_cap = torch.tensor([[0,0,3,0],[0,0,3,0]]), torch.tensor([[0,0,0,3],[0,0,0,3]])\n",
    "#     y = torch.randint(0, PARA_LEN, (BATCH_SIZE,)).float(), torch.randint(0, PARA_LEN, (BATCH_SIZE,)).float()\n",
    "#     y_cap = torch.rand((BATCH_SIZE, PARA_LEN)), torch.rand((BATCH_SIZE, PARA_LEN))\n",
    "    print(eval(y_cap, y))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(_models, _data, _macros, _epochs, _save=0, _test_eval=0, _train_eval=0, _debug=2):\n",
    "    \"\"\"\n",
    "        > Instantiate models\n",
    "        > Instantiate loss, optimizer\n",
    "        > Instantiate ways to store loss\n",
    "\n",
    "        > Per epoch\n",
    "            > sample batch and give to train fn\n",
    "            > get loss\n",
    "            > if epoch %k ==0: get test accuracy\n",
    "\n",
    "        > have fn to calculate test accuracy\n",
    "        \n",
    "        > _save: int\n",
    "            > 0: dont\n",
    "            > 1+: save every _save epoch (overwrite)\n",
    "            > -1 -> save best (turned to 1 if test evals dont happen.)\n",
    "        \n",
    "        > Save the model at every epoch if we don't test on test. \n",
    "            > else save on the best performning mode\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack data\n",
    "    DEBUG = _debug\n",
    "    train_P = _data['train']['P']\n",
    "    train_Q = _data['train']['Q']\n",
    "    train_Y_start = _data['train']['Ys']\n",
    "    train_Y_end = _data['train']['Ye']\n",
    "    test_P = _data['test']['P']\n",
    "    test_Q = _data['test']['Q']\n",
    "    test_Y_start = _data['test']['Ys']\n",
    "    test_Y_end = _data['test']['Ye']\n",
    "\n",
    "    ques_model, para_model, mlstm_model, pointer_decoder_model = _models\n",
    "    _data = None\n",
    "\n",
    "    # Instantiate Loss\n",
    "#         loss_fn = nn.MSELoss()\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, ques_model.parameters())) + \n",
    "                             list(filter(lambda p: p.requires_grad, para_model.parameters())) + \n",
    "                             list(mlstm_model.parameters()) + \n",
    "                             list(pointer_decoder_model.parameters()), lr=macros['lr'])\n",
    "#         optimizer = optim.Adam(list(ques_model.parameters()) + \\\n",
    "#                                list(para_model.parameters()) + \\\n",
    "#                                list(mlstm_model.parameters()) + \\\n",
    "#                               list(pointer_decoder_model.parameters()), lr=macros['lr'])\n",
    "\n",
    "    # Losses\n",
    "    train_losses = []\n",
    "    train_em = []\n",
    "    test_losses = []\n",
    "    test_em = []\n",
    "    best_test_em = 0.0\n",
    "    found_best_test_em = False\n",
    "    \n",
    "    try: \n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(_epochs):\n",
    "            print(\"Epoch: \", epoch, \"/\", _epochs)\n",
    "\n",
    "            epoch_loss = []\n",
    "            epoch_train_em = []\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            for iter in range(int(len(train_P)/BATCH_SIZE)):\n",
    "    #         for iter in range(2):\n",
    "\n",
    "                batch_time = time.time()\n",
    "\n",
    "                # Sample batch and train on it\n",
    "                sample_index = np.random.randint(0, len(train_P), _macros['batch_size'])\n",
    "            \n",
    "#                 grad_old = sum([x.grad.sum().item() for x in params])\n",
    "\n",
    "                y_cap_start, y_cap_end, loss = train(\n",
    "                    para_batch = torch.tensor(train_P[sample_index], dtype=torch.long, device=device),\n",
    "                    ques_batch = torch.tensor(train_Q[sample_index], dtype=torch.long, device=device),\n",
    "                    answer_start_batch = torch.tensor(train_Y_start[sample_index], dtype=torch.float, device=device).view( _macros['batch_size'], 1, _macros['para_len']),\n",
    "                    answer_end_batch = torch.tensor(train_Y_end[sample_index], dtype=torch.float, device=device).view(_macros['batch_size'], 1, _macros['para_len']),\n",
    "                    ques_model = ques_model,\n",
    "                    para_model = para_model,\n",
    "                    mlstm_model = mlstm_model,\n",
    "                    pointer_decoder_model = pointer_decoder_model,\n",
    "                    optimizer = optimizer, \n",
    "                    loss_fn= loss_fn,\n",
    "                    macros=_macros,\n",
    "                    debug=_macros['debug']\n",
    "                )\n",
    "\n",
    "                if _train_eval: \n",
    "\n",
    "                    # Calculate train accuracy for this minibatch\n",
    "                    metrics = eval(\n",
    "                        y=(torch.tensor(train_Y_start[sample_index], dtype=torch.long, device=device).view( -1, _macros['para_len']),\n",
    "                            torch.tensor(train_Y_end[sample_index], dtype=torch.long, device=device).view(-1, _macros['para_len'])),\n",
    "                        y_cap=[y_cap_start.squeeze(), y_cap_end.squeeze()])\n",
    "\n",
    "                    epoch_train_em.append(metrics['em'])\n",
    "    \n",
    "                epoch_loss.append(loss.item())\n",
    "    \n",
    "#                 grad_new = sum([x.grad.sum().item() for x in params])\n",
    "\n",
    "                print(\"Batch:\\t%d\" % iter,\"/%d\\t: \" % (len(train_P)/_macros['batch_size']),\n",
    "                      str(\"%s\" % (time.time() - batch_time))[:8], \n",
    "                      str(\"\\t\\b%s\" % (time.time() - epoch_time))[:10], \n",
    "                      \"\\tl:%f\" % loss.item(),\n",
    "                      \"\\tem:%f\" % epoch_train_em[-1] if _train_eval else \"\")\n",
    "#                      \"\\t\\b\\b%s\" % grad_new - grad_old)\n",
    "#                      end=None if iter+1 == int(len(train_P)/BATCH_SIZE) else \"\\r\")\n",
    "\n",
    "            train_losses.append(epoch_loss)\n",
    "        \n",
    "            if _train_eval: train_em.append(epoch_train_em)\n",
    "            if _test_eval and epoch % _test_eval == 0:\n",
    "\n",
    "                y_cap_start, y_cap_end, test_loss = predict(\n",
    "                    para_batch = torch.tensor(test_P, dtype=torch.long, device=device),\n",
    "                    ques_batch = torch.tensor(test_Q, dtype=torch.long, device=device),\n",
    "                    ques_model = ques_model,\n",
    "                    para_model = para_model,\n",
    "                    mlstm_model = mlstm_model,\n",
    "                    pointer_decoder_model = pointer_decoder_model,\n",
    "                    macros = _macros,\n",
    "                    loss_fn= loss_fn,\n",
    "                    debug = _macros['debug']\n",
    "                )\n",
    "                metrics = eval(\n",
    "                    y=(torch.tensor(test_Y_start, dtype=torch.long, device=device).view( -1, _macros['para_len']),\n",
    "                        torch.tensor(test_Y_end, dtype=torch.long, device=device).view(-1, _macros['para_len'])),\n",
    "                    y_cap=[y_cap_start.squeeze(), y_cap_end.squeeze()])\n",
    "\n",
    "                test_losses.append(test_loss)\n",
    "                test_em.append(metrics['em'])\n",
    "                \n",
    "                # Check if we outperformed the best one.\n",
    "                if metrics['em'] > best_test_em:\n",
    "                    \n",
    "                    # Set flag\n",
    "                    found_best_test_em = True\n",
    "                    \n",
    "                    # Update value\n",
    "                    best_test_em = metrics['em']   \n",
    "                \n",
    "            # Saving logic\n",
    "            if _save == 0:\n",
    "                pass\n",
    "            elif ( _save>0 and epoch % _save == 0) or \\\n",
    "            ( _save == -1 and found_best_test_em ):\n",
    "                models = { 'ques_model': ques_model,\n",
    "                           'para_model': para_model,\n",
    "                           'mlstm_model':  mlstm_model,\n",
    "                           'pointer_decoder_model': pointer_decoder_model\n",
    "                         }\n",
    "                \n",
    "                save_model(macros['save_model_loc'], models,\n",
    "                          epochs=epoch,\n",
    "                           optimizer=optimizer)\n",
    "                \n",
    "                print(\"Saving new model on epoch %d\" % epoch)\n",
    "            \n",
    "            # Reset flags\n",
    "            found_best_test_em = False\n",
    "            \n",
    "            # At the end of every epoch, do print the average epoch loss, and other stat\n",
    "            print(\"\\nEpoch performance: \",\n",
    "                  \"%ssec\" % str(time.time() - epoch_time)[:6],\n",
    "                  \"Trl:%f\" % np.mean(epoch_loss, axis=0),\n",
    "                  \"\\tTrem:%f\" % np.mean(epoch_train_em) if _train_eval and epoch % _train_eval == 0 else \"\",\n",
    "                  \"\\tTeem:%f\\n\" % test_em[-1] if _test_eval and epoch % _test_eval == 0 else \"\\n\")\n",
    "\n",
    "#         return train_losses, train_em, test_losses, test_em\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        # someone called a ctrl+c on it. Let' return the things computed so far atlest.\n",
    "        print(\"Found keyboard interrupt. Stopping training loop\")\n",
    "        \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:       \n",
    "        return train_losses, train_em, test_losses, test_em\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "    Saving said models.\n",
    "    TODO\n",
    "\"\"\"\n",
    "# ques_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(loss, _label=\"Some label\", _only_epoch=True):\n",
    "    \"\"\"\n",
    "        Fn to visualize loss.\n",
    "        Expects either\n",
    "            - [int, int] for epoch level stuff\n",
    "            - [ [int, int], [int, int] ] for batch level data. \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = [15, 8] \n",
    "    \n",
    "    # Detect input format\n",
    "    if type(loss[0]) in [int, float, long]:\n",
    "        \n",
    "#         print(\"here\")\n",
    "        \n",
    "        plt.plot(loss)\n",
    "        plt.ylabel(_label)\n",
    "        plt.show()\n",
    "        \n",
    "    elif type(loss[0]) == list:\n",
    "        \n",
    "        if _only_epoch:\n",
    "            loss = [ sum(x) for x in loss ]\n",
    "            \n",
    "        else:\n",
    "            loss = [ y for x in loss for y in x ]\n",
    "            \n",
    "        plt.plot(loss)\n",
    "        plt.ylabel(_label)\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator\n",
    "\n",
    "One cell which instantiates and runs everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Q:  (585, 30)\n",
      "Train P:  (585, 200)\n",
      "Train Y:  (585, 200)\n",
      "Test Q:  (585, 30)\n",
      "Test P:  (585, 200)\n",
      "Test Y:  (585, 200)\n",
      "Crop_train:  7\n",
      "Crop_test:  7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Cell which pulls everything together.\n",
    "\n",
    "    > init models\n",
    "    > get data prepared\n",
    "    > pass models and data to training loop\n",
    "    > gets trained models and loss\n",
    "    > saves models\n",
    "    > visualizes loss?\n",
    "\n",
    "No other function but this one ever sees global macros!\n",
    "\"\"\"\n",
    "macros = {\n",
    "    \"ques_len\": QUES_LEN,\n",
    "    \"hidden_dim\": HIDDEN_DIM, \n",
    "    \"vocab_size\": VOCAB_SIZE, \n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"para_len\": PARA_LEN,\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"lr\": LR,\n",
    "    \"debug\":DEBUG,\n",
    "    \"save_model_loc\": MODEL_LOC\n",
    "#     \"device\": device\n",
    "} \n",
    "\n",
    "data = {'train':{}, 'test':{}}\n",
    "data['train']['P'], data['train']['Q'], data['train']['Ys'], data['train']['Ye'], \\\n",
    "data['test']['P'], data['test']['Q'], data['test']['Ys'], data['test']['Ye'], vectors = \\\n",
    "    prepare_data(DATA_LOC, macros, crop=CROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate modelshttp://localhost:8888/notebooks/model.ipynb#\n",
    "ques_model = Encoder(QUES_LEN, macros, vectors, device).cuda(device)\n",
    "para_model = Encoder(PARA_LEN, macros, vectors, device).cuda(device)\n",
    "mlstm_model = MatchLSTMEncoder(macros, device).cuda(device)\n",
    "pointer_decoder_model = PointerDecoder(macros, device).cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 / 300\n",
      "Batch:\t0 /13\t:  2.899268 2.899290 \tl:10.599597 \tem:0.000000\n",
      "Batch:\t1 /13\t:  2.098588 4.998671 \tl:10.601473 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.224442 6.224090 \tl:16.737503 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.110588 7.336225 \tl:11.116800 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.148734 8.485335 \tl:11.286239 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.208545 9.695005 \tl:11.417948 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.416246 11.11267 \tl:11.204296 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.338632 12.45235 \tl:11.151123 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.203566 13.65629 \tl:10.702057 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.236538 14.89353 \tl:10.615250 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.089864 15.98459 \tl:10.679499 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.202126 17.18744 \tl:10.677469 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.219276 18.40701 \tl:10.562300 \tem:0.000000\n",
      "\n",
      "Epoch performance:  20.184sec Trl:11.334735 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  1 / 300\n",
      "Batch:\t0 /13\t:  1.288601 1.288634 \tl:10.558344 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.295498 2.585401 \tl:10.593222 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.110733 3.697048 \tl:10.576469 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.185040 4.882676 \tl:10.578621 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.124706 6.008298 \tl:10.580595 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.117336 7.138339 \tl:10.581765 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.228946 8.367696 \tl:10.647057 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.175133 9.543792 \tl:10.592227 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.201708 10.74644 \tl:10.540033 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.154237 11.90101 \tl:10.666575 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.257226 13.15876 \tl:10.517505 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.187388 14.34681 \tl:10.562189 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.242743 15.59050 \tl:10.561272 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.123sec Trl:10.581221 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  2 / 300\n",
      "Batch:\t0 /13\t:  1.167186 1.167212 \tl:10.574823 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.097177 2.265253 \tl:10.605648 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.231361 3.497545 \tl:10.570452 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.210948 4.708691 \tl:10.572412 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.254240 5.963390 \tl:10.676628 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.263602 7.227456 \tl:10.380026 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.209419 8.437307 \tl:10.603927 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.199809 9.637675 \tl:10.518993 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.144123 10.78244 \tl:10.531889 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.201667 11.98479 \tl:10.562141 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.263051 13.24855 \tl:10.471272 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.157495 14.40687 \tl:10.558237 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.083003 15.49029 \tl:10.520990 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.025sec Trl:10.549803 \tTrem:0.001709 \tTeem:0.000000\n",
      "\n",
      "Epoch:  3 / 300\n",
      "Batch:\t0 /13\t:  1.142214 1.142230 \tl:10.504309 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.103126 2.245916 \tl:10.519783 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.226710 3.472830 \tl:10.464006 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.274137 4.747765 \tl:10.421238 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.204464 5.952880 \tl:10.544694 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.172413 7.125792 \tl:10.537102 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.170197 8.296504 \tl:10.552584 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.293326 9.590113 \tl:10.547243 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.088649 10.67930 \tl:10.420856 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.161746 11.84152 \tl:10.459864 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.146511 13.00024 \tl:10.486206 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.145915 14.14711 \tl:10.451286 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.230720 15.37931 \tl:10.651583 \tem:0.000000\n",
      "\n",
      "Epoch performance:  15.915sec Trl:10.504673 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  4 / 300\n",
      "Batch:\t0 /13\t:  1.305648 1.305668 \tl:10.477534 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.253311 2.559250 \tl:10.534729 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.251795 3.811628 \tl:10.359625 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.192209 5.004764 \tl:10.468309 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.134164 6.140115 \tl:10.607088 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.296320 7.436921 \tl:10.353140 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.223998 8.661615 \tl:10.571280 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.221058 9.883978 \tl:10.433025 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.214567 11.09950 \tl:10.457592 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.128909 12.22903 \tl:10.364666 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.301703 13.53128 \tl:10.500957 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.192293 14.72456 \tl:10.484700 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.191380 15.91645 \tl:10.393897 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.449sec Trl:10.462042 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  5 / 300\n",
      "Batch:\t0 /13\t:  1.276288 1.276304 \tl:10.281937 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.300591 2.577764 \tl:10.420095 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.233850 3.812169 \tl:10.370939 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.184797 4.997480 \tl:10.419935 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.193097 6.190789 \tl:10.422617 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.183312 7.374796 \tl:10.295092 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.144820 8.520057 \tl:10.151340 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.152580 9.673250 \tl:10.440813 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.227585 10.90134 \tl:10.360497 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.266519 12.16861 \tl:10.337448 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.218837 13.38870 \tl:10.652464 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.138152 14.52729 \tl:10.299635 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.212023 15.73979 \tl:10.612547 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.273sec Trl:10.389643 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  6 / 300\n",
      "Batch:\t0 /13\t:  1.102085 1.102108 \tl:10.302416 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.188664 2.291368 \tl:10.361710 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.178555 3.470109 \tl:10.325151 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.160423 4.631654 \tl:10.437874 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.186493 5.818800 \tl:10.423096 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.265866 7.085581 \tl:10.469971 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.200677 8.286941 \tl:10.534081 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.229875 9.517652 \tl:10.212286 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.176717 10.69460 \tl:10.293383 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.179336 11.87507 \tl:10.338276 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.295553 13.17180 \tl:10.322929 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.178714 14.35102 \tl:10.646908 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.253159 15.60497 \tl:10.286560 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.135sec Trl:10.381126 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  7 / 300\n",
      "Batch:\t0 /13\t:  1.190835 1.190852 \tl:10.378548 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.156128 2.347988 \tl:10.392788 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.217208 3.565654 \tl:10.353286 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.224196 4.802919 \tl:10.373683 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.263406 6.066903 \tl:10.327045 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.154103 7.222244 \tl:10.336287 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.206548 8.429073 \tl:10.597111 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.251183 9.680494 \tl:10.353275 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.169528 10.85108 \tl:10.595487 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.163589 12.01695 \tl:10.714811 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.173983 13.19162 \tl:10.400461 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.195802 14.38789 \tl:10.301948 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.125020 15.51394 \tl:10.285866 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.048sec Trl:10.416200 \tTrem:0.001709 \tTeem:0.000000\n",
      "\n",
      "Epoch:  8 / 300\n",
      "Batch:\t0 /13\t:  1.311484 1.311501 \tl:10.410955 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.183339 2.495799 \tl:10.320769 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.180263 3.676526 \tl:10.235895 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.149491 4.826831 \tl:10.197386 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.179295 6.006916 \tl:10.240444 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.230975 7.238461 \tl:10.249050 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.190212 8.429476 \tl:10.198064 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.238781 9.668912 \tl:10.393721 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.204755 10.87488 \tl:10.474289 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.633414 12.50929 \tl:10.249160 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.478970 13.98915 \tl:10.268774 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.509950 15.50010 \tl:10.293573 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.479967 16.98135 \tl:10.623552 \tem:0.000000\n",
      "\n",
      "Epoch performance:  17.513sec Trl:10.319664 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  9 / 300\n",
      "Batch:\t0 /13\t:  1.519825 1.519845 \tl:10.310542 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.555994 3.076156 \tl:10.091393 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.492290 4.569709 \tl:10.201294 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.455924 6.026095 \tl:10.162121 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.255589 7.282305 \tl:10.148632 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.248214 8.530966 \tl:10.314333 \tem:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t6 /13\t:  1.629773 10.16144 \tl:10.146042 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.264277 11.42599 \tl:10.143568 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.268383 12.69583 \tl:10.177079 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.224024 13.92065 \tl:10.193758 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.100074 15.03355 \tl:10.274378 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.246199 16.27998 \tl:9.976515 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.214328 17.49484 \tl:10.207054 \tem:0.000000\n",
      "\n",
      "Epoch performance:  18.031sec Trl:10.180516 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  10 / 300\n",
      "Batch:\t0 /13\t:  1.317121 1.317137 \tl:10.153084 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.185134 2.502820 \tl:9.943476 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.272467 3.775929 \tl:10.317671 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.306425 5.082540 \tl:10.136183 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.214663 6.298072 \tl:10.235573 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.206660 7.505429 \tl:10.027359 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.162761 8.669305 \tl:10.242208 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.244574 9.914696 \tl:9.892371 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.221595 11.13686 \tl:9.987556 \tem:0.022222\n",
      "Batch:\t9 /13\t:  1.238716 12.37623 \tl:9.934086 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.378128 13.75468 \tl:10.369780 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.211459 14.96646 \tl:10.115465 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.244623 16.21161 \tl:9.731517 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.747sec Trl:10.083564 \tTrem:0.001709 \tTeem:0.000000\n",
      "\n",
      "Epoch:  11 / 300\n",
      "Batch:\t0 /13\t:  1.272358 1.272383 \tl:10.252838 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.241899 2.516736 \tl:10.186334 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.211857 3.729329 \tl:10.208024 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.243113 4.972929 \tl:10.061153 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.144942 6.119003 \tl:9.874279 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.289932 7.409693 \tl:10.475174 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.290640 8.700965 \tl:9.346409 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.215538 9.916959 \tl:10.192062 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.155178 11.07270 \tl:10.070095 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.301491 12.37467 \tl:10.189409 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.284852 13.66008 \tl:9.890689 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.181087 14.84237 \tl:9.782635 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.235581 16.07880 \tl:10.313828 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.611sec Trl:10.064841 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  12 / 300\n",
      "Batch:\t0 /13\t:  1.640642 1.640656 \tl:10.087777 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.327168 2.968163 \tl:9.769502 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.173689 4.142069 \tl:9.975809 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.166757 5.309553 \tl:10.053875 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.228029 6.538554 \tl:9.688935 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.261787 7.800832 \tl:9.759376 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.300340 9.102375 \tl:9.498003 \tem:0.022222\n",
      "Batch:\t7 /13\t:  1.247857 10.35150 \tl:9.734652 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.346174 11.69838 \tl:9.979439 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.187129 12.88634 \tl:9.510120 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.297645 14.18496 \tl:9.887855 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.194970 15.38036 \tl:9.695872 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.172857 16.55382 \tl:9.969589 \tem:0.000000\n",
      "\n",
      "Epoch performance:  17.089sec Trl:9.816216 \tTrem:0.001709 \tTeem:0.000000\n",
      "\n",
      "Epoch:  13 / 300\n",
      "Batch:\t0 /13\t:  1.152736 1.152751 \tl:9.720465 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.157994 2.311701 \tl:9.407373 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.174728 3.487586 \tl:9.994823 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.349658 4.837653 \tl:9.799475 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.184494 6.022615 \tl:9.807125 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.275602 7.298455 \tl:9.465191 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.152076 8.451081 \tl:9.044508 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.238044 9.690111 \tl:9.636521 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.190650 10.88137 \tl:9.381676 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.210843 12.09339 \tl:9.889518 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.279476 13.37347 \tl:10.335836 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.276391 14.65010 \tl:9.847540 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.221141 15.87171 \tl:9.022600 \tem:0.022222\n",
      "\n",
      "Epoch performance:  16.402sec Trl:9.642512 \tTrem:0.001709 \tTeem:0.000000\n",
      "\n",
      "Epoch:  14 / 300\n",
      "Batch:\t0 /13\t:  1.149776 1.149793 \tl:9.437243 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.237900 2.388525 \tl:9.650467 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.218725 3.607807 \tl:9.644781 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.348152 4.956261 \tl:9.565754 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.149538 6.106302 \tl:9.543861 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.205772 7.312376 \tl:9.547321 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.249289 8.562309 \tl:9.303256 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.245505 9.809330 \tl:9.534592 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.167637 10.97741 \tl:9.623940 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.250164 12.22856 \tl:9.548469 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.218873 13.44792 \tl:9.137768 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.183560 14.63199 \tl:9.636426 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.340058 15.97440 \tl:9.043137 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.509sec Trl:9.478232 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  15 / 300\n",
      "Batch:\t0 /13\t:  1.193317 1.193332 \tl:9.253262 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.256938 2.450762 \tl:9.632204 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.153306 3.604923 \tl:9.053946 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.187461 4.793300 \tl:9.441423 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.155734 5.950009 \tl:9.088122 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.177742 7.128360 \tl:9.072201 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.205092 8.334352 \tl:9.041765 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.246834 9.581979 \tl:9.354176 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.263400 10.84659 \tl:9.343435 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.138380 11.98619 \tl:9.605530 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.185302 13.17207 \tl:9.712057 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.193194 14.36557 \tl:9.085930 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.307831 15.67472 \tl:9.380798 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.210sec Trl:9.312681 \tTrem:0.001709 \tTeem:0.000000\n",
      "\n",
      "Epoch:  16 / 300\n",
      "Batch:\t0 /13\t:  1.335052 1.335068 \tl:9.295473 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.271279 2.606910 \tl:8.928741 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.276984 3.884314 \tl:9.222708 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.271245 5.156243 \tl:9.127083 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.211842 6.368860 \tl:9.119851 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.181351 7.550877 \tl:9.142066 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.220675 8.772051 \tl:9.366276 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.294216 10.06680 \tl:9.284879 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.289283 11.35697 \tl:8.523394 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.338636 12.69628 \tl:9.123686 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.589246 14.28620 \tl:9.147365 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.488938 15.77598 \tl:9.612261 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.508147 17.30040 \tl:9.029409 \tem:0.000000\n",
      "\n",
      "Epoch performance:  17.836sec Trl:9.147938 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  17 / 300\n",
      "Batch:\t0 /13\t:  1.303547 1.303568 \tl:9.005051 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.208113 2.513028 \tl:9.529240 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.304303 3.818500 \tl:9.559334 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.258941 5.077972 \tl:9.506984 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.271967 6.350226 \tl:8.840162 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.309722 7.660404 \tl:9.184137 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.199200 8.860247 \tl:9.314137 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.362951 10.22433 \tl:8.888811 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.279495 11.52254 \tl:8.907751 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.240943 12.76515 \tl:9.172248 \tem:0.022222\n",
      "Batch:\t10 /13\t:  1.229346 13.99541 \tl:9.502884 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.311728 15.30768 \tl:9.178699 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.175762 16.48390 \tl:9.092152 \tem:0.000000\n",
      "\n",
      "Epoch performance:  17.019sec Trl:9.206276 \tTrem:0.003419 \tTeem:0.000000\n",
      "\n",
      "Epoch:  18 / 300\n",
      "Batch:\t0 /13\t:  1.206274 1.206293 \tl:9.253672 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.254224 2.461279 \tl:9.076286 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.194972 3.656728 \tl:9.588442 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.224543 4.881850 \tl:9.325075 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.252901 6.135383 \tl:8.948755 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.271584 7.407646 \tl:9.489116 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.293138 8.701467 \tl:9.402541 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.378723 10.08113 \tl:9.080732 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.206112 11.28744 \tl:8.856001 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.218811 12.50697 \tl:8.892442 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.219077 13.72668 \tl:9.027042 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.157207 14.88515 \tl:9.036777 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.215080 16.10081 \tl:9.185188 \tem:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch performance:  16.634sec Trl:9.166313 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  19 / 300\n",
      "Batch:\t0 /13\t:  1.191817 1.191838 \tl:8.954819 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.218003 2.410455 \tl:8.967849 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.201412 3.612701 \tl:8.913912 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.200503 4.813416 \tl:9.103481 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.257920 6.071951 \tl:8.912699 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.266173 7.338605 \tl:8.579823 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.255811 8.595403 \tl:9.315403 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.328448 9.924140 \tl:8.925169 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.238105 11.16472 \tl:9.704403 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.325431 12.49043 \tl:9.150646 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.340656 13.83174 \tl:8.806993 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.252622 15.08485 \tl:8.976320 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.240444 16.32616 \tl:9.013291 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.861sec Trl:9.024985 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  20 / 300\n",
      "Batch:\t0 /13\t:  1.199485 1.199501 \tl:9.031777 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.183757 2.383698 \tl:8.972687 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.235656 3.620275 \tl:9.238770 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.207566 4.828058 \tl:9.134686 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.269152 6.097902 \tl:8.896807 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.198498 7.296886 \tl:9.442226 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.246042 8.544339 \tl:9.040560 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.227941 9.772727 \tl:8.694909 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.279186 11.05311 \tl:9.048506 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.201044 12.25444 \tl:8.889326 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.236948 13.49184 \tl:9.180933 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.260802 14.75386 \tl:9.188101 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.228126 15.98288 \tl:9.005573 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.514sec Trl:9.058835 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  21 / 300\n",
      "Batch:\t0 /13\t:  1.212306 1.212320 \tl:9.039936 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.230489 2.443359 \tl:9.373665 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.322917 3.767422 \tl:9.117695 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.195520 4.963137 \tl:8.716351 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.220371 6.184245 \tl:8.942555 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.269824 7.454790 \tl:9.382999 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.397073 8.852708 \tl:9.212931 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.397626 10.25112 \tl:8.620899 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.217303 11.46889 \tl:9.006226 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.262083 12.73177 \tl:8.910135 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.197161 13.92913 \tl:8.722471 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.277709 15.20750 \tl:8.888388 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.214118 16.42262 \tl:9.388374 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.959sec Trl:9.024817 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  22 / 300\n",
      "Batch:\t0 /13\t:  1.196418 1.196433 \tl:8.852482 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.194386 2.393335 \tl:8.571863 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.311254 3.704816 \tl:9.013641 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.258702 4.964671 \tl:9.124222 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.169903 6.135863 \tl:9.036419 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.231869 7.368246 \tl:9.064489 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.190875 8.559580 \tl:8.866963 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.206556 9.767094 \tl:9.312361 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.207962 10.97557 \tl:8.977670 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.274080 12.24994 \tl:8.634614 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.183987 13.43436 \tl:8.598758 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.279220 14.71424 \tl:8.990343 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.254386 15.96958 \tl:8.871187 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.508sec Trl:8.916539 \tTrem:0.001709 \tTeem:0.000000\n",
      "\n",
      "Epoch:  23 / 300\n",
      "Batch:\t0 /13\t:  1.343926 1.343945 \tl:8.899601 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.260787 2.605026 \tl:9.082879 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.205222 3.811124 \tl:8.580325 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.219440 5.032178 \tl:9.439228 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.179110 6.212475 \tl:8.871908 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.203174 7.416147 \tl:8.628862 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.277124 8.694222 \tl:8.753891 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.320659 10.01535 \tl:9.023378 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.192260 11.20814 \tl:9.022621 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.245617 12.45398 \tl:8.688148 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.260075 13.71456 \tl:8.842295 \tem:0.022222\n",
      "Batch:\t11 /13\t:  1.215811 14.93106 \tl:9.151600 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.213322 16.14482 \tl:9.033411 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.680sec Trl:8.924473 \tTrem:0.003419 \tTeem:0.000000\n",
      "\n",
      "Epoch:  24 / 300\n",
      "Batch:\t0 /13\t:  1.180593 1.180608 \tl:8.425240 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.217859 2.398941 \tl:9.153856 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.142073 3.541607 \tl:8.969608 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.206369 4.748432 \tl:8.437906 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.172009 5.920957 \tl:8.826463 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.187307 7.108678 \tl:8.942003 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.182036 8.291489 \tl:8.326485 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.151252 9.444064 \tl:8.548790 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.251136 10.69593 \tl:8.287395 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.198102 11.89449 \tl:8.452100 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.223682 13.11874 \tl:8.695498 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.175538 14.29492 \tl:8.610889 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.231993 15.52750 \tl:9.078392 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.063sec Trl:8.673433 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  25 / 300\n",
      "Batch:\t0 /13\t:  1.276757 1.276772 \tl:8.752816 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.224262 2.501610 \tl:8.556479 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.255352 3.757626 \tl:8.930883 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.235012 4.993768 \tl:8.619383 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.386306 6.380620 \tl:8.739574 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.181370 7.562467 \tl:9.186625 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.216331 8.779296 \tl:8.705978 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.351494 10.13135 \tl:9.123389 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.397809 11.52996 \tl:8.123573 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.110620 12.64140 \tl:8.509490 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.314163 13.95671 \tl:9.146784 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.290488 15.24912 \tl:8.730728 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.185876 16.43555 \tl:8.303419 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.973sec Trl:8.725317 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  26 / 300\n",
      "Batch:\t0 /13\t:  1.294549 1.294584 \tl:8.872883 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.351982 2.647763 \tl:9.481662 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.604595 4.255040 \tl:8.281748 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.505481 5.760963 \tl:8.385891 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.268113 7.029654 \tl:8.945137 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.289932 8.321905 \tl:8.997074 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.247254 9.569772 \tl:8.555321 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.201270 10.77180 \tl:8.531797 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.201740 11.97398 \tl:8.491299 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.149187 13.12358 \tl:8.876071 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.170604 14.29473 \tl:9.074696 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.198761 15.49439 \tl:8.890577 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.228443 16.72353 \tl:9.015052 \tem:0.000000\n",
      "\n",
      "Epoch performance:  17.257sec Trl:8.799939 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  27 / 300\n",
      "Batch:\t0 /13\t:  1.326996 1.327016 \tl:8.888168 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.240086 2.567631 \tl:8.733479 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.332490 3.900390 \tl:8.753208 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.248450 5.150002 \tl:8.727417 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.190105 6.340757 \tl:8.975716 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.268270 7.609611 \tl:8.628640 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.282640 8.892826 \tl:8.513092 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.213593 10.10746 \tl:8.459350 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.315021 11.42309 \tl:8.545984 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.168955 12.59270 \tl:8.536831 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.209095 13.80199 \tl:8.782397 \tem:0.022222\n",
      "Batch:\t11 /13\t:  1.218420 15.02091 \tl:8.192596 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.208642 16.23003 \tl:8.751322 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.769sec Trl:8.652939 \tTrem:0.003419 \tTeem:0.000000\n",
      "\n",
      "Epoch:  28 / 300\n",
      "Batch:\t0 /13\t:  1.302283 1.302303 \tl:8.411612 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.261398 2.563961 \tl:8.343856 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.206902 3.771857 \tl:8.636714 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.343363 5.117237 \tl:8.465893 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.137461 6.255197 \tl:8.570679 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.275178 7.531151 \tl:8.945465 \tem:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t6 /13\t:  1.278530 8.810286 \tl:8.814169 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.675161 10.48594 \tl:8.302948 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.498485 11.98517 \tl:8.300072 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.343045 13.32871 \tl:8.577065 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.268935 14.59860 \tl:8.194869 \tem:0.022222\n",
      "Batch:\t11 /13\t:  1.213076 15.81195 \tl:8.561716 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.338537 17.15098 \tl:8.463583 \tem:0.000000\n",
      "\n",
      "Epoch performance:  17.682sec Trl:8.506818 \tTrem:0.001709 \tTeem:0.000000\n",
      "\n",
      "Epoch:  29 / 300\n",
      "Batch:\t0 /13\t:  1.334999 1.335021 \tl:8.615215 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.348253 2.683615 \tl:8.661180 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.283866 3.967972 \tl:8.382283 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.182403 5.151139 \tl:8.780975 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.220479 6.371823 \tl:8.249283 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.167196 7.540329 \tl:8.417465 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.176419 8.718096 \tl:8.695539 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.236386 9.954911 \tl:8.904514 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.266767 11.22249 \tl:8.317862 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.266767 12.48982 \tl:8.337731 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.130074 13.62044 \tl:8.271240 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.199572 14.82049 \tl:8.046055 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.200723 16.02150 \tl:8.215097 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.560sec Trl:8.453419 \tTrem:0.001709 \tTeem:0.000000\n",
      "\n",
      "Epoch:  30 / 300\n",
      "Batch:\t0 /13\t:  1.210124 1.210150 \tl:8.471633 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.239828 2.452533 \tl:7.924708 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.193335 3.646909 \tl:7.972855 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.273710 4.935868 \tl:8.125196 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.300380 6.237015 \tl:7.875460 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.215548 7.453173 \tl:8.516905 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.200533 8.654594 \tl:8.954203 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.365134 10.02065 \tl:8.375786 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.334007 11.35520 \tl:8.729168 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.263849 12.61957 \tl:8.871173 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.183234 13.80416 \tl:8.493835 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.273612 15.07877 \tl:7.758430 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.121443 16.20102 \tl:8.166790 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.735sec Trl:8.325857 \tTrem:0.001709 \tTeem:0.000000\n",
      "\n",
      "Epoch:  31 / 300\n",
      "Batch:\t0 /13\t:  1.167768 1.167788 \tl:8.290895 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.185173 2.353251 \tl:9.228396 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.175003 3.528758 \tl:8.505827 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.176923 4.706574 \tl:8.622837 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.198096 5.905525 \tl:8.910270 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.205223 7.110949 \tl:8.776941 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.199186 8.311314 \tl:8.579092 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.341562 9.653769 \tl:8.573021 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.142663 10.79720 \tl:8.458212 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.311612 12.10929 \tl:8.516915 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.148192 13.25804 \tl:8.675684 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.228281 14.48743 \tl:8.575176 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.349517 15.83737 \tl:8.041924 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.373sec Trl:8.596553 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  32 / 300\n",
      "Batch:\t0 /13\t:  1.153918 1.153938 \tl:8.559215 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.291630 2.446778 \tl:8.807372 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.190377 3.637522 \tl:7.933272 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.223644 4.862209 \tl:8.514511 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.208527 6.071728 \tl:8.307297 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.547072 7.619911 \tl:8.106140 \tem:0.066667\n",
      "Batch:\t6 /13\t:  1.298851 8.919665 \tl:8.295706 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.229367 10.14959 \tl:8.223936 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.244308 11.39473 \tl:8.827527 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.195333 12.59113 \tl:8.028180 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.179888 13.77198 \tl:8.810190 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.313704 15.08620 \tl:8.677314 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.334708 16.42171 \tl:8.783210 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.958sec Trl:8.451836 \tTrem:0.005128 \tTeem:0.000000\n",
      "\n",
      "Epoch:  33 / 300\n",
      "Batch:\t0 /13\t:  1.330647 1.330664 \tl:8.088719 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.191877 2.522807 \tl:7.980292 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.200948 3.724256 \tl:8.398115 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.186105 4.911178 \tl:7.817904 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.226307 6.138245 \tl:8.586231 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.304551 7.443531 \tl:8.391344 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.303881 8.748298 \tl:8.856771 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.200505 9.949224 \tl:8.184082 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.231992 11.18213 \tl:8.221627 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.296519 12.47953 \tl:8.545789 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.181453 13.66148 \tl:7.857780 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.217397 14.87911 \tl:8.048580 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.267368 16.14734 \tl:8.454550 \tem:0.022222\n",
      "\n",
      "Epoch performance:  16.679sec Trl:8.263983 \tTrem:0.001709 \tTeem:0.000000\n",
      "\n",
      "Epoch:  34 / 300\n",
      "Batch:\t0 /13\t:  1.191133 1.191153 \tl:7.768799 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.216677 2.408528 \tl:7.815019 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.220355 3.629182 \tl:8.250979 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.290068 4.919801 \tl:7.484645 \tem:0.066667\n",
      "Batch:\t4 /13\t:  1.163179 6.083918 \tl:8.885254 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.219962 7.304327 \tl:7.912739 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.201116 8.506793 \tl:7.879612 \tem:0.022222\n",
      "Batch:\t7 /13\t:  1.257858 9.765140 \tl:7.865361 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.167428 10.93307 \tl:8.602383 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.176795 12.11008 \tl:7.999548 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.169740 13.28076 \tl:8.533072 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.337198 14.61882 \tl:8.394985 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.250550 15.87000 \tl:8.660488 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.409sec Trl:8.157914 \tTrem:0.008547 \tTeem:0.000000\n",
      "\n",
      "Epoch:  35 / 300\n",
      "Batch:\t0 /13\t:  1.343910 1.343928 \tl:8.350033 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.207506 2.551660 \tl:8.414207 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.289304 3.841585 \tl:8.281324 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.251090 5.094016 \tl:7.919201 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.271651 6.366875 \tl:8.513466 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.212319 7.579890 \tl:7.703610 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.145081 8.733371 \tl:7.836852 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.217607 9.951880 \tl:8.013437 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.186491 11.13908 \tl:8.047680 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.194545 12.33393 \tl:8.308525 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.247519 13.58173 \tl:8.368629 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.204964 14.78699 \tl:8.142673 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.217526 16.01213 \tl:8.115026 \tem:0.022222\n",
      "\n",
      "Epoch performance:  16.544sec Trl:8.154974 \tTrem:0.003419 \tTeem:0.000000\n",
      "\n",
      "Epoch:  36 / 300\n",
      "Batch:\t0 /13\t:  1.213442 1.213463 \tl:8.623519 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.331570 2.545642 \tl:8.294036 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.329427 3.875835 \tl:8.202866 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.235512 5.112035 \tl:8.020631 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.324542 6.437367 \tl:8.038687 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.240612 7.678508 \tl:7.818054 \tem:0.022222\n",
      "Batch:\t6 /13\t:  1.208929 8.888017 \tl:8.801531 \tem:0.022222\n",
      "Batch:\t7 /13\t:  1.189267 10.07823 \tl:8.287732 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.168210 11.24735 \tl:8.597201 \tem:0.022222\n",
      "Batch:\t9 /13\t:  1.182745 12.43037 \tl:8.386820 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.249538 13.68069 \tl:7.858961 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.298618 14.97990 \tl:7.866929 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.230338 16.21076 \tl:8.392172 \tem:0.044444\n",
      "\n",
      "Epoch performance:  16.749sec Trl:8.245318 \tTrem:0.011966 \tTeem:0.000000\n",
      "\n",
      "Epoch:  37 / 300\n",
      "Batch:\t0 /13\t:  1.217200 1.217216 \tl:8.461790 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.212435 2.429921 \tl:8.271684 \tem:0.022222\n",
      "Batch:\t2 /13\t:  1.166514 3.617584 \tl:8.147572 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.319293 4.937678 \tl:8.318580 \tem:0.022222\n",
      "Batch:\t4 /13\t:  1.149949 6.088069 \tl:8.118242 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.190279 7.279218 \tl:8.239823 \tem:0.022222\n",
      "Batch:\t6 /13\t:  1.256639 8.536600 \tl:8.154295 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.312396 9.850060 \tl:7.855173 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.322511 11.17394 \tl:8.050277 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.162815 12.33730 \tl:8.112083 \tem:0.022222\n",
      "Batch:\t10 /13\t:  1.193704 13.53147 \tl:8.270039 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.190807 14.72356 \tl:8.415090 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.188303 15.91233 \tl:8.540968 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.447sec Trl:8.227355 \tTrem:0.011966 \tTeem:0.000000\n",
      "\n",
      "Epoch:  38 / 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t0 /13\t:  1.208418 1.208437 \tl:8.189112 \tem:0.044444\n",
      "Batch:\t1 /13\t:  1.187083 2.396034 \tl:7.846777 \tem:0.022222\n",
      "Batch:\t2 /13\t:  1.186818 3.585213 \tl:8.384945 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.244778 4.830187 \tl:8.212650 \tem:0.022222\n",
      "Batch:\t4 /13\t:  1.173564 6.006362 \tl:9.082903 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.262842 7.269747 \tl:7.940022 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.267364 8.538008 \tl:8.137354 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.587845 10.12637 \tl:8.213108 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.364126 11.49139 \tl:7.382912 \tem:0.044444\n",
      "Batch:\t9 /13\t:  1.301906 12.79398 \tl:8.252935 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.268820 14.06402 \tl:7.739686 \tem:0.022222\n",
      "Batch:\t11 /13\t:  1.221064 15.28605 \tl:8.968601 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.265130 16.55171 \tl:8.375381 \tem:0.022222\n",
      "\n",
      "Epoch performance:  17.088sec Trl:8.209722 \tTrem:0.022222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  39 / 300\n",
      "Batch:\t0 /13\t:  1.184944 1.184961 \tl:7.846654 \tem:0.044444\n",
      "Batch:\t1 /13\t:  1.217423 2.403501 \tl:8.080296 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.297735 3.701610 \tl:7.979401 \tem:0.066667\n",
      "Batch:\t3 /13\t:  1.253916 4.955944 \tl:7.809287 \tem:0.022222\n",
      "Batch:\t4 /13\t:  1.339498 6.296463 \tl:7.862908 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.175411 7.472503 \tl:7.816832 \tem:0.022222\n",
      "Batch:\t6 /13\t:  1.175296 8.648702 \tl:7.485662 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.304570 9.954026 \tl:8.401226 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.266408 11.22144 \tl:8.522997 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.266515 12.48852 \tl:7.884951 \tem:0.044444\n",
      "Batch:\t10 /13\t:  1.204850 13.69359 \tl:8.280998 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.225134 14.91902 \tl:7.297113 \tem:0.044444\n",
      "Batch:\t12 /13\t:  1.252977 16.17243 \tl:7.711696 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.711sec Trl:7.921540 \tTrem:0.023932 \tTeem:0.000000\n",
      "\n",
      "Epoch:  40 / 300\n",
      "Batch:\t0 /13\t:  1.331896 1.331913 \tl:7.616091 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.219937 2.552376 \tl:8.196748 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.166299 3.719548 \tl:7.820551 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.319844 5.040241 \tl:7.706018 \tem:0.022222\n",
      "Batch:\t4 /13\t:  1.252380 6.293262 \tl:7.054142 \tem:0.044444\n",
      "Batch:\t5 /13\t:  1.228285 7.521970 \tl:8.041332 \tem:0.044444\n",
      "Batch:\t6 /13\t:  1.188848 8.711513 \tl:7.629307 \tem:0.022222\n",
      "Batch:\t7 /13\t:  1.277336 9.989458 \tl:8.159110 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.217476 11.20778 \tl:8.008317 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.212201 12.42088 \tl:7.926672 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.222973 13.64457 \tl:7.576621 \tem:0.066667\n",
      "Batch:\t11 /13\t:  1.174151 14.81899 \tl:7.836101 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.236846 16.05712 \tl:8.513645 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.590sec Trl:7.852666 \tTrem:0.018803 \tTeem:0.000000\n",
      "\n",
      "Epoch:  41 / 300\n",
      "Batch:\t0 /13\t:  1.164880 1.164894 \tl:7.969744 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.250354 2.415973 \tl:8.039244 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.106383 3.522848 \tl:8.471632 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.247111 4.770428 \tl:7.751754 \tem:0.022222\n",
      "Batch:\t4 /13\t:  1.202282 5.973557 \tl:8.213772 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.182631 7.156385 \tl:7.991119 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.185743 8.342530 \tl:8.103681 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.283961 9.627137 \tl:7.373272 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.401867 11.03044 \tl:7.559899 \tem:0.022222\n",
      "Batch:\t9 /13\t:  1.636204 12.66751 \tl:8.100901 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.382414 14.05040 \tl:7.751249 \tem:0.022222\n",
      "Batch:\t11 /13\t:  1.301300 15.35221 \tl:7.814229 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.244987 16.59764 \tl:8.157984 \tem:0.000000\n",
      "\n",
      "Epoch performance:  17.134sec Trl:7.946037 \tTrem:0.008547 \tTeem:0.000000\n",
      "\n",
      "Epoch:  42 / 300\n",
      "Batch:\t0 /13\t:  1.352366 1.352385 \tl:7.477145 \tem:0.044444\n",
      "Batch:\t1 /13\t:  1.615470 2.968116 \tl:7.776153 \tem:0.044444\n",
      "Batch:\t2 /13\t:  1.496416 4.465419 \tl:7.535265 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.174897 5.640822 \tl:7.496595 \tem:0.044444\n",
      "Batch:\t4 /13\t:  1.195694 6.837779 \tl:6.887394 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.248793 8.087007 \tl:7.289301 \tem:0.022222\n",
      "Batch:\t6 /13\t:  1.203688 9.291646 \tl:7.857049 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.280684 10.57321 \tl:7.964096 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.197698 11.77187 \tl:7.704589 \tem:0.022222\n",
      "Batch:\t9 /13\t:  1.303229 13.07554 \tl:8.349669 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.209043 14.28559 \tl:7.376174 \tem:0.066667\n",
      "Batch:\t11 /13\t:  1.172665 15.45914 \tl:8.004293 \tem:0.044444\n",
      "Batch:\t12 /13\t:  1.188992 16.64875 \tl:8.590515 \tem:0.022222\n",
      "\n",
      "Epoch performance:  17.181sec Trl:7.716018 \tTrem:0.032479 \tTeem:0.000000\n",
      "\n",
      "Epoch:  43 / 300\n",
      "Batch:\t0 /13\t:  1.258791 1.258810 \tl:7.490868 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.166502 2.426269 \tl:6.927629 \tem:0.022222\n",
      "Batch:\t2 /13\t:  1.215990 3.643369 \tl:8.262190 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.194061 4.838051 \tl:8.050982 \tem:0.044444\n",
      "Batch:\t4 /13\t:  1.279540 6.118806 \tl:8.448709 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.233985 7.355692 \tl:7.919102 \tem:0.022222\n",
      "Batch:\t6 /13\t:  1.238553 8.594784 \tl:7.461196 \tem:0.088889\n",
      "Batch:\t7 /13\t:  1.203399 9.798891 \tl:7.385002 \tem:0.066667\n",
      "Batch:\t8 /13\t:  1.267279 11.06679 \tl:7.335268 \tem:0.022222\n",
      "Batch:\t9 /13\t:  1.178205 12.24555 \tl:6.926106 \tem:0.066667\n",
      "Batch:\t10 /13\t:  1.261307 13.50765 \tl:7.592084 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.204539 14.71331 \tl:8.077385 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.342396 16.05632 \tl:8.739880 \tem:0.022222\n",
      "\n",
      "Epoch performance:  16.596sec Trl:7.739723 \tTrem:0.030769 \tTeem:0.000000\n",
      "\n",
      "Epoch:  44 / 300\n",
      "Batch:\t0 /13\t:  1.168146 1.168163 \tl:7.787086 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.224401 2.393096 \tl:8.155975 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.636685 4.030409 \tl:7.907960 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.374167 5.405309 \tl:7.690233 \tem:0.044444\n",
      "Batch:\t4 /13\t:  1.313745 6.719923 \tl:8.324516 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.214746 7.935497 \tl:7.356205 \tem:0.044444\n",
      "Batch:\t6 /13\t:  1.291854 9.228725 \tl:7.605189 \tem:0.066667\n",
      "Batch:\t7 /13\t:  1.284842 10.51399 \tl:7.392550 \tem:0.111111\n",
      "Batch:\t8 /13\t:  1.261807 11.77707 \tl:7.312113 \tem:0.044444\n",
      "Batch:\t9 /13\t:  1.199206 12.97678 \tl:7.644146 \tem:0.022222\n",
      "Batch:\t10 /13\t:  1.202303 14.17992 \tl:7.375071 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.321436 15.50187 \tl:7.438284 \tem:0.066667\n",
      "Batch:\t12 /13\t:  1.230906 16.73476 \tl:7.340870 \tem:0.022222\n",
      "\n",
      "Epoch performance:  17.268sec Trl:7.640785 \tTrem:0.039316 \tTeem:0.000000\n",
      "\n",
      "Epoch:  45 / 300\n",
      "Batch:\t0 /13\t:  1.227603 1.227619 \tl:8.090642 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.182893 2.410864 \tl:7.734607 \tem:0.022222\n",
      "Batch:\t2 /13\t:  1.215852 3.627311 \tl:8.021862 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.230379 4.858170 \tl:7.658856 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.181063 6.040120 \tl:8.045045 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.212146 7.253129 \tl:7.252237 \tem:0.044444\n",
      "Batch:\t6 /13\t:  1.120595 8.374737 \tl:8.232165 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.202790 9.578346 \tl:7.851608 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.215492 10.79404 \tl:7.408338 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.266900 12.06123 \tl:8.377495 \tem:0.022222\n",
      "Batch:\t10 /13\t:  1.373095 13.43482 \tl:8.062363 \tem:0.022222\n",
      "Batch:\t11 /13\t:  1.246636 14.68166 \tl:7.369726 \tem:0.000000\n",
      "Batch:\t12 /13\t:  1.295140 15.97709 \tl:7.984106 \tem:0.022222\n",
      "\n",
      "Epoch performance:  16.512sec Trl:7.853004 \tTrem:0.013675 \tTeem:0.000000\n",
      "\n",
      "Epoch:  46 / 300\n",
      "Batch:\t0 /13\t:  1.148001 1.148017 \tl:7.732158 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.282387 2.430922 \tl:7.620093 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.223442 3.655180 \tl:7.755550 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.209391 4.864987 \tl:6.875086 \tem:0.044444\n",
      "Batch:\t4 /13\t:  1.202626 6.068269 \tl:7.928446 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.228662 7.299623 \tl:8.274668 \tem:0.022222\n",
      "Batch:\t6 /13\t:  1.171093 8.471781 \tl:8.552507 \tem:0.022222\n",
      "Batch:\t7 /13\t:  1.172585 9.644819 \tl:7.946558 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.214375 10.85976 \tl:8.114997 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.241121 12.10146 \tl:7.791122 \tem:0.022222\n",
      "Batch:\t10 /13\t:  1.221709 13.32361 \tl:7.772381 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.232245 14.55704 \tl:7.661661 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.251517 15.80882 \tl:7.293249 \tem:0.044444\n",
      "\n",
      "Epoch performance:  16.347sec Trl:7.793729 \tTrem:0.020513 \tTeem:0.000000\n",
      "\n",
      "Epoch:  47 / 300\n",
      "Batch:\t0 /13\t:  1.313158 1.313174 \tl:7.286972 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.199466 2.513427 \tl:8.386621 \tem:0.022222\n",
      "Batch:\t2 /13\t:  1.212069 3.725986 \tl:8.022131 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.303755 5.030416 \tl:8.330561 \tem:0.022222\n",
      "Batch:\t4 /13\t:  1.110242 6.141530 \tl:8.253338 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.248792 7.391324 \tl:7.857268 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.232453 8.624726 \tl:7.623429 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.382695 10.00765 \tl:7.846476 \tem:0.066667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t8 /13\t:  1.191896 11.19995 \tl:7.661027 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.261132 12.46136 \tl:7.414926 \tem:0.066667\n",
      "Batch:\t10 /13\t:  1.231427 13.69401 \tl:7.735720 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.206017 14.90061 \tl:7.761703 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.242202 16.14340 \tl:7.844460 \tem:0.044444\n",
      "\n",
      "Epoch performance:  16.677sec Trl:7.848049 \tTrem:0.022222 \tTeem:0.000000\n",
      "\n",
      "Epoch:  48 / 300\n",
      "Batch:\t0 /13\t:  1.303324 1.303343 \tl:7.683990 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.305784 2.609755 \tl:7.365055 \tem:0.088889\n",
      "Batch:\t2 /13\t:  1.166458 3.776457 \tl:7.263144 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.353377 5.130269 \tl:7.271157 \tem:0.044444\n",
      "Batch:\t4 /13\t:  1.226620 6.357568 \tl:8.305884 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.162201 7.520509 \tl:7.489197 \tem:0.022222\n",
      "Batch:\t6 /13\t:  1.186228 8.707853 \tl:6.982262 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.261159 9.969650 \tl:7.689156 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.327145 11.29728 \tl:7.981586 \tem:0.022222\n",
      "Batch:\t9 /13\t:  1.636858 12.93541 \tl:7.011295 \tem:0.044444\n",
      "Batch:\t10 /13\t:  1.481572 14.41812 \tl:7.561069 \tem:0.022222\n",
      "Batch:\t11 /13\t:  1.572377 15.99074 \tl:7.561733 \tem:0.111111\n",
      "Batch:\t12 /13\t:  1.220494 17.21218 \tl:7.214257 \tem:0.066667\n",
      "\n",
      "Epoch performance:  17.749sec Trl:7.490753 \tTrem:0.039316 \tTeem:0.000000\n",
      "\n",
      "Epoch:  49 / 300\n",
      "Batch:\t0 /13\t:  1.268998 1.269012 \tl:7.520702 \tem:0.022222\n",
      "Batch:\t1 /13\t:  1.134773 2.404058 \tl:8.041538 \tem:0.022222\n",
      "Batch:\t2 /13\t:  1.197081 3.601733 \tl:7.971166 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.187589 4.790237 \tl:7.070767 \tem:0.066667\n",
      "Batch:\t4 /13\t:  1.220805 6.011949 \tl:7.312543 \tem:0.066667\n",
      "Batch:\t5 /13\t:  1.205343 7.217782 \tl:7.546911 \tem:0.066667\n",
      "Batch:\t6 /13\t:  1.197274 8.416233 \tl:7.258719 \tem:0.066667\n",
      "Batch:\t7 /13\t:  1.253358 9.670222 \tl:7.736838 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.268220 10.93896 \tl:7.145426 \tem:0.111111\n",
      "Batch:\t9 /13\t:  1.324357 12.26353 \tl:7.239957 \tem:0.088889\n",
      "Batch:\t10 /13\t:  1.243999 13.50862 \tl:7.616248 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.152047 14.66120 \tl:6.888960 \tem:0.044444\n",
      "Batch:\t12 /13\t:  1.174404 15.83608 \tl:7.229032 \tem:0.044444\n",
      "\n",
      "Epoch performance:  16.368sec Trl:7.429139 \tTrem:0.051282 \tTeem:0.000000\n",
      "\n",
      "Epoch:  50 / 300\n",
      "Batch:\t0 /13\t:  1.285559 1.285574 \tl:7.478715 \tem:0.066667\n",
      "Batch:\t1 /13\t:  1.228488 2.515383 \tl:8.061477 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.250643 3.767853 \tl:7.766538 \tem:0.000000\n",
      "Batch:\t3 /13\t:  1.228610 4.997099 \tl:7.882324 \tem:0.000000\n",
      "Batch:\t4 /13\t:  1.283915 6.281569 \tl:7.740876 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.653234 7.935105 \tl:7.820805 \tem:0.022222\n",
      "Batch:\t6 /13\t:  1.280785 9.216552 \tl:7.461338 \tem:0.066667\n",
      "Batch:\t7 /13\t:  1.238780 10.45692 \tl:7.840147 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.195398 11.65285 \tl:7.248730 \tem:0.022222\n",
      "Batch:\t9 /13\t:  1.266507 12.92005 \tl:7.907730 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.162087 14.08261 \tl:7.302681 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.203789 15.28703 \tl:7.175440 \tem:0.066667\n",
      "Batch:\t12 /13\t:  1.281306 16.56894 \tl:7.333811 \tem:0.044444\n",
      "\n",
      "Epoch performance:  17.108sec Trl:7.616970 \tTrem:0.029060 \tTeem:0.000000\n",
      "\n",
      "Epoch:  51 / 300\n",
      "Batch:\t0 /13\t:  1.392714 1.392733 \tl:7.547543 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.180381 2.574033 \tl:7.584400 \tem:0.022222\n",
      "Batch:\t2 /13\t:  1.218123 3.793390 \tl:7.478811 \tem:0.066667\n",
      "Batch:\t3 /13\t:  1.287514 5.081702 \tl:7.519014 \tem:0.044444\n",
      "Batch:\t4 /13\t:  1.303689 6.385962 \tl:6.997826 \tem:0.066667\n",
      "Batch:\t5 /13\t:  1.228116 7.615955 \tl:7.961312 \tem:0.066667\n",
      "Batch:\t6 /13\t:  1.213124 8.830045 \tl:7.408107 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.212618 10.04312 \tl:6.986599 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.254222 11.29829 \tl:6.632335 \tem:0.044444\n",
      "Batch:\t9 /13\t:  1.335551 12.63411 \tl:7.439452 \tem:0.066667\n",
      "Batch:\t10 /13\t:  1.156607 13.79100 \tl:7.446473 \tem:0.022222\n",
      "Batch:\t11 /13\t:  1.209749 15.00130 \tl:8.326048 \tem:0.044444\n",
      "Batch:\t12 /13\t:  1.179762 16.18189 \tl:7.434845 \tem:0.000000\n",
      "\n",
      "Epoch performance:  16.718sec Trl:7.443290 \tTrem:0.039316 \tTeem:0.000000\n",
      "\n",
      "Epoch:  52 / 300\n",
      "Batch:\t0 /13\t:  1.348606 1.348623 \tl:7.931543 \tem:0.044444\n",
      "Batch:\t1 /13\t:  1.211308 2.560562 \tl:7.561633 \tem:0.088889\n",
      "Batch:\t2 /13\t:  1.246459 3.807883 \tl:7.107083 \tem:0.111111\n",
      "Batch:\t3 /13\t:  1.280270 5.088430 \tl:7.352934 \tem:0.088889\n",
      "Batch:\t4 /13\t:  1.252386 6.341311 \tl:7.523805 \tem:0.044444\n",
      "Batch:\t5 /13\t:  1.203999 7.545831 \tl:8.084913 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.209045 8.755453 \tl:7.304647 \tem:0.066667\n",
      "Batch:\t7 /13\t:  1.222980 9.978698 \tl:7.611354 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.180084 11.15942 \tl:7.778950 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.271465 12.43190 \tl:7.302982 \tem:0.066667\n",
      "Batch:\t10 /13\t:  1.288505 13.72094 \tl:7.810361 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.145867 14.86796 \tl:7.919050 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.181466 16.04997 \tl:7.846892 \tem:0.066667\n",
      "\n",
      "Epoch performance:  16.587sec Trl:7.625858 \tTrem:0.047863 \tTeem:0.000000\n",
      "\n",
      "Epoch:  53 / 300\n",
      "Batch:\t0 /13\t:  1.248660 1.248679 \tl:7.174169 \tem:0.044444\n",
      "Batch:\t1 /13\t:  1.353244 2.602203 \tl:7.454983 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.211490 3.814382 \tl:6.610106 \tem:0.044444\n",
      "Batch:\t3 /13\t:  1.277972 5.092552 \tl:7.314984 \tem:0.022222\n",
      "Batch:\t4 /13\t:  1.574858 6.667865 \tl:7.200196 \tem:0.066667\n",
      "Batch:\t5 /13\t:  1.248622 7.916969 \tl:7.248176 \tem:0.088889\n",
      "Batch:\t6 /13\t:  1.338358 9.255885 \tl:7.319375 \tem:0.066667\n",
      "Batch:\t7 /13\t:  1.207638 10.46400 \tl:7.219071 \tem:0.000000\n",
      "Batch:\t8 /13\t:  1.322278 11.78687 \tl:7.577238 \tem:0.066667\n",
      "Batch:\t9 /13\t:  1.317123 13.10428 \tl:6.784483 \tem:0.044444\n",
      "Batch:\t10 /13\t:  1.204613 14.30986 \tl:6.958696 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.210935 15.52130 \tl:6.663303 \tem:0.066667\n",
      "Batch:\t12 /13\t:  1.311174 16.83293 \tl:7.116528 \tem:0.111111\n",
      "\n",
      "Epoch performance:  17.370sec Trl:7.126254 \tTrem:0.051282 \tTeem:0.000000\n",
      "\n",
      "Epoch:  54 / 300\n",
      "Batch:\t0 /13\t:  1.274009 1.274029 \tl:7.979632 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.118201 2.392917 \tl:6.633691 \tem:0.088889\n",
      "Batch:\t2 /13\t:  1.196789 3.589989 \tl:7.912675 \tem:0.066667\n",
      "Batch:\t3 /13\t:  1.175889 4.766470 \tl:7.102759 \tem:0.066667\n",
      "Batch:\t4 /13\t:  1.228704 5.995826 \tl:7.839653 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.178440 7.175091 \tl:7.021087 \tem:0.111111\n",
      "Batch:\t6 /13\t:  1.184024 8.360024 \tl:7.011774 \tem:0.022222\n",
      "Batch:\t7 /13\t:  1.287100 9.647629 \tl:7.643335 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.250396 10.89845 \tl:7.851575 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.188009 12.08731 \tl:7.638898 \tem:0.044444\n",
      "Batch:\t10 /13\t:  1.294577 13.38272 \tl:7.514105 \tem:0.022222\n",
      "Batch:\t11 /13\t:  1.336856 14.72015 \tl:7.301389 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.274131 15.99493 \tl:6.916036 \tem:0.044444\n",
      "\n",
      "Epoch performance:  16.530sec Trl:7.412816 \tTrem:0.039316 \tTeem:0.000000\n",
      "\n",
      "Epoch:  55 / 300\n",
      "Batch:\t0 /13\t:  1.203083 1.203100 \tl:6.741049 \tem:0.066667\n",
      "Batch:\t1 /13\t:  1.308888 2.512933 \tl:7.263451 \tem:0.044444\n",
      "Batch:\t2 /13\t:  1.220520 3.734174 \tl:7.099302 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.128314 4.863289 \tl:7.385352 \tem:0.022222\n",
      "Batch:\t4 /13\t:  1.140902 6.020104 \tl:6.996148 \tem:0.066667\n",
      "Batch:\t5 /13\t:  1.203516 7.224314 \tl:7.178869 \tem:0.044444\n",
      "Batch:\t6 /13\t:  1.201322 8.426174 \tl:7.328542 \tem:0.066667\n",
      "Batch:\t7 /13\t:  1.240049 9.666675 \tl:6.579504 \tem:0.066667\n",
      "Batch:\t8 /13\t:  1.208800 10.87613 \tl:7.487701 \tem:0.044444\n",
      "Batch:\t9 /13\t:  1.177105 12.05370 \tl:6.674433 \tem:0.155556\n",
      "Batch:\t10 /13\t:  1.173706 13.22788 \tl:6.889533 \tem:0.066667\n",
      "Batch:\t11 /13\t:  1.272958 14.50135 \tl:6.974749 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.295995 15.79865 \tl:7.600206 \tem:0.022222\n",
      "\n",
      "Epoch performance:  16.337sec Trl:7.092218 \tTrem:0.054701 \tTeem:0.000000\n",
      "\n",
      "Epoch:  56 / 300\n",
      "Batch:\t0 /13\t:  1.254024 1.254039 \tl:7.693287 \tem:0.000000\n",
      "Batch:\t1 /13\t:  1.205639 2.460571 \tl:6.327256 \tem:0.066667\n",
      "Batch:\t2 /13\t:  1.258800 3.719657 \tl:6.917004 \tem:0.066667\n",
      "Batch:\t3 /13\t:  1.376932 5.097214 \tl:7.382770 \tem:0.066667\n",
      "Batch:\t4 /13\t:  1.589874 6.687798 \tl:7.639197 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.483719 8.171963 \tl:7.638090 \tem:0.066667\n",
      "Batch:\t6 /13\t:  1.271130 9.444385 \tl:7.434360 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.213072 10.65767 \tl:7.447401 \tem:0.066667\n",
      "Batch:\t8 /13\t:  1.149883 11.80784 \tl:7.178882 \tem:0.066667\n",
      "Batch:\t9 /13\t:  1.396838 13.20549 \tl:8.216243 \tem:0.022222\n",
      "Batch:\t10 /13\t:  1.264055 14.47056 \tl:7.140077 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.203531 15.67430 \tl:7.307337 \tem:0.111111\n",
      "Batch:\t12 /13\t:  1.206127 16.88131 \tl:6.965568 \tem:0.022222\n",
      "\n",
      "Epoch performance:  17.412sec Trl:7.329805 \tTrem:0.051282 \tTeem:0.000000\n",
      "\n",
      "Epoch:  57 / 300\n",
      "Batch:\t0 /13\t:  1.297339 1.297359 \tl:7.619081 \tem:0.066667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t1 /13\t:  1.116771 2.414994 \tl:7.004052 \tem:0.022222\n",
      "Batch:\t2 /13\t:  1.194713 3.610310 \tl:7.095398 \tem:0.044444\n",
      "Batch:\t3 /13\t:  1.173743 4.784820 \tl:7.178915 \tem:0.044444\n",
      "Batch:\t4 /13\t:  1.237162 6.022219 \tl:7.735615 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.425894 7.448781 \tl:6.824162 \tem:0.066667\n",
      "Batch:\t6 /13\t:  1.187648 8.636726 \tl:7.195035 \tem:0.088889\n",
      "Batch:\t7 /13\t:  1.205189 9.842931 \tl:7.197079 \tem:0.044444\n",
      "Batch:\t8 /13\t:  1.229902 11.07312 \tl:7.488467 \tem:0.066667\n",
      "Batch:\t9 /13\t:  1.196325 12.27233 \tl:7.190402 \tem:0.066667\n",
      "Batch:\t10 /13\t:  1.255527 13.52882 \tl:6.514241 \tem:0.088889\n",
      "Batch:\t11 /13\t:  1.230661 14.76003 \tl:6.350024 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.197080 15.95757 \tl:6.341732 \tem:0.066667\n",
      "\n",
      "Epoch performance:  16.496sec Trl:7.056477 \tTrem:0.054701 \tTeem:0.000000\n",
      "\n",
      "Epoch:  58 / 300\n",
      "Batch:\t0 /13\t:  1.159960 1.159981 \tl:6.961891 \tem:0.133333\n",
      "Batch:\t1 /13\t:  1.243587 2.404264 \tl:7.294584 \tem:0.022222\n",
      "Batch:\t2 /13\t:  1.290464 3.695657 \tl:7.335271 \tem:0.044444\n",
      "Batch:\t3 /13\t:  1.180550 4.876960 \tl:6.884158 \tem:0.111111\n",
      "Batch:\t4 /13\t:  1.252496 6.129993 \tl:6.864993 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.180681 7.311492 \tl:6.794824 \tem:0.066667\n",
      "Batch:\t6 /13\t:  1.201987 8.514434 \tl:7.102730 \tem:0.022222\n",
      "Batch:\t7 /13\t:  1.176527 9.691551 \tl:7.119740 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.128209 10.82099 \tl:6.788233 \tem:0.044444\n",
      "Batch:\t9 /13\t:  1.150378 11.97226 \tl:6.769204 \tem:0.044444\n",
      "Batch:\t10 /13\t:  1.239588 13.21319 \tl:7.217845 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.215849 14.42955 \tl:7.337834 \tem:0.066667\n",
      "Batch:\t12 /13\t:  1.249387 15.67953 \tl:7.634430 \tem:0.044444\n",
      "\n",
      "Epoch performance:  16.214sec Trl:7.085057 \tTrem:0.049573 \tTeem:0.000000\n",
      "\n",
      "Epoch:  59 / 300\n",
      "Batch:\t0 /13\t:  1.513113 1.513132 \tl:7.101456 \tem:0.066667\n",
      "Batch:\t1 /13\t:  1.193071 2.706732 \tl:7.387298 \tem:0.044444\n",
      "Batch:\t2 /13\t:  1.286509 3.993757 \tl:7.328579 \tem:0.066667\n",
      "Batch:\t3 /13\t:  1.358018 5.352527 \tl:7.143173 \tem:0.022222\n",
      "Batch:\t4 /13\t:  1.272886 6.626466 \tl:6.512690 \tem:0.088889\n",
      "Batch:\t5 /13\t:  1.230075 7.857293 \tl:6.831465 \tem:0.044444\n",
      "Batch:\t6 /13\t:  1.146585 9.004140 \tl:7.042863 \tem:0.066667\n",
      "Batch:\t7 /13\t:  1.190363 10.19532 \tl:7.236632 \tem:0.066667\n",
      "Batch:\t8 /13\t:  1.231909 11.42835 \tl:7.175838 \tem:0.022222\n",
      "Batch:\t9 /13\t:  1.212698 12.64176 \tl:7.439063 \tem:0.066667\n",
      "Batch:\t10 /13\t:  1.218629 13.86111 \tl:7.225812 \tem:0.088889\n",
      "Batch:\t11 /13\t:  1.259618 15.12109 \tl:6.572639 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.173823 16.29509 \tl:6.608687 \tem:0.088889\n",
      "\n",
      "Epoch performance:  16.832sec Trl:7.046630 \tTrem:0.058120 \tTeem:0.000000\n",
      "\n",
      "Epoch:  60 / 300\n",
      "Batch:\t0 /13\t:  1.270895 1.270910 \tl:6.768002 \tem:0.088889\n",
      "Batch:\t1 /13\t:  1.171145 2.442548 \tl:7.088645 \tem:0.022222\n",
      "Batch:\t2 /13\t:  1.233923 3.676704 \tl:6.329494 \tem:0.044444\n",
      "Batch:\t3 /13\t:  1.215564 4.892568 \tl:7.436810 \tem:0.066667\n",
      "Batch:\t4 /13\t:  1.151897 6.045238 \tl:6.842579 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.195044 7.241333 \tl:7.347260 \tem:0.022222\n",
      "Batch:\t6 /13\t:  1.151098 8.392935 \tl:6.892970 \tem:0.022222\n",
      "Batch:\t7 /13\t:  1.176572 9.570060 \tl:6.223291 \tem:0.066667\n",
      "Batch:\t8 /13\t:  1.210760 10.78169 \tl:6.713534 \tem:0.044444\n",
      "Batch:\t9 /13\t:  1.272159 12.05441 \tl:6.896936 \tem:0.088889\n",
      "Batch:\t10 /13\t:  1.627076 13.68194 \tl:6.658698 \tem:0.066667\n",
      "Batch:\t11 /13\t:  1.494432 15.17732 \tl:6.425532 \tem:0.088889\n",
      "Batch:\t12 /13\t:  1.267481 16.44559 \tl:7.242504 \tem:0.066667\n",
      "\n",
      "Epoch performance:  16.982sec Trl:6.835866 \tTrem:0.054701 \tTeem:0.000000\n",
      "\n",
      "Epoch:  61 / 300\n",
      "Batch:\t0 /13\t:  1.558511 1.558526 \tl:6.335197 \tem:0.111111\n",
      "Batch:\t1 /13\t:  1.499881 3.058840 \tl:6.358891 \tem:0.044444\n",
      "Batch:\t2 /13\t:  1.528833 4.587886 \tl:7.020646 \tem:0.044444\n",
      "Batch:\t3 /13\t:  1.235373 5.823760 \tl:6.470371 \tem:0.066667\n",
      "Batch:\t4 /13\t:  1.188305 7.013142 \tl:6.233006 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.174757 8.188126 \tl:6.272835 \tem:0.133333\n",
      "Batch:\t6 /13\t:  1.184020 9.372734 \tl:7.770780 \tem:0.022222\n",
      "Batch:\t7 /13\t:  1.235929 10.60913 \tl:7.847962 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.157424 11.76785 \tl:7.386750 \tem:0.000000\n",
      "Batch:\t9 /13\t:  1.255025 13.02626 \tl:6.558749 \tem:0.044444\n",
      "Batch:\t10 /13\t:  1.160027 14.18667 \tl:8.133624 \tem:0.022222\n",
      "Batch:\t11 /13\t:  1.185095 15.37198 \tl:6.274346 \tem:0.088889\n",
      "Batch:\t12 /13\t:  1.263394 16.63597 \tl:7.702209 \tem:0.044444\n",
      "\n",
      "Epoch performance:  17.175sec Trl:6.951182 \tTrem:0.051282 \tTeem:0.000000\n",
      "\n",
      "Epoch:  62 / 300\n",
      "Batch:\t0 /13\t:  1.360672 1.360692 \tl:6.800164 \tem:0.066667\n",
      "Batch:\t1 /13\t:  1.538982 2.900223 \tl:6.608855 \tem:0.133333\n",
      "Batch:\t2 /13\t:  1.303910 4.204925 \tl:6.800117 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.147500 5.353479 \tl:7.022106 \tem:0.088889\n",
      "Batch:\t4 /13\t:  1.284941 6.639303 \tl:7.008486 \tem:0.044444\n",
      "Batch:\t5 /13\t:  1.244431 7.884476 \tl:7.198542 \tem:0.044444\n",
      "Batch:\t6 /13\t:  1.185022 9.070225 \tl:6.950448 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.218508 10.28892 \tl:7.533478 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.204432 11.49398 \tl:7.170494 \tem:0.044444\n",
      "Batch:\t9 /13\t:  1.252112 12.74649 \tl:6.540335 \tem:0.155556\n",
      "Batch:\t10 /13\t:  1.224699 13.97184 \tl:6.883648 \tem:0.088889\n",
      "Batch:\t11 /13\t:  1.143226 15.11553 \tl:6.169440 \tem:0.155556\n",
      "Batch:\t12 /13\t:  1.214189 16.33038 \tl:7.301886 \tem:0.022222\n",
      "\n",
      "Epoch performance:  16.870sec Trl:6.922154 \tTrem:0.071795 \tTeem:0.000000\n",
      "\n",
      "Epoch:  63 / 300\n",
      "Batch:\t0 /13\t:  1.303610 1.303627 \tl:7.246045 \tem:0.022222\n",
      "Batch:\t1 /13\t:  1.155686 2.460488 \tl:7.463489 \tem:0.066667\n",
      "Batch:\t2 /13\t:  1.160835 3.621926 \tl:7.650118 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.203383 4.825805 \tl:7.009658 \tem:0.066667\n",
      "Batch:\t4 /13\t:  1.128049 5.954775 \tl:6.846376 \tem:0.044444\n",
      "Batch:\t5 /13\t:  1.253010 7.207985 \tl:6.736547 \tem:0.088889\n",
      "Batch:\t6 /13\t:  1.167212 8.376415 \tl:7.339470 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.180312 9.557449 \tl:7.118960 \tem:0.111111\n",
      "Batch:\t8 /13\t:  1.170209 10.72829 \tl:6.904310 \tem:0.044444\n",
      "Batch:\t9 /13\t:  1.306791 12.03541 \tl:6.513399 \tem:0.088889\n",
      "Batch:\t10 /13\t:  1.287459 13.32318 \tl:6.760990 \tem:0.066667\n",
      "Batch:\t11 /13\t:  1.273645 14.59785 \tl:6.285973 \tem:0.066667\n",
      "Batch:\t12 /13\t:  1.228322 15.82664 \tl:6.556566 \tem:0.133333\n",
      "\n",
      "Epoch performance:  16.363sec Trl:6.956300 \tTrem:0.066667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  64 / 300\n",
      "Batch:\t0 /13\t:  1.574110 1.574126 \tl:7.052119 \tem:0.066667\n",
      "Batch:\t1 /13\t:  1.296330 2.871235 \tl:6.993557 \tem:0.066667\n",
      "Batch:\t2 /13\t:  1.366922 4.239062 \tl:7.399772 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.207726 5.447225 \tl:6.510589 \tem:0.066667\n",
      "Batch:\t4 /13\t:  1.116754 6.564864 \tl:7.364407 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.198185 7.763345 \tl:6.494301 \tem:0.088889\n",
      "Batch:\t6 /13\t:  1.241816 9.005627 \tl:6.951091 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.344242 10.35121 \tl:7.235944 \tem:0.088889\n",
      "Batch:\t8 /13\t:  1.274828 11.62651 \tl:6.073233 \tem:0.088889\n",
      "Batch:\t9 /13\t:  1.236266 12.86369 \tl:6.404049 \tem:0.022222\n",
      "Batch:\t10 /13\t:  1.265398 14.12970 \tl:7.222301 \tem:0.088889\n",
      "Batch:\t11 /13\t:  1.224653 15.35489 \tl:6.318127 \tem:0.088889\n",
      "Batch:\t12 /13\t:  1.188689 16.54479 \tl:6.815898 \tem:0.066667\n",
      "\n",
      "Epoch performance:  17.082sec Trl:6.833491 \tTrem:0.063248 \tTeem:0.000000\n",
      "\n",
      "Epoch:  65 / 300\n",
      "Batch:\t0 /13\t:  1.245301 1.245320 \tl:5.841633 \tem:0.044444\n",
      "Batch:\t1 /13\t:  1.295243 2.541024 \tl:6.868878 \tem:0.000000\n",
      "Batch:\t2 /13\t:  1.377075 3.918874 \tl:6.101942 \tem:0.155556\n",
      "Batch:\t3 /13\t:  1.203763 5.124393 \tl:7.028901 \tem:0.111111\n",
      "Batch:\t4 /13\t:  1.198719 6.323671 \tl:6.013287 \tem:0.066667\n",
      "Batch:\t5 /13\t:  1.247195 7.571725 \tl:6.587125 \tem:0.000000\n",
      "Batch:\t6 /13\t:  1.232033 8.804287 \tl:6.824286 \tem:0.000000\n",
      "Batch:\t7 /13\t:  1.215760 10.02082 \tl:6.909679 \tem:0.044444\n",
      "Batch:\t8 /13\t:  1.165437 11.18655 \tl:6.631158 \tem:0.088889\n",
      "Batch:\t9 /13\t:  1.668584 12.85603 \tl:6.111230 \tem:0.066667\n",
      "Batch:\t10 /13\t:  1.206605 14.06285 \tl:7.097869 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.249739 15.31317 \tl:6.825419 \tem:0.088889\n",
      "Batch:\t12 /13\t:  1.242212 16.55561 \tl:7.166284 \tem:0.022222\n",
      "\n",
      "Epoch performance:  17.093sec Trl:6.615976 \tTrem:0.056410 \tTeem:0.000000\n",
      "\n",
      "Epoch:  66 / 300\n",
      "Batch:\t0 /13\t:  1.201846 1.201861 \tl:7.498839 \tem:0.133333\n",
      "Batch:\t1 /13\t:  1.146829 2.349528 \tl:6.070273 \tem:0.088889\n",
      "Batch:\t2 /13\t:  1.184960 3.535363 \tl:6.265273 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.182072 4.718266 \tl:6.045015 \tem:0.088889\n",
      "Batch:\t4 /13\t:  1.309000 6.027740 \tl:7.107389 \tem:0.044444\n",
      "Batch:\t5 /13\t:  1.217426 7.245557 \tl:6.249036 \tem:0.133333\n",
      "Batch:\t6 /13\t:  1.191260 8.437503 \tl:7.001106 \tem:0.066667\n",
      "Batch:\t7 /13\t:  1.185500 9.623618 \tl:6.595045 \tem:0.088889\n",
      "Batch:\t8 /13\t:  1.359555 10.98427 \tl:6.729246 \tem:0.022222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t9 /13\t:  1.611346 12.59626 \tl:7.386455 \tem:0.000000\n",
      "Batch:\t10 /13\t:  1.194097 13.79104 \tl:6.816950 \tem:0.088889\n",
      "Batch:\t11 /13\t:  1.239887 15.03181 \tl:5.948918 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.161570 16.19420 \tl:7.105240 \tem:0.066667\n",
      "\n",
      "Epoch performance:  16.728sec Trl:6.678368 \tTrem:0.066667 \tTeem:0.000000\n",
      "\n",
      "Epoch:  67 / 300\n",
      "Batch:\t0 /13\t:  1.379084 1.379101 \tl:6.961443 \tem:0.066667\n",
      "Batch:\t1 /13\t:  1.204032 2.583961 \tl:6.432675 \tem:0.111111\n",
      "Batch:\t2 /13\t:  1.185533 3.770380 \tl:6.509604 \tem:0.177778\n",
      "Batch:\t3 /13\t:  1.328209 5.099201 \tl:6.391733 \tem:0.088889\n",
      "Batch:\t4 /13\t:  1.148263 6.248287 \tl:6.835643 \tem:0.044444\n",
      "Batch:\t5 /13\t:  1.335870 7.584961 \tl:6.237793 \tem:0.022222\n",
      "Batch:\t6 /13\t:  1.264353 8.849849 \tl:6.721598 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.215543 10.06590 \tl:6.228030 \tem:0.133333\n",
      "Batch:\t8 /13\t:  1.170289 11.23667 \tl:6.539603 \tem:0.088889\n",
      "Batch:\t9 /13\t:  1.255450 12.49269 \tl:6.015684 \tem:0.133333\n",
      "Batch:\t10 /13\t:  1.216995 13.71051 \tl:6.059768 \tem:0.088889\n",
      "Batch:\t11 /13\t:  1.202504 14.91412 \tl:6.491285 \tem:0.066667\n",
      "Batch:\t12 /13\t:  1.224646 16.13980 \tl:6.770831 \tem:0.044444\n",
      "\n",
      "Epoch performance:  16.677sec Trl:6.476592 \tTrem:0.085470 \tTeem:0.000000\n",
      "\n",
      "Epoch:  68 / 300\n",
      "Batch:\t0 /13\t:  1.376195 1.376219 \tl:6.872089 \tem:0.066667\n",
      "Batch:\t1 /13\t:  1.206400 2.583323 \tl:6.988823 \tem:0.066667\n",
      "Batch:\t2 /13\t:  1.190413 3.774617 \tl:6.985081 \tem:0.022222\n",
      "Batch:\t3 /13\t:  1.194252 4.969384 \tl:6.197352 \tem:0.111111\n",
      "Batch:\t4 /13\t:  1.237971 6.207828 \tl:6.498492 \tem:0.066667\n",
      "Batch:\t5 /13\t:  1.168097 7.376486 \tl:7.058432 \tem:0.111111\n",
      "Batch:\t6 /13\t:  1.250786 8.628546 \tl:6.765955 \tem:0.111111\n",
      "Batch:\t7 /13\t:  1.204009 9.832988 \tl:6.321210 \tem:0.111111\n",
      "Batch:\t8 /13\t:  1.180315 11.01382 \tl:6.637007 \tem:0.111111\n",
      "Batch:\t9 /13\t:  1.174753 12.18936 \tl:6.961466 \tem:0.022222\n",
      "Batch:\t10 /13\t:  1.288064 13.47807 \tl:6.228260 \tem:0.111111\n",
      "Batch:\t11 /13\t:  1.196355 14.67483 \tl:6.156903 \tem:0.044444\n",
      "Batch:\t12 /13\t:  1.247368 15.92249 \tl:6.775203 \tem:0.133333\n",
      "\n",
      "Epoch performance:  16.460sec Trl:6.649713 \tTrem:0.083761 \tTeem:0.000000\n",
      "\n",
      "Epoch:  69 / 300\n",
      "Batch:\t0 /13\t:  1.172468 1.172480 \tl:6.827098 \tem:0.044444\n",
      "Batch:\t1 /13\t:  1.249803 2.422761 \tl:6.030640 \tem:0.133333\n",
      "Batch:\t2 /13\t:  1.315010 3.739045 \tl:5.607776 \tem:0.066667\n",
      "Batch:\t3 /13\t:  1.240853 4.980392 \tl:6.397758 \tem:0.066667\n",
      "Batch:\t4 /13\t:  1.311945 6.292850 \tl:6.311840 \tem:0.111111\n",
      "Batch:\t5 /13\t:  1.240570 7.534565 \tl:7.084296 \tem:0.066667\n",
      "Batch:\t6 /13\t:  1.132385 8.667980 \tl:6.075663 \tem:0.133333\n",
      "Batch:\t7 /13\t:  1.309518 9.977823 \tl:6.617689 \tem:0.066667\n",
      "Batch:\t8 /13\t:  1.104291 11.08287 \tl:6.835731 \tem:0.066667\n",
      "Batch:\t9 /13\t:  1.247453 12.33054 \tl:6.750927 \tem:0.066667\n",
      "Batch:\t10 /13\t:  1.556300 13.88795 \tl:5.856794 \tem:0.111111\n",
      "Batch:\t11 /13\t:  1.495326 15.38446 \tl:6.339253 \tem:0.044444\n",
      "Batch:\t12 /13\t:  1.456625 16.84194 \tl:6.916384 \tem:0.022222\n",
      "\n",
      "Epoch performance:  17.377sec Trl:6.434758 \tTrem:0.076923 \tTeem:0.000000\n",
      "\n",
      "Epoch:  70 / 300\n",
      "Batch:\t0 /13\t:  1.231036 1.231050 \tl:6.235629 \tem:0.111111\n",
      "Batch:\t1 /13\t:  1.169389 2.401233 \tl:6.133724 \tem:0.066667\n",
      "Batch:\t2 /13\t:  1.202585 3.604783 \tl:6.122294 \tem:0.177778\n",
      "Batch:\t3 /13\t:  1.305981 4.910962 \tl:6.263101 \tem:0.066667\n",
      "Batch:\t4 /13\t:  1.238388 6.149755 \tl:7.022161 \tem:0.044444\n",
      "Batch:\t5 /13\t:  1.217260 7.367255 \tl:6.245778 \tem:0.111111\n",
      "Batch:\t6 /13\t:  1.183311 8.551365 \tl:6.985603 \tem:0.022222\n",
      "Batch:\t7 /13\t:  1.262139 9.814289 \tl:6.451431 \tem:0.066667\n",
      "Batch:\t8 /13\t:  1.171385 10.98614 \tl:5.902475 \tem:0.111111\n",
      "Batch:\t9 /13\t:  1.152812 12.13965 \tl:6.432794 \tem:0.066667\n",
      "Batch:\t10 /13\t:  1.205826 13.34625 \tl:6.923748 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.296426 14.64296 \tl:5.741728 \tem:0.066667\n",
      "Batch:\t12 /13\t:  1.140161 15.78331 \tl:6.366210 \tem:0.066667\n",
      "\n",
      "Epoch performance:  16.320sec Trl:6.371283 \tTrem:0.078632 \tTeem:0.000000\n",
      "\n",
      "Epoch:  71 / 300\n",
      "Batch:\t0 /13\t:  1.556862 1.556879 \tl:6.442772 \tem:0.066667\n",
      "Batch:\t1 /13\t:  1.446943 3.004586 \tl:7.292645 \tem:0.066667\n",
      "Batch:\t2 /13\t:  1.399430 4.404529 \tl:6.046270 \tem:0.111111\n",
      "Batch:\t3 /13\t:  1.517682 5.922713 \tl:6.161621 \tem:0.111111\n",
      "Batch:\t4 /13\t:  1.420571 7.343781 \tl:7.116496 \tem:0.000000\n",
      "Batch:\t5 /13\t:  1.272425 8.616724 \tl:6.080404 \tem:0.044444\n",
      "Batch:\t6 /13\t:  1.193228 9.810363 \tl:7.761190 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.235153 11.04608 \tl:6.494351 \tem:0.133333\n",
      "Batch:\t8 /13\t:  1.174422 12.22106 \tl:7.055955 \tem:0.111111\n",
      "Batch:\t9 /13\t:  1.253619 13.47540 \tl:6.592983 \tem:0.111111\n",
      "Batch:\t10 /13\t:  1.231041 14.70707 \tl:6.795582 \tem:0.000000\n",
      "Batch:\t11 /13\t:  1.258777 15.96635 \tl:6.948417 \tem:0.088889\n",
      "Batch:\t12 /13\t:  1.283565 17.25078 \tl:6.136234 \tem:0.133333\n",
      "\n",
      "Epoch performance:  17.788sec Trl:6.686532 \tTrem:0.078632 \tTeem:0.000000\n",
      "\n",
      "Epoch:  72 / 300\n",
      "Batch:\t0 /13\t:  1.299472 1.299490 \tl:5.848778 \tem:0.222222\n",
      "Batch:\t1 /13\t:  1.245204 2.545580 \tl:6.680719 \tem:0.133333\n",
      "Batch:\t2 /13\t:  1.211682 3.757991 \tl:6.488165 \tem:0.066667\n",
      "Batch:\t3 /13\t:  1.186595 4.945457 \tl:5.562202 \tem:0.133333\n",
      "Batch:\t4 /13\t:  1.247007 6.193848 \tl:6.404343 \tem:0.088889\n",
      "Batch:\t5 /13\t:  1.157738 7.351818 \tl:6.702429 \tem:0.044444\n",
      "Batch:\t6 /13\t:  1.218705 8.571511 \tl:5.738426 \tem:0.133333\n",
      "Batch:\t7 /13\t:  1.146571 9.718487 \tl:7.389107 \tem:0.022222\n",
      "Batch:\t8 /13\t:  1.150356 10.86968 \tl:6.071477 \tem:0.066667\n",
      "Batch:\t9 /13\t:  1.191009 12.06118 \tl:6.270949 \tem:0.133333\n",
      "Batch:\t10 /13\t:  1.294401 13.35625 \tl:6.488416 \tem:0.088889\n",
      "Batch:\t11 /13\t:  1.327636 14.68432 \tl:5.270425 \tem:0.044444\n",
      "Batch:\t12 /13\t:  1.132542 15.81739 \tl:6.824338 \tem:0.066667\n",
      "\n",
      "Epoch performance:  16.354sec Trl:6.287675 \tTrem:0.095726 \tTeem:0.000000\n",
      "\n",
      "Epoch:  73 / 300\n",
      "Batch:\t0 /13\t:  1.284938 1.284957 \tl:5.968732 \tem:0.088889\n",
      "Batch:\t1 /13\t:  1.270999 2.556241 \tl:6.411986 \tem:0.066667\n",
      "Batch:\t2 /13\t:  1.213557 3.771026 \tl:6.044257 \tem:0.066667\n",
      "Batch:\t3 /13\t:  1.590609 5.362529 \tl:5.459572 \tem:0.066667\n",
      "Batch:\t4 /13\t:  1.215925 6.578713 \tl:6.295136 \tem:0.066667\n",
      "Batch:\t5 /13\t:  1.169409 7.749011 \tl:7.024840 \tem:0.044444\n",
      "Batch:\t6 /13\t:  1.190710 8.940016 \tl:6.706110 \tem:0.088889\n",
      "Batch:\t7 /13\t:  1.158497 10.09910 \tl:5.953870 \tem:0.133333\n",
      "Batch:\t8 /13\t:  1.308623 11.40860 \tl:5.547957 \tem:0.200000\n",
      "Batch:\t9 /13\t:  1.290341 12.69927 \tl:6.867937 \tem:0.022222\n",
      "Batch:\t10 /13\t:  1.194893 13.89526 \tl:6.146473 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.173561 15.06900 \tl:6.353257 \tem:0.088889\n",
      "Batch:\t12 /13\t:  1.091839 16.16129 \tl:6.277233 \tem:0.088889\n",
      "\n",
      "Epoch performance:  16.697sec Trl:6.235182 \tTrem:0.082051 \tTeem:0.000000\n",
      "\n",
      "Epoch:  74 / 300\n",
      "Batch:\t0 /13\t:  1.295781 1.295796 \tl:6.846610 \tem:0.044444\n",
      "Batch:\t1 /13\t:  1.141247 2.438221 \tl:6.309504 \tem:0.044444\n",
      "Batch:\t2 /13\t:  1.228253 3.666913 \tl:6.661923 \tem:0.133333\n",
      "Batch:\t3 /13\t:  1.188682 4.856442 \tl:6.449272 \tem:0.044444\n",
      "Batch:\t4 /13\t:  1.174827 6.032109 \tl:6.689641 \tem:0.044444\n",
      "Batch:\t5 /13\t:  1.291601 7.324426 \tl:6.302026 \tem:0.066667\n",
      "Batch:\t6 /13\t:  1.645905 8.971607 \tl:6.166547 \tem:0.133333\n",
      "Batch:\t7 /13\t:  1.183327 10.15526 \tl:6.235427 \tem:0.088889\n",
      "Batch:\t8 /13\t:  1.129317 11.28497 \tl:6.022790 \tem:0.066667\n",
      "Batch:\t9 /13\t:  1.271636 12.55776 \tl:5.771924 \tem:0.111111\n",
      "Batch:\t10 /13\t:  1.117748 13.67667 \tl:6.621607 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.228198 14.90565 \tl:5.858262 \tem:0.133333\n",
      "Batch:\t12 /13\t:  1.199305 16.10550 \tl:6.030204 \tem:0.066667\n",
      "\n",
      "Epoch performance:  16.641sec Trl:6.305057 \tTrem:0.078632 \tTeem:0.000000\n",
      "\n",
      "Epoch:  75 / 300\n",
      "Batch:\t0 /13\t:  1.250216 1.250234 \tl:5.707268 \tem:0.066667\n",
      "Batch:\t1 /13\t:  1.134305 2.385349 \tl:5.780674 \tem:0.133333\n",
      "Batch:\t2 /13\t:  1.189223 3.575006 \tl:5.676333 \tem:0.111111\n",
      "Batch:\t3 /13\t:  1.258020 4.833693 \tl:5.392267 \tem:0.200000\n",
      "Batch:\t4 /13\t:  1.252202 6.086752 \tl:6.557905 \tem:0.088889\n",
      "Batch:\t5 /13\t:  1.243851 7.331190 \tl:6.810270 \tem:0.066667\n",
      "Batch:\t6 /13\t:  1.259432 8.591704 \tl:5.128568 \tem:0.133333\n",
      "Batch:\t7 /13\t:  1.220032 9.812657 \tl:5.343244 \tem:0.133333\n",
      "Batch:\t8 /13\t:  1.301199 11.11513 \tl:6.306909 \tem:0.066667\n",
      "Batch:\t9 /13\t:  1.226863 12.34301 \tl:6.311431 \tem:0.044444\n",
      "Batch:\t10 /13\t:  1.142652 13.48587 \tl:6.536948 \tem:0.088889\n",
      "Batch:\t11 /13\t:  1.238054 14.72414 \tl:6.151556 \tem:0.044444\n",
      "Batch:\t12 /13\t:  1.174638 15.89946 \tl:5.179587 \tem:0.155556\n",
      "\n",
      "Epoch performance:  16.437sec Trl:5.914074 \tTrem:0.102564 \tTeem:0.000000\n",
      "\n",
      "Epoch:  76 / 300\n",
      "Batch:\t0 /13\t:  1.317301 1.317328 \tl:6.318470 \tem:0.044444\n",
      "Batch:\t1 /13\t:  1.295634 2.613716 \tl:6.369155 \tem:0.133333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t2 /13\t:  1.235563 3.849728 \tl:6.714953 \tem:0.044444\n",
      "Batch:\t3 /13\t:  1.299339 5.149561 \tl:6.024949 \tem:0.111111\n",
      "Batch:\t4 /13\t:  1.291872 6.442435 \tl:5.489487 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.325559 7.770185 \tl:6.314895 \tem:0.088889\n",
      "Batch:\t6 /13\t:  1.280632 9.051473 \tl:6.876702 \tem:0.066667\n",
      "Batch:\t7 /13\t:  1.225950 10.27811 \tl:5.839423 \tem:0.088889\n",
      "Batch:\t8 /13\t:  1.195107 11.47454 \tl:5.945044 \tem:0.133333\n",
      "Batch:\t9 /13\t:  1.249892 12.72524 \tl:5.917655 \tem:0.111111\n",
      "Batch:\t10 /13\t:  1.452963 14.17847 \tl:5.711803 \tem:0.133333\n",
      "Batch:\t11 /13\t:  1.469702 15.64889 \tl:6.104566 \tem:0.044444\n",
      "Batch:\t12 /13\t:  1.190299 16.84049 \tl:6.036381 \tem:0.066667\n",
      "\n",
      "Epoch performance:  17.378sec Trl:6.127960 \tTrem:0.083761 \tTeem:0.000000\n",
      "\n",
      "Epoch:  77 / 300\n",
      "Batch:\t0 /13\t:  1.335264 1.335281 \tl:6.661150 \tem:0.133333\n",
      "Batch:\t1 /13\t:  1.268509 2.604518 \tl:6.416002 \tem:0.088889\n",
      "Batch:\t2 /13\t:  1.310543 3.915302 \tl:5.776568 \tem:0.177778\n",
      "Batch:\t3 /13\t:  1.274667 5.190840 \tl:6.713096 \tem:0.044444\n",
      "Batch:\t4 /13\t:  1.282979 6.474623 \tl:5.608885 \tem:0.111111\n",
      "Batch:\t5 /13\t:  1.161584 7.636754 \tl:5.920180 \tem:0.155556\n",
      "Batch:\t6 /13\t:  1.294388 8.931413 \tl:6.501606 \tem:0.111111\n",
      "Batch:\t7 /13\t:  1.282866 10.21551 \tl:6.582404 \tem:0.133333\n",
      "Batch:\t8 /13\t:  1.298362 11.51448 \tl:5.765841 \tem:0.111111\n",
      "Batch:\t9 /13\t:  1.216334 12.73111 \tl:5.863431 \tem:0.088889\n",
      "Batch:\t10 /13\t:  1.315496 14.04759 \tl:6.198313 \tem:0.088889\n",
      "Batch:\t11 /13\t:  1.239423 15.28811 \tl:5.963021 \tem:0.066667\n",
      "Batch:\t12 /13\t:  1.157242 16.44624 \tl:5.973946 \tem:0.133333\n",
      "\n",
      "Epoch performance:  16.983sec Trl:6.149572 \tTrem:0.111111 \tTeem:0.000000\n",
      "\n",
      "Epoch:  78 / 300\n",
      "Batch:\t0 /13\t:  1.332710 1.332736 \tl:5.794515 \tem:0.111111\n",
      "Batch:\t1 /13\t:  1.330137 2.664057 \tl:6.178695 \tem:0.133333\n",
      "Batch:\t2 /13\t:  1.515454 4.180199 \tl:6.918217 \tem:0.088889\n",
      "Batch:\t3 /13\t:  1.321767 5.502543 \tl:6.323895 \tem:0.066667\n",
      "Batch:\t4 /13\t:  1.258391 6.762166 \tl:6.120013 \tem:0.088889\n",
      "Batch:\t5 /13\t:  1.321220 8.084089 \tl:6.239738 \tem:0.088889\n",
      "Batch:\t6 /13\t:  1.324215 9.408898 \tl:5.294528 \tem:0.111111\n",
      "Batch:\t7 /13\t:  1.262693 10.67229 \tl:5.688498 \tem:0.177778\n",
      "Batch:\t8 /13\t:  1.209558 11.88241 \tl:5.607490 \tem:0.111111\n",
      "Batch:\t9 /13\t:  1.265572 13.14818 \tl:5.581165 \tem:0.200000\n",
      "Batch:\t10 /13\t:  1.205883 14.35457 \tl:5.194626 \tem:0.111111\n",
      "Batch:\t11 /13\t:  1.158575 15.51332 \tl:5.883122 \tem:0.155556\n",
      "Batch:\t12 /13\t:  1.199285 16.71353 \tl:6.563736 \tem:0.133333\n",
      "\n",
      "Epoch performance:  17.250sec Trl:5.952941 \tTrem:0.121368 \tTeem:0.000000\n",
      "\n",
      "Epoch:  79 / 300\n",
      "Batch:\t0 /13\t:  1.261427 1.261445 \tl:6.395212 \tem:0.133333\n",
      "Batch:\t1 /13\t:  1.276967 2.538676 \tl:6.043899 \tem:0.111111\n",
      "Batch:\t2 /13\t:  1.132231 3.671679 \tl:5.696036 \tem:0.044444\n",
      "Batch:\t3 /13\t:  1.215528 4.888252 \tl:5.924624 \tem:0.111111\n",
      "Batch:\t4 /13\t:  1.258798 6.148158 \tl:5.585924 \tem:0.155556\n",
      "Batch:\t5 /13\t:  1.132560 7.281585 \tl:6.404660 \tem:0.088889\n",
      "Batch:\t6 /13\t:  1.265650 8.547703 \tl:5.854677 \tem:0.111111\n",
      "Batch:\t7 /13\t:  1.124525 9.672803 \tl:5.380644 \tem:0.133333\n",
      "Batch:\t8 /13\t:  1.227591 10.90095 \tl:5.474603 \tem:0.222222\n",
      "Batch:\t9 /13\t:  1.218735 12.12086 \tl:6.276628 \tem:0.111111\n",
      "Batch:\t10 /13\t:  1.184570 13.30594 \tl:5.732833 \tem:0.177778\n",
      "Batch:\t11 /13\t:  1.260650 14.56679 \tl:5.576442 \tem:0.111111\n",
      "Batch:\t12 /13\t:  1.146121 15.71348 \tl:6.290503 \tem:0.088889\n",
      "\n",
      "Epoch performance:  16.251sec Trl:5.895130 \tTrem:0.123077 \tTeem:0.000000\n",
      "\n",
      "Epoch:  80 / 300\n",
      "Batch:\t0 /13\t:  1.334092 1.334107 \tl:5.999980 \tem:0.088889\n",
      "Batch:\t1 /13\t:  1.170790 2.505430 \tl:6.019779 \tem:0.044444\n",
      "Batch:\t2 /13\t:  1.297406 3.803129 \tl:6.380833 \tem:0.088889\n",
      "Batch:\t3 /13\t:  1.192376 4.996025 \tl:6.078180 \tem:0.022222\n",
      "Batch:\t4 /13\t:  1.322408 6.318717 \tl:6.105778 \tem:0.133333\n",
      "Batch:\t5 /13\t:  1.187923 7.507129 \tl:5.769265 \tem:0.044444\n",
      "Batch:\t6 /13\t:  1.268029 8.775662 \tl:6.390105 \tem:0.044444\n",
      "Batch:\t7 /13\t:  1.609480 10.38605 \tl:6.053450 \tem:0.133333\n",
      "Batch:\t8 /13\t:  1.557386 11.94373 \tl:5.545367 \tem:0.155556\n",
      "Batch:\t9 /13\t:  1.510435 13.45480 \tl:5.591379 \tem:0.111111\n",
      "Batch:\t10 /13\t:  1.562026 15.01759 \tl:6.066175 \tem:0.111111\n",
      "Batch:\t11 /13\t:  1.534790 16.55360 \tl:6.421168 \tem:0.044444\n",
      "Batch:\t12 /13\t:  1.370656 17.92535 \tl:5.605947 \tem:0.111111\n",
      "\n",
      "Epoch performance:  18.463sec Trl:6.002108 \tTrem:0.087179 \tTeem:0.000000\n",
      "\n",
      "Epoch:  81 / 300\n",
      "Batch:\t0 /13\t:  1.200093 1.200108 \tl:6.239957 \tem:0.088889\n",
      "Batch:\t1 /13\t:  1.190800 2.391628 \tl:6.242030 \tem:0.088889\n",
      "Batch:\t2 /13\t:  1.207952 3.600232 \tl:5.451739 \tem:0.111111\n",
      "Batch:\t3 /13\t:  1.211680 4.812425 \tl:4.994236 \tem:0.177778\n",
      "Batch:\t4 /13\t:  1.195573 6.008544 \tl:6.357204 \tem:0.066667\n",
      "Batch:\t5 /13\t:  1.273263 7.282294 \tl:5.673905 \tem:0.111111\n",
      "Batch:\t6 /13\t:  1.264677 8.548160 \tl:6.387322 \tem:0.088889\n",
      "Batch:\t7 /13\t:  1.246506 9.795541 \tl:6.133811 \tem:0.066667\n",
      "Batch:\t8 /13\t:  1.171900 10.96789 \tl:5.504683 \tem:0.155556\n",
      "Batch:\t9 /13\t:  1.170953 12.13952 \tl:5.456359 \tem:0.111111\n",
      "Batch:\t10 /13\t:  1.221573 13.36187 \tl:5.641378 \tem:0.155556\n",
      "Batch:\t11 /13\t:  1.210204 14.57313 \tl:5.491616 \tem:0.133333\n",
      "Batch:\t12 /13\t:  1.222232 15.79581 \tl:5.855881 \tem:0.177778\n",
      "\n",
      "Epoch performance:  16.334sec Trl:5.802317 \tTrem:0.117949 \tTeem:0.000000\n",
      "\n",
      "Epoch:  82 / 300\n",
      "Batch:\t0 /13\t:  1.286155 1.286171 \tl:5.936337 \tem:0.111111\n",
      "Batch:\t1 /13\t:  1.209995 2.496389 \tl:5.463254 \tem:0.133333\n",
      "Batch:\t2 /13\t:  1.194926 3.692034 \tl:5.388171 \tem:0.133333\n",
      "Batch:\t3 /13\t:  1.221780 4.914338 \tl:6.093289 \tem:0.133333\n",
      "Batch:\t4 /13\t:  1.177839 6.094080 \tl:6.045391 \tem:0.177778\n",
      "Batch:\t5 /13\t:  1.197588 7.292710 \tl:5.659496 \tem:0.111111\n",
      "Batch:\t6 /13\t:  1.318444 8.611986 \tl:5.966841 \tem:0.155556\n",
      "Batch:\t7 /13\t:  1.144157 9.756687 \tl:5.600937 \tem:0.288889\n",
      "Batch:\t8 /13\t:  1.291740 11.04885 \tl:5.402807 \tem:0.111111\n",
      "Batch:\t9 /13\t:  1.195021 12.24462 \tl:5.454896 \tem:0.133333\n",
      "Batch:\t10 /13\t:  1.220937 13.46615 \tl:6.495186 \tem:0.044444\n",
      "Batch:\t11 /13\t:  1.230879 14.69772 \tl:5.791930 \tem:0.133333\n",
      "Batch:\t12 /13\t:  1.242112 15.94048 \tl:5.655569 \tem:0.111111\n",
      "\n",
      "Epoch performance:  16.478sec Trl:5.765700 \tTrem:0.136752 \tTeem:0.000000\n",
      "\n",
      "Epoch:  83 / 300\n",
      "Batch:\t0 /13\t:  1.260563 1.260581 \tl:5.008508 \tem:0.177778\n",
      "Batch:\t1 /13\t:  1.225519 2.487343 \tl:5.089206 \tem:0.177778\n",
      "Batch:\t2 /13\t:  1.177892 3.666448 \tl:6.441851 \tem:0.088889\n",
      "Batch:\t3 /13\t:  1.184237 4.851388 \tl:6.082896 \tem:0.111111\n",
      "Batch:\t4 /13\t:  1.162640 6.015340 \tl:5.965218 \tem:0.022222\n",
      "Batch:\t5 /13\t:  1.246109 7.261954 \tl:5.476541 \tem:0.066667\n",
      "Batch:\t6 /13\t:  1.304975 8.567373 \tl:5.500113 \tem:0.133333\n",
      "Batch:\t7 /13\t:  1.176991 9.744913 \tl:6.028214 \tem:0.044444\n",
      "Batch:\t8 /13\t:  1.207788 10.95378 \tl:5.682024 \tem:0.088889\n",
      "Batch:\t9 /13\t:  1.143377 12.09837 \tl:4.936508 \tem:0.177778\n",
      "Batch:\t10 /13\t:  1.179857 13.27911 \tl:5.793626 \tem:0.155556\n",
      "Batch:\t11 /13\t:  1.191301 14.47316 \tl:7.414454 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.258063 15.73244 \tl:6.394547 \tem:0.066667\n",
      "\n",
      "Epoch performance:  16.268sec Trl:5.831823 \tTrem:0.102564 \tTeem:0.000000\n",
      "\n",
      "Epoch:  84 / 300\n",
      "Batch:\t0 /13\t:  1.259212 1.259232 \tl:5.773216 \tem:0.088889\n",
      "Batch:\t1 /13\t:  1.179336 2.439110 \tl:5.880388 \tem:0.155556\n",
      "Batch:\t2 /13\t:  1.175009 3.614582 \tl:5.932771 \tem:0.200000\n",
      "Batch:\t3 /13\t:  1.255342 4.870993 \tl:6.086266 \tem:0.088889\n",
      "Batch:\t4 /13\t:  1.225525 6.097838 \tl:5.007095 \tem:0.177778\n",
      "Batch:\t5 /13\t:  1.292443 7.390608 \tl:5.744897 \tem:0.066667\n",
      "Batch:\t6 /13\t:  1.261878 8.653081 \tl:6.069843 \tem:0.133333\n",
      "Batch:\t7 /13\t:  1.147647 9.801692 \tl:6.098584 \tem:0.111111\n",
      "Batch:\t8 /13\t:  1.184451 10.98665 \tl:5.797154 \tem:0.088889\n",
      "Batch:\t9 /13\t:  1.207598 12.19514 \tl:5.598013 \tem:0.155556\n",
      "Batch:\t10 /13\t:  1.274895 13.47113 \tl:5.665338 \tem:0.177778\n",
      "Batch:\t11 /13\t:  1.217901 14.69010 \tl:6.050792 \tem:0.022222\n",
      "Batch:\t12 /13\t:  1.244441 15.93533 \tl:5.707422 \tem:0.044444\n",
      "\n",
      "Epoch performance:  16.473sec Trl:5.800906 \tTrem:0.116239 \tTeem:0.000000\n",
      "\n",
      "Epoch:  85 / 300\n",
      "Batch:\t0 /13\t:  1.516571 1.516585 \tl:6.667345 \tem:0.044444\n",
      "Batch:\t1 /13\t:  1.217914 2.735572 \tl:6.480497 \tem:0.133333\n",
      "Batch:\t2 /13\t:  1.643631 4.379731 \tl:5.830039 \tem:0.088889\n",
      "Batch:\t3 /13\t:  1.521328 5.901609 \tl:5.856514 \tem:0.088889\n",
      "Batch:\t4 /13\t:  1.273485 7.176247 \tl:5.190323 \tem:0.111111\n",
      "Batch:\t5 /13\t:  1.200016 8.376833 \tl:5.895335 \tem:0.044444\n",
      "Batch:\t6 /13\t:  1.212273 9.591271 \tl:6.530969 \tem:0.066667\n",
      "Batch:\t7 /13\t:  1.179724 10.77182 \tl:5.383213 \tem:0.066667\n",
      "Batch:\t8 /13\t:  1.227688 11.99980 \tl:5.968540 \tem:0.088889\n",
      "Batch:\t9 /13\t:  1.158944 13.17593 \tl:5.865893 \tem:0.222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t10 /13\t:  1.189327 14.36589 \tl:5.271995 \tem:0.200000\n",
      "Batch:\t11 /13\t:  1.157418 15.52413 \tl:5.949439 \tem:0.066667\n",
      "Batch:\t12 /13\t:  1.614816 17.13949 \tl:5.725629 \tem:0.177778\n",
      "\n",
      "Epoch performance:  17.676sec Trl:5.893518 \tTrem:0.107692 \tTeem:0.000000\n",
      "\n",
      "Epoch:  86 / 300\n",
      "Batch:\t0 /13\t:  1.545009 1.545036 \tl:5.632473 \tem:0.088889\n",
      "Batch:\t1 /13\t:  1.453027 2.998753 \tl:5.657427 \tem:0.155556\n",
      "Batch:\t2 /13\t:  1.312621 4.312324 \tl:5.193488 \tem:0.111111\n",
      "Batch:\t3 /13\t:  1.315919 5.628501 \tl:5.504465 \tem:0.111111\n",
      "Batch:\t4 /13\t:  1.162839 6.791596 \tl:5.493328 \tem:0.133333\n",
      "Batch:\t5 /13\t:  1.249315 8.041601 \tl:5.474663 \tem:0.222222\n",
      "Batch:\t6 /13\t:  1.110724 9.152569 \tl:5.695811 \tem:0.133333\n",
      "Batch:\t7 /13\t:  1.251112 10.40413 \tl:5.481245 \tem:0.066667\n",
      "Batch:\t8 /13\t:  1.168226 11.57331 \tl:5.154232 \tem:0.133333\n",
      "Batch:\t9 /13\t:  1.183150 12.75731 \tl:6.009338 \tem:0.022222\n",
      "Batch:\t10 /13\t:  1.325384 14.08335 \tl:5.624886 \tem:0.133333\n",
      "Batch:\t11 /13\t:  1.182880 15.26688 \tl:5.903529 \tem:0.155556\n",
      "Batch:\t12 /13\t:  1.225064 16.49216 \tl:5.198195 \tem:0.088889\n",
      "\n",
      "Epoch performance:  17.031sec Trl:5.540237 \tTrem:0.119658 \tTeem:0.000000\n",
      "\n",
      "Epoch:  87 / 300\n",
      "Batch:\t0 /13\t:  1.264814 1.264829 \tl:5.361092 \tem:0.111111\n",
      "Batch:\t1 /13\t:  1.249566 2.515218 \tl:5.001031 \tem:0.333333\n",
      "Batch:\t2 /13\t:  1.234830 3.750872 \tl:6.027130 \tem:0.066667\n",
      "Batch:\t3 /13\t:  1.253248 5.004671 \tl:6.291617 \tem:0.177778\n",
      "Batch:\t4 /13\t:  1.203038 6.208243 \tl:5.208726 \tem:0.200000\n",
      "Batch:\t5 /13\t:  1.249252 7.457934 \tl:6.005203 \tem:0.133333\n",
      "Batch:\t6 /13\t:  1.256056 8.714706 \tl:5.603971 \tem:0.200000\n",
      "Batch:\t7 /13\t:  1.242529 9.958186 \tl:5.946342 \tem:0.133333\n",
      "Batch:\t8 /13\t:  1.320391 11.27938 \tl:5.706774 \tem:0.066667\n",
      "Batch:\t9 /13\t:  1.182092 12.46209 \tl:5.331248 \tem:0.133333\n",
      "Batch:\t10 /13\t:  1.392165 13.85486 \tl:5.908983 \tem:0.133333\n",
      "Batch:\t11 /13\t:  1.221503 15.09177 \tl:5.713192 \tem:0.111111\n",
      "Batch:\t12 /13\t:  1.206615 16.29936 \tl:5.880567 \tem:0.088889\n",
      "\n",
      "Epoch performance:  16.835sec Trl:5.691221 \tTrem:0.145299 \tTeem:0.000000\n",
      "\n",
      "Epoch:  88 / 300\n",
      "Batch:\t0 /13\t:  1.297037 1.297052 \tl:5.028490 \tem:0.244444\n",
      "Batch:\t1 /13\t:  1.188301 2.486397 \tl:4.298507 \tem:0.333333\n",
      "Batch:\t2 /13\t:  1.141891 3.628968 \tl:6.283701 \tem:0.111111\n",
      "Batch:\t3 /13\t:  1.149078 4.778574 \tl:5.827119 \tem:0.044444\n",
      "Batch:\t4 /13\t:  1.594433 6.373737 \tl:6.080653 \tem:0.088889\n",
      "Batch:\t5 /13\t:  1.324251 7.698187 \tl:5.367778 \tem:0.155556\n",
      "Batch:\t6 /13\t:  1.178516 8.877253 \tl:5.694593 \tem:0.066667\n",
      "Batch:\t7 /13\t:  1.210228 10.08823 \tl:5.165749 \tem:0.088889\n",
      "Batch:\t8 /13\t:  1.169541 11.25878 \tl:5.127810 \tem:0.155556\n",
      "Batch:\t9 /13\t:  1.116839 12.37604 \tl:6.084927 \tem:0.044444\n",
      "Batch:\t10 /13\t:  1.233217 13.60987 \tl:4.668961 \tem:0.200000\n",
      "Batch:\t11 /13\t:  1.144891 14.76321 \tl:5.728086 \tem:0.133333\n",
      "Batch:\t12 /13\t:  1.332765 16.09666 \tl:5.581854 \tem:0.155556\n",
      "\n",
      "Epoch performance:  16.635sec Trl:5.456787 \tTrem:0.140171 \tTeem:0.000000\n",
      "\n",
      "Epoch:  89 / 300\n",
      "Batch:\t0 /13\t:  1.318900 1.318919 \tl:6.319161 \tem:0.088889\n",
      "Batch:\t1 /13\t:  1.227202 2.546864 \tl:5.392189 \tem:0.111111\n",
      "Batch:\t2 /13\t:  1.222414 3.769953 \tl:6.084487 \tem:0.111111\n",
      "Batch:\t3 /13\t:  1.223913 4.994604 \tl:5.379185 \tem:0.222222\n",
      "Batch:\t4 /13\t:  1.174097 6.169974 \tl:5.497638 \tem:0.133333\n",
      "Batch:\t5 /13\t:  1.195721 7.366752 \tl:5.672053 \tem:0.088889\n",
      "Batch:\t6 /13\t:  1.283146 8.650292 \tl:4.941981 \tem:0.200000\n",
      "Batch:\t7 /13\t:  1.301125 9.952651 \tl:4.854160 \tem:0.133333\n",
      "Batch:\t8 /13\t:  1.193864 11.14698 \tl:5.087282 \tem:0.222222\n",
      "Batch:\t9 /13\t:  1.191248 12.33890 \tl:4.634642 \tem:0.288889\n",
      "Batch:\t10 /13\t:  1.233734 13.57312 \tl:5.394347 \tem:0.111111\n",
      "Batch:\t11 /13\t:  1.303116 14.87680 \tl:6.332451 \tem:0.044444\n",
      "Batch:\t12 /13\t:  1.184712 16.06195 \tl:5.563518 \tem:0.155556\n",
      "\n",
      "Epoch performance:  16.601sec Trl:5.473315 \tTrem:0.147009 \tTeem:0.000000\n",
      "\n",
      "Epoch:  90 / 300\n",
      "Batch:\t0 /13\t:  1.140399 1.140414 \tl:4.916814 \tem:0.155556\n",
      "Batch:\t1 /13\t:  1.210240 2.351161 \tl:5.135273 \tem:0.155556\n",
      "Batch:\t2 /13\t:  1.187170 3.538557 \tl:4.338139 \tem:0.155556\n",
      "Batch:\t3 /13\t:  1.311328 4.850435 \tl:5.700970 \tem:0.111111\n",
      "Batch:\t4 /13\t:  1.128421 5.979343 \tl:5.541152 \tem:0.155556\n",
      "Batch:\t5 /13\t:  1.299500 7.279445 \tl:4.988176 \tem:0.200000\n",
      "Batch:\t6 /13\t:  1.227601 8.507956 \tl:7.041121 \tem:0.022222\n",
      "Batch:\t7 /13\t:  1.270100 9.778337 \tl:5.174363 \tem:0.155556\n",
      "Batch:\t8 /13\t:  1.357764 11.13813 \tl:4.243423 \tem:0.288889\n",
      "Batch:\t9 /13\t:  1.135035 12.27364 \tl:6.081857 \tem:0.177778\n",
      "Batch:\t10 /13\t:  1.202164 13.47654 \tl:5.146354 \tem:0.222222\n",
      "Batch:\t11 /13\t:  1.164247 14.64157 \tl:5.420610 \tem:0.111111\n",
      "Batch:\t12 /13\t:  1.217966 15.86041 \tl:5.160881 \tem:0.111111\n",
      "\n",
      "Epoch performance:  16.398sec Trl:5.299164 \tTrem:0.155556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  91 / 300\n",
      "Batch:\t0 /13\t:  1.097825 1.097841 \tl:5.643423 \tem:0.155556\n",
      "Batch:\t1 /13\t:  1.194081 2.292136 \tl:5.932372 \tem:0.088889\n",
      "Batch:\t2 /13\t:  1.236464 3.529331 \tl:5.776314 \tem:0.066667\n",
      "Batch:\t3 /13\t:  1.187422 4.716964 \tl:5.174927 \tem:0.155556\n",
      "Batch:\t4 /13\t:  1.227142 5.944586 \tl:5.830961 \tem:0.177778\n",
      "Batch:\t5 /13\t:  1.312048 7.257484 \tl:6.130091 \tem:0.222222\n",
      "Batch:\t6 /13\t:  1.164535 8.422590 \tl:4.787981 \tem:0.222222\n",
      "Batch:\t7 /13\t:  1.147096 9.570158 \tl:5.365749 \tem:0.222222\n",
      "Batch:\t8 /13\t:  1.187844 10.75828 \tl:5.761129 \tem:0.133333\n",
      "Batch:\t9 /13\t:  1.302917 12.06149 \tl:5.040717 \tem:0.111111\n",
      "Batch:\t10 /13\t:  1.317320 13.37961 \tl:5.426740 \tem:0.133333\n",
      "Batch:\t11 /13\t:  1.254647 14.63474 \tl:5.449959 \tem:0.066667\n",
      "Batch:\t12 /13\t:  1.206336 15.84156 \tl:4.440670 \tem:0.177778\n",
      "\n",
      "Epoch performance:  16.377sec Trl:5.443156 \tTrem:0.148718 \tTeem:0.000000\n",
      "\n",
      "Epoch:  92 / 300\n",
      "Batch:\t0 /13\t:  1.156624 1.156640 \tl:5.981951 \tem:0.133333\n",
      "Batch:\t1 /13\t:  1.191826 2.349114 \tl:5.487291 \tem:0.200000\n",
      "Batch:\t2 /13\t:  1.252663 3.602365 \tl:5.601859 \tem:0.133333\n",
      "Batch:\t3 /13\t:  1.274546 4.877115 \tl:5.203525 \tem:0.111111\n",
      "Batch:\t4 /13\t:  1.225337 6.103471 \tl:4.661547 \tem:0.266667\n",
      "Batch:\t5 /13\t:  1.193225 7.297646 \tl:4.788996 \tem:0.155556\n",
      "Batch:\t6 /13\t:  1.196094 8.494094 \tl:6.267854 \tem:0.088889\n",
      "Batch:\t7 /13\t:  1.148482 9.643007 \tl:5.654883 \tem:0.133333\n",
      "Batch:\t8 /13\t:  1.159359 10.80360 \tl:6.073738 \tem:0.133333\n",
      "Batch:\t9 /13\t:  1.353837 12.15770 \tl:5.564537 \tem:0.155556\n",
      "Batch:\t10 /13\t:  1.171750 13.33050 \tl:4.966916 \tem:0.155556\n",
      "Batch:\t11 /13\t:  1.287230 14.61846 \tl:5.250261 \tem:0.088889\n",
      "Batch:\t12 /13\t:  1.275093 15.89401 \tl:6.027403 \tem:0.111111\n",
      "\n",
      "Epoch performance:  16.430sec Trl:5.502366 \tTrem:0.143590 \tTeem:0.000000\n",
      "\n",
      "Epoch:  93 / 300\n",
      "Batch:\t0 /13\t:  1.555970 1.555989 \tl:5.099784 \tem:0.155556\n",
      "Batch:\t1 /13\t:  1.504690 3.061200 \tl:5.003028 \tem:0.200000\n",
      "Batch:\t2 /13\t:  1.299854 4.361500 \tl:5.264555 \tem:0.200000\n",
      "Batch:\t3 /13\t:  1.194499 5.556570 \tl:5.552861 \tem:0.111111\n",
      "Batch:\t4 /13\t:  1.178895 6.736417 \tl:4.426636 \tem:0.266667\n",
      "Batch:\t5 /13\t:  1.228317 7.965669 \tl:4.616129 \tem:0.155556\n",
      "Batch:\t6 /13\t:  1.226423 9.192517 \tl:5.191605 \tem:0.222222\n",
      "Batch:\t7 /13\t:  1.266180 10.45934 \tl:4.947361 \tem:0.177778\n",
      "Batch:\t8 /13\t:  1.327043 11.78700 \tl:5.541101 \tem:0.155556\n",
      "Batch:\t9 /13\t:  1.224018 13.01169 \tl:6.066833 \tem:0.088889\n",
      "Batch:\t10 /13\t:  1.280365 14.29251 \tl:5.188673 \tem:0.133333\n",
      "Batch:\t11 /13\t:  1.234030 15.52757 \tl:5.614320 \tem:0.133333\n",
      "Batch:\t12 /13\t:  1.186121 16.71393 \tl:5.136758 \tem:0.155556\n",
      "\n",
      "Epoch performance:  17.252sec Trl:5.203819 \tTrem:0.165812 \tTeem:0.000000\n",
      "\n",
      "Epoch:  94 / 300\n",
      "Batch:\t0 /13\t:  1.098971 1.098987 \tl:5.500839 \tem:0.111111\n",
      "Batch:\t1 /13\t:  1.198405 2.298156 \tl:5.150079 \tem:0.177778\n",
      "Batch:\t2 /13\t:  1.183437 3.482826 \tl:3.975359 \tem:0.355556\n",
      "Batch:\t3 /13\t:  1.176623 4.659730 \tl:5.595402 \tem:0.177778\n",
      "Batch:\t4 /13\t:  1.262930 5.923717 \tl:5.723473 \tem:0.111111\n",
      "Batch:\t5 /13\t:  1.175696 7.100037 \tl:4.834268 \tem:0.222222\n",
      "Batch:\t6 /13\t:  1.198693 8.299202 \tl:5.467514 \tem:0.155556\n",
      "Batch:\t7 /13\t:  1.142502 9.442520 \tl:5.769698 \tem:0.111111\n",
      "Batch:\t8 /13\t:  1.281313 10.72493 \tl:4.905443 \tem:0.177778\n",
      "Batch:\t9 /13\t:  1.191492 11.91711 \tl:5.770069 \tem:0.133333\n",
      "Batch:\t10 /13\t:  1.329291 13.24698 \tl:5.311346 \tem:0.177778\n",
      "Batch:\t11 /13\t:  1.248377 14.49623 \tl:5.752751 \tem:0.155556\n",
      "Batch:\t12 /13\t:  1.305281 15.80240 \tl:5.182206 \tem:0.155556\n",
      "\n",
      "Epoch performance:  16.339sec Trl:5.302957 \tTrem:0.170940 \tTeem:0.000000\n",
      "\n",
      "Epoch:  95 / 300\n",
      "Batch:\t0 /13\t:  1.188992 1.189007 \tl:4.945158 \tem:0.311111\n",
      "Batch:\t1 /13\t:  1.279505 2.469285 \tl:5.011568 \tem:0.244444\n",
      "Batch:\t2 /13\t:  1.240494 3.710489 \tl:5.513959 \tem:0.155556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t3 /13\t:  1.171072 4.882639 \tl:4.715835 \tem:0.177778\n",
      "Batch:\t4 /13\t:  1.361136 6.244310 \tl:5.037410 \tem:0.244444\n",
      "Batch:\t5 /13\t:  1.306435 7.551969 \tl:5.342187 \tem:0.155556\n",
      "Batch:\t6 /13\t:  1.274074 8.826258 \tl:4.653421 \tem:0.200000\n",
      "Batch:\t7 /13\t:  1.164460 9.991318 \tl:5.082252 \tem:0.266667\n",
      "Batch:\t8 /13\t:  1.253479 11.24532 \tl:5.206324 \tem:0.200000\n",
      "Batch:\t9 /13\t:  1.181699 12.42729 \tl:4.780197 \tem:0.266667\n",
      "Batch:\t10 /13\t:  1.256238 13.68444 \tl:4.803442 \tem:0.288889\n",
      "Batch:\t11 /13\t:  1.328116 15.01306 \tl:5.983500 \tem:0.133333\n",
      "Batch:\t12 /13\t:  1.197741 16.21153 \tl:5.110669 \tem:0.155556\n",
      "\n",
      "Epoch performance:  16.749sec Trl:5.091225 \tTrem:0.215385 \tTeem:0.000000\n",
      "\n",
      "Epoch:  96 / 300\n",
      "Batch:\t0 /13\t:  1.177964 1.177985 \tl:4.587953 \tem:0.288889\n",
      "Batch:\t1 /13\t:  1.299376 2.477761 \tl:4.565787 \tem:0.222222\n",
      "Batch:\t2 /13\t:  1.227925 3.706420 \tl:5.467566 \tem:0.133333\n",
      "Batch:\t3 /13\t:  1.144292 4.851366 \tl:4.492886 \tem:0.355556\n",
      "Batch:\t4 /13\t:  1.316185 6.169875 \tl:4.705434 \tem:0.222222\n",
      "Batch:\t5 /13\t:  1.287700 7.457888 \tl:5.196357 \tem:0.200000\n",
      "Batch:\t6 /13\t:  1.256457 8.714796 \tl:5.332015 \tem:0.155556\n",
      "Batch:\t7 /13\t:  1.212059 9.928524 \tl:4.318806 \tem:0.355556\n",
      "Batch:\t8 /13\t:  1.180807 11.11064 \tl:5.367501 \tem:0.111111\n",
      "Batch:\t9 /13\t:  1.217124 12.32830 \tl:5.391952 \tem:0.111111\n",
      "Batch:\t10 /13\t:  1.291976 13.62074 \tl:6.086127 \tem:0.177778\n",
      "Batch:\t11 /13\t:  1.331862 14.95306 \tl:5.170575 \tem:0.222222\n",
      "Batch:\t12 /13\t:  1.602933 16.55689 \tl:5.302030 \tem:0.222222\n",
      "\n",
      "Epoch performance:  17.093sec Trl:5.075768 \tTrem:0.213675 \tTeem:0.000000\n",
      "\n",
      "Epoch:  97 / 300\n",
      "Batch:\t0 /13\t:  1.466164 1.466179 \tl:5.780393 \tem:0.155556\n",
      "Batch:\t1 /13\t:  1.521130 2.988183 \tl:4.999500 \tem:0.222222\n",
      "Batch:\t2 /13\t:  1.208151 4.196779 \tl:5.178404 \tem:0.177778\n",
      "Batch:\t3 /13\t:  1.201899 5.399865 \tl:5.491047 \tem:0.177778\n",
      "Batch:\t4 /13\t:  1.185919 6.586858 \tl:5.408422 \tem:0.155556\n",
      "Batch:\t5 /13\t:  1.183906 7.771234 \tl:5.543587 \tem:0.088889\n",
      "Batch:\t6 /13\t:  1.201048 8.972996 \tl:5.416921 \tem:0.111111\n",
      "Batch:\t7 /13\t:  1.185137 10.15880 \tl:4.788792 \tem:0.177778\n",
      "Batch:\t8 /13\t:  1.283773 11.44308 \tl:4.597522 \tem:0.155556\n",
      "Batch:\t9 /13\t:  1.137257 12.58083 \tl:4.774570 \tem:0.155556\n",
      "Batch:\t10 /13\t:  1.209808 13.79139 \tl:5.800568 \tem:0.088889\n",
      "Batch:\t11 /13\t:  1.257685 15.04965 \tl:5.383306 \tem:0.088889\n",
      "Batch:\t12 /13\t:  1.173156 16.22345 \tl:5.277534 \tem:0.133333\n",
      "\n",
      "Epoch performance:  16.761sec Trl:5.264659 \tTrem:0.145299 \tTeem:0.000000\n",
      "\n",
      "Epoch:  98 / 300\n",
      "Batch:\t0 /13\t:  1.145001 1.145019 \tl:5.193920 \tem:0.200000\n",
      "Batch:\t1 /13\t:  1.160794 2.306374 \tl:5.327662 \tem:0.177778\n",
      "Batch:\t2 /13\t:  1.206558 3.513437 \tl:5.868233 \tem:0.088889\n",
      "Batch:\t3 /13\t:  1.167034 4.681273 \tl:5.442240 \tem:0.200000\n",
      "Batch:\t4 /13\t:  1.275071 5.957236 \tl:5.088359 \tem:0.200000\n",
      "Batch:\t5 /13\t:  1.163002 7.121000 \tl:4.944873 \tem:0.155556\n",
      "Batch:\t6 /13\t:  1.265215 8.387058 \tl:5.229408 \tem:0.155556\n",
      "Batch:\t7 /13\t:  1.205214 9.593193 \tl:4.960076 \tem:0.155556\n",
      "Batch:\t8 /13\t:  1.130059 10.72370 \tl:4.787877 \tem:0.244444\n",
      "Batch:\t9 /13\t:  1.312155 12.03645 \tl:4.723162 \tem:0.177778\n",
      "Batch:\t10 /13\t:  1.140578 13.17836 \tl:4.767936 \tem:0.222222\n",
      "Batch:\t11 /13\t:  1.340476 14.51948 \tl:5.460824 \tem:0.177778\n",
      "Batch:\t12 /13\t:  1.184064 15.70405 \tl:4.151030 \tem:0.288889\n",
      "\n",
      "Epoch performance:  16.240sec Trl:5.072739 \tTrem:0.188034 \tTeem:0.000000\n",
      "\n",
      "Epoch:  99 / 300\n",
      "Batch:\t0 /13\t:  1.285777 1.285790 \tl:4.496309 \tem:0.266667\n",
      "Batch:\t1 /13\t:  1.226547 2.513341 \tl:5.961111 \tem:0.133333\n",
      "Batch:\t2 /13\t:  1.262043 3.776458 \tl:5.001249 \tem:0.111111\n",
      "Batch:\t3 /13\t:  1.113413 4.890619 \tl:4.624863 \tem:0.222222\n",
      "Batch:\t4 /13\t:  1.249709 6.140531 \tl:5.151416 \tem:0.155556\n",
      "Batch:\t5 /13\t:  1.184010 7.325093 \tl:5.284187 \tem:0.133333\n",
      "Batch:\t6 /13\t:  1.261981 8.587265 \tl:4.530550 \tem:0.200000\n",
      "Batch:\t7 /13\t:  1.188205 9.776350 \tl:4.820588 \tem:0.177778\n",
      "Batch:\t8 /13\t:  1.285467 11.06213 \tl:5.023803 \tem:0.222222\n",
      "Batch:\t9 /13\t:  1.255104 12.31792 \tl:4.789953 \tem:0.266667\n",
      "Batch:\t10 /13\t:  1.323103 13.64151 \tl:4.941032 \tem:0.133333\n",
      "Batch:\t11 /13\t:  1.373025 15.01533 \tl:4.967417 \tem:0.066667\n",
      "Batch:\t12 /13\t:  1.113152 16.12904 \tl:4.627617 \tem:0.311111\n",
      "\n",
      "Epoch performance:  16.666sec Trl:4.940007 \tTrem:0.184615 \tTeem:0.000000\n",
      "\n",
      "Epoch:  100 / 300\n",
      "Batch:\t0 /13\t:  1.280159 1.280175 \tl:5.196147 \tem:0.244444\n",
      "Batch:\t1 /13\t:  1.140681 2.421380 \tl:4.691128 \tem:0.311111\n",
      "Batch:\t2 /13\t:  1.214708 3.637212 \tl:4.213026 \tem:0.244444\n",
      "Batch:\t3 /13\t:  1.201606 4.839818 \tl:4.185758 \tem:0.266667\n",
      "Batch:\t4 /13\t:  1.268463 6.108681 \tl:4.852796 \tem:0.311111\n",
      "Batch:\t5 /13\t:  1.250189 7.359260 \tl:4.802212 \tem:0.200000\n",
      "Batch:\t6 /13\t:  1.192924 8.552690 \tl:4.237571 \tem:0.222222\n",
      "Batch:\t7 /13\t:  1.196350 9.749551 \tl:4.942519 \tem:0.155556\n",
      "Batch:\t8 /13\t:  1.168096 10.91813 \tl:4.364263 \tem:0.266667\n",
      "Batch:\t9 /13\t:  1.197402 12.11614 \tl:4.599848 \tem:0.222222\n",
      "Batch:\t10 /13\t:  1.175847 13.29249 \tl:4.661471 \tem:0.333333\n",
      "Batch:\t11 /13\t:  1.176511 14.46981 \tl:4.964023 \tem:0.222222\n",
      "Batch:\t12 /13\t:  1.257700 15.72815 \tl:6.358669 \tem:0.155556\n",
      "\n",
      "Epoch performance:  16.265sec Trl:4.774572 \tTrem:0.242735 \tTeem:0.000000\n",
      "\n",
      "Epoch:  101 / 300\n",
      "Batch:\t0 /13\t:  1.226900 1.226917 \tl:4.904945 \tem:0.200000\n",
      "Batch:\t1 /13\t:  1.255362 2.482778 \tl:4.140278 \tem:0.288889\n",
      "Batch:\t2 /13\t:  1.206367 3.690109 \tl:4.609363 \tem:0.200000\n",
      "Batch:\t3 /13\t:  1.200579 4.891117 \tl:3.844764 \tem:0.266667\n",
      "Batch:\t4 /13\t:  1.168195 6.060350 \tl:5.083544 \tem:0.177778\n",
      "Batch:\t5 /13\t:  1.198800 7.259382 \tl:4.558661 \tem:0.244444\n",
      "Batch:\t6 /13\t:  1.186236 8.446310 \tl:5.632184 \tem:0.266667\n",
      "Batch:\t7 /13\t:  1.190500 9.637698 \tl:5.057407 \tem:0.244444\n",
      "Batch:\t8 /13\t:  1.314527 10.95245 \tl:5.433025 \tem:0.177778\n",
      "Batch:\t9 /13\t:  1.291378 12.24470 \tl:5.358845 \tem:0.155556\n",
      "Batch:\t10 /13\t:  1.228266 13.47398 \tl:5.316475 \tem:0.244444\n",
      "Batch:\t11 /13\t:  1.272572 14.74759 \tl:4.921086 \tem:0.133333\n",
      "Batch:\t12 /13\t:  1.241297 15.98949 \tl:5.752572 \tem:0.133333\n",
      "\n",
      "Epoch performance:  16.527sec Trl:4.970242 \tTrem:0.210256 \tTeem:0.000000\n",
      "\n",
      "Epoch:  102 / 300\n",
      "Batch:\t0 /13\t:  1.192477 1.192494 \tl:4.542557 \tem:0.266667\n",
      "Batch:\t1 /13\t:  1.647260 2.840401 \tl:4.622643 \tem:0.311111\n",
      "Batch:\t2 /13\t:  1.518521 4.359761 \tl:3.867975 \tem:0.400000\n",
      "Batch:\t3 /13\t:  1.274649 5.635614 \tl:5.257892 \tem:0.155556\n",
      "Batch:\t4 /13\t:  1.200406 6.836639 \tl:5.311205 \tem:0.155556\n",
      "Batch:\t5 /13\t:  1.237246 8.074740 \tl:3.904349 \tem:0.311111\n",
      "Batch:\t6 /13\t:  1.205951 9.281158 \tl:4.625443 \tem:0.200000\n",
      "Batch:\t7 /13\t:  1.249263 10.53080 \tl:5.553803 \tem:0.222222\n",
      "Batch:\t8 /13\t:  1.209131 11.74076 \tl:4.742131 \tem:0.222222\n",
      "Batch:\t9 /13\t:  1.202777 12.94417 \tl:4.771249 \tem:0.200000\n",
      "Batch:\t10 /13\t:  1.209618 14.15470 \tl:4.902597 \tem:0.155556\n",
      "Batch:\t11 /13\t:  1.166835 15.32236 \tl:4.803540 \tem:0.266667\n",
      "Batch:\t12 /13\t:  1.267968 16.59084 \tl:4.862647 \tem:0.244444\n",
      "\n",
      "Epoch performance:  17.129sec Trl:4.751387 \tTrem:0.239316 \tTeem:0.000000\n",
      "\n",
      "Epoch:  103 / 300\n",
      "Batch:\t0 /13\t:  1.322470 1.322487 \tl:5.214597 \tem:0.177778\n",
      "Batch:\t1 /13\t:  1.156639 2.479855 \tl:5.303440 \tem:0.200000\n",
      "Batch:\t2 /13\t:  1.163527 3.644181 \tl:4.951572 \tem:0.177778\n",
      "Batch:\t3 /13\t:  1.298002 4.942986 \tl:5.188039 \tem:0.177778\n",
      "Batch:\t4 /13\t:  1.200469 6.144790 \tl:5.482008 \tem:0.155556\n",
      "Batch:\t5 /13\t:  1.228543 7.373768 \tl:4.974733 \tem:0.222222\n",
      "Batch:\t6 /13\t:  1.163795 8.538052 \tl:4.544721 \tem:0.266667\n",
      "Batch:\t7 /13\t:  1.251140 9.789894 \tl:5.704552 \tem:0.066667\n",
      "Batch:\t8 /13\t:  1.188786 10.97927 \tl:4.608652 \tem:0.177778\n",
      "Batch:\t9 /13\t:  1.309120 12.28884 \tl:5.349952 \tem:0.155556\n",
      "Batch:\t10 /13\t:  1.245359 13.53442 \tl:3.863794 \tem:0.311111\n",
      "Batch:\t11 /13\t:  1.187379 14.72247 \tl:5.293850 \tem:0.133333\n",
      "Batch:\t12 /13\t:  1.445602 16.16832 \tl:5.517436 \tem:0.111111\n",
      "\n",
      "Epoch performance:  16.706sec Trl:5.076719 \tTrem:0.179487 \tTeem:0.000000\n",
      "\n",
      "Epoch:  104 / 300\n",
      "Batch:\t0 /13\t:  1.356993 1.357011 \tl:4.347446 \tem:0.266667\n",
      "Batch:\t1 /13\t:  1.322469 2.680071 \tl:4.567369 \tem:0.244444\n",
      "Batch:\t2 /13\t:  1.159145 3.840286 \tl:4.855466 \tem:0.177778\n",
      "Batch:\t3 /13\t:  1.309191 5.150410 \tl:5.437078 \tem:0.133333\n",
      "Batch:\t4 /13\t:  1.210521 6.361845 \tl:4.175751 \tem:0.244444\n",
      "Batch:\t5 /13\t:  1.215753 7.578531 \tl:4.451459 \tem:0.177778\n",
      "Batch:\t6 /13\t:  1.185768 8.764965 \tl:4.368488 \tem:0.244444\n",
      "Batch:\t7 /13\t:  1.316380 10.08209 \tl:4.838779 \tem:0.200000\n",
      "Batch:\t8 /13\t:  1.175924 11.25879 \tl:5.053927 \tem:0.222222\n",
      "Batch:\t9 /13\t:  1.269225 12.52868 \tl:5.851720 \tem:0.155556\n",
      "Batch:\t10 /13\t:  1.167439 13.69751 \tl:4.725236 \tem:0.244444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t11 /13\t:  1.189896 14.88834 \tl:5.529966 \tem:0.133333\n",
      "Batch:\t12 /13\t:  1.205585 16.09448 \tl:4.955268 \tem:0.155556\n",
      "\n",
      "Epoch performance:  16.631sec Trl:4.858304 \tTrem:0.200000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  105 / 300\n",
      "Batch:\t0 /13\t:  1.311851 1.311866 \tl:4.701422 \tem:0.266667\n",
      "Batch:\t1 /13\t:  1.236974 2.549287 \tl:5.242278 \tem:0.222222\n",
      "Batch:\t2 /13\t:  1.244404 3.794600 \tl:4.643709 \tem:0.244444\n",
      "Batch:\t3 /13\t:  1.207047 5.002568 \tl:3.906671 \tem:0.333333\n",
      "Batch:\t4 /13\t:  1.145272 6.149025 \tl:4.561994 \tem:0.288889\n",
      "Batch:\t5 /13\t:  1.314615 7.464228 \tl:5.159281 \tem:0.200000\n",
      "Batch:\t6 /13\t:  1.166702 8.631150 \tl:5.453938 \tem:0.177778\n",
      "Batch:\t7 /13\t:  1.162693 9.794074 \tl:5.307329 \tem:0.200000\n",
      "Batch:\t8 /13\t:  1.245194 11.03994 \tl:3.907757 \tem:0.311111\n",
      "Batch:\t9 /13\t:  1.252132 12.29281 \tl:4.388330 \tem:0.244444\n",
      "Batch:\t10 /13\t:  1.261307 13.55459 \tl:4.710231 \tem:0.222222\n",
      "Batch:\t11 /13\t:  1.170108 14.72588 \tl:4.670554 \tem:0.200000\n",
      "Batch:\t12 /13\t:  1.197556 15.92490 \tl:3.793766 \tem:0.222222\n",
      "\n",
      "Epoch performance:  16.462sec Trl:4.649789 \tTrem:0.241026 \tTeem:0.000000\n",
      "\n",
      "Epoch:  106 / 300\n",
      "Batch:\t0 /13\t:  1.309939 1.309959 \tl:3.734702 \tem:0.311111\n",
      "Batch:\t1 /13\t:  1.234946 2.545950 \tl:4.971427 \tem:0.222222\n",
      "Batch:\t2 /13\t:  1.181202 3.727416 \tl:5.695003 \tem:0.222222\n",
      "Batch:\t3 /13\t:  1.192986 4.920826 \tl:5.047072 \tem:0.244444\n",
      "Batch:\t4 /13\t:  1.321314 6.242854 \tl:5.305949 \tem:0.133333\n",
      "Batch:\t5 /13\t:  1.151275 7.394731 \tl:4.465177 \tem:0.222222\n",
      "Batch:\t6 /13\t:  1.245990 8.641172 \tl:4.443419 \tem:0.200000\n",
      "Batch:\t7 /13\t:  1.225776 9.868138 \tl:5.643357 \tem:0.200000\n",
      "Batch:\t8 /13\t:  1.285443 11.15419 \tl:4.876987 \tem:0.200000\n",
      "Batch:\t9 /13\t:  1.244131 12.39938 \tl:4.558407 \tem:0.244444\n",
      "Batch:\t10 /13\t:  1.210164 13.61079 \tl:4.740753 \tem:0.244444\n",
      "Batch:\t11 /13\t:  1.136327 14.74732 \tl:5.247731 \tem:0.177778\n",
      "Batch:\t12 /13\t:  1.175793 15.92384 \tl:4.160542 \tem:0.222222\n",
      "\n",
      "Epoch performance:  16.465sec Trl:4.837733 \tTrem:0.218803 \tTeem:0.000000\n",
      "\n",
      "Epoch:  107 / 300\n",
      "Batch:\t0 /13\t:  1.219848 1.219865 \tl:4.055064 \tem:0.266667\n",
      "Batch:\t1 /13\t:  1.229452 2.449896 \tl:4.478703 \tem:0.311111\n",
      "Batch:\t2 /13\t:  1.124178 3.574485 \tl:3.829153 \tem:0.266667\n",
      "Batch:\t3 /13\t:  1.202340 4.777362 \tl:4.062575 \tem:0.288889\n",
      "Batch:\t4 /13\t:  1.203494 5.981384 \tl:4.760328 \tem:0.288889\n",
      "Batch:\t5 /13\t:  1.267684 7.249716 \tl:4.909906 \tem:0.155556\n",
      "Batch:\t6 /13\t:  1.237738 8.488229 \tl:4.437921 \tem:0.222222\n",
      "Batch:\t7 /13\t:  1.180534 9.669366 \tl:5.357360 \tem:0.155556\n",
      "Batch:\t8 /13\t:  1.202946 10.87284 \tl:4.997706 \tem:0.155556\n",
      "Batch:\t9 /13\t:  1.346675 12.22020 \tl:3.803522 \tem:0.377778\n",
      "Batch:\t10 /13\t:  1.302133 13.52347 \tl:6.030397 \tem:0.155556\n",
      "Batch:\t11 /13\t:  1.361791 14.88610 \tl:4.427491 \tem:0.244444\n",
      "Batch:\t12 /13\t:  1.275393 16.16285 \tl:4.850760 \tem:0.200000\n",
      "\n",
      "Epoch performance:  16.699sec Trl:4.615453 \tTrem:0.237607 \tTeem:0.000000\n",
      "\n",
      "Epoch:  108 / 300\n",
      "Batch:\t0 /13\t:  1.161388 1.161403 \tl:5.256914 \tem:0.155556\n",
      "Batch:\t1 /13\t:  1.169041 2.330991 \tl:4.444322 \tem:0.288889\n",
      "Batch:\t2 /13\t:  1.247712 3.579120 \tl:3.985774 \tem:0.400000\n",
      "Batch:\t3 /13\t:  1.228960 4.808495 \tl:4.416305 \tem:0.311111\n",
      "Batch:\t4 /13\t:  1.269638 6.078658 \tl:5.097153 \tem:0.177778\n",
      "Batch:\t5 /13\t:  1.235714 7.315045 \tl:4.885149 \tem:0.222222\n",
      "Batch:\t6 /13\t:  1.300987 8.616708 \tl:4.099232 \tem:0.311111\n",
      "Batch:\t7 /13\t:  1.163115 9.781053 \tl:5.232817 \tem:0.200000\n",
      "Batch:\t8 /13\t:  1.284759 11.06692 \tl:4.532155 \tem:0.133333\n",
      "Batch:\t9 /13\t:  1.262171 12.32935 \tl:4.300832 \tem:0.288889\n",
      "Batch:\t10 /13\t:  1.274181 13.60477 \tl:4.212079 \tem:0.355556\n",
      "Batch:\t11 /13\t:  1.192990 14.79888 \tl:4.701886 \tem:0.177778\n",
      "Batch:\t12 /13\t:  1.215589 16.01491 \tl:5.114490 \tem:0.111111\n",
      "\n",
      "Epoch performance:  16.551sec Trl:4.636854 \tTrem:0.241026 \tTeem:0.000000\n",
      "\n",
      "Epoch:  109 / 300\n",
      "Batch:\t0 /13\t:  1.290827 1.290843 \tl:4.747463 \tem:0.266667\n",
      "Batch:\t1 /13\t:  1.170860 2.462273 \tl:3.738769 \tem:0.377778\n",
      "Batch:\t2 /13\t:  1.244272 3.706975 \tl:5.291977 \tem:0.244444\n",
      "Batch:\t3 /13\t:  1.243605 4.950876 \tl:4.588489 \tem:0.377778\n",
      "Batch:\t4 /13\t:  1.229413 6.181541 \tl:4.817149 \tem:0.200000\n",
      "Batch:\t5 /13\t:  1.185804 7.368331 \tl:4.582125 \tem:0.244444\n",
      "Batch:\t6 /13\t:  1.215458 8.584369 \tl:4.873028 \tem:0.266667\n",
      "Batch:\t7 /13\t:  1.230241 9.816980 \tl:4.884861 \tem:0.288889\n",
      "Batch:\t8 /13\t:  1.199724 11.03054 \tl:4.664222 \tem:0.288889\n",
      "Batch:\t9 /13\t:  1.190751 12.22182 \tl:4.090546 \tem:0.333333\n",
      "Batch:\t10 /13\t:  1.187114 13.40941 \tl:4.497183 \tem:0.311111\n",
      "Batch:\t11 /13\t:  1.224566 14.63472 \tl:4.873953 \tem:0.200000\n",
      "Batch:\t12 /13\t:  1.192298 15.82725 \tl:5.226929 \tem:0.133333\n",
      "\n",
      "Epoch performance:  16.367sec Trl:4.682823 \tTrem:0.271795 \tTeem:0.000000\n",
      "\n",
      "Epoch:  110 / 300\n",
      "Batch:\t0 /13\t:  1.296270 1.296288 \tl:4.699931 \tem:0.288889\n",
      "Batch:\t1 /13\t:  1.161563 2.458333 \tl:4.532627 \tem:0.288889\n",
      "Batch:\t2 /13\t:  1.259249 3.718140 \tl:5.496212 \tem:0.200000\n",
      "Batch:\t3 /13\t:  1.245043 4.963943 \tl:4.537412 \tem:0.311111\n",
      "Batch:\t4 /13\t:  1.254624 6.219208 \tl:3.341260 \tem:0.377778\n",
      "Batch:\t5 /13\t:  1.189476 7.409906 \tl:4.503471 \tem:0.288889\n",
      "Batch:\t6 /13\t:  1.289083 8.699888 \tl:4.686743 \tem:0.222222\n",
      "Batch:\t7 /13\t:  1.204735 9.905730 \tl:4.210557 \tem:0.177778\n",
      "Batch:\t8 /13\t:  1.196640 11.10302 \tl:4.060540 \tem:0.311111\n",
      "Batch:\t9 /13\t:  1.207435 12.31131 \tl:5.096158 \tem:0.155556\n",
      "Batch:\t10 /13\t:  1.202686 13.51445 \tl:4.354209 \tem:0.222222\n",
      "Batch:\t11 /13\t:  1.285394 14.80011 \tl:3.373813 \tem:0.311111\n",
      "Batch:\t12 /13\t:  1.334900 16.13550 \tl:4.532887 \tem:0.266667\n",
      "\n",
      "Epoch performance:  16.674sec Trl:4.417371 \tTrem:0.263248 \tTeem:0.000000\n",
      "\n",
      "Epoch:  111 / 300\n",
      "Batch:\t0 /13\t:  1.151209 1.151225 \tl:3.585405 \tem:0.311111\n",
      "Batch:\t1 /13\t:  1.169455 2.320924 \tl:5.183673 \tem:0.222222\n",
      "Batch:\t2 /13\t:  1.250227 3.571428 \tl:4.851225 \tem:0.244444\n",
      "Batch:\t3 /13\t:  1.173998 4.746044 \tl:4.247086 \tem:0.311111\n",
      "Batch:\t4 /13\t:  1.191329 5.937879 \tl:4.118027 \tem:0.377778\n",
      "Batch:\t5 /13\t:  1.289859 7.228363 \tl:3.045446 \tem:0.333333\n",
      "Batch:\t6 /13\t:  1.253194 8.482124 \tl:4.579632 \tem:0.222222\n",
      "Batch:\t7 /13\t:  1.171455 9.653961 \tl:3.928582 \tem:0.377778\n",
      "Batch:\t8 /13\t:  1.197427 10.85220 \tl:4.658699 \tem:0.333333\n",
      "Batch:\t9 /13\t:  1.260121 12.11287 \tl:4.349584 \tem:0.200000\n",
      "Batch:\t10 /13\t:  1.161687 13.27558 \tl:4.648248 \tem:0.133333\n",
      "Batch:\t11 /13\t:  1.327035 14.60323 \tl:4.494573 \tem:0.200000\n",
      "Batch:\t12 /13\t:  1.247354 15.85195 \tl:4.224486 \tem:0.177778\n",
      "\n",
      "Epoch performance:  16.390sec Trl:4.301128 \tTrem:0.264957 \tTeem:0.000000\n",
      "\n",
      "Epoch:  112 / 300\n",
      "Batch:\t0 /13\t:  1.290991 1.291007 \tl:4.712654 \tem:0.155556\n",
      "Batch:\t1 /13\t:  1.122354 2.414236 \tl:4.849007 \tem:0.266667\n",
      "Batch:\t2 /13\t:  1.165780 3.580358 \tl:4.455988 \tem:0.288889\n",
      "Batch:\t3 /13\t:  1.195420 4.776703 \tl:4.676274 \tem:0.244444\n",
      "Batch:\t4 /13\t:  1.210973 5.988101 \tl:3.549571 \tem:0.355556\n",
      "Batch:\t5 /13\t:  1.200442 7.189160 \tl:4.029356 \tem:0.333333\n",
      "Batch:\t6 /13\t:  1.166551 8.356295 \tl:3.740725 \tem:0.311111\n",
      "Batch:\t7 /13\t:  1.211943 9.569039 \tl:4.650075 \tem:0.311111\n",
      "Batch:\t8 /13\t:  1.241615 10.81165 \tl:4.094485 \tem:0.244444\n",
      "Batch:\t9 /13\t:  1.290176 12.10209 \tl:5.207042 \tem:0.222222\n",
      "Batch:\t10 /13\t:  1.230326 13.33299 \tl:3.719941 \tem:0.266667\n",
      "Batch:\t11 /13\t:  1.213093 14.54732 \tl:4.384729 \tem:0.222222\n",
      "Batch:\t12 /13\t:  1.315243 15.86369 \tl:4.760740 \tem:0.177778\n",
      "\n",
      "Epoch performance:  16.400sec Trl:4.371584 \tTrem:0.261538 \tTeem:0.000000\n",
      "\n",
      "Epoch:  113 / 300\n",
      "Batch:\t0 /13\t:  1.224362 1.224377 \tl:5.155943 \tem:0.200000\n",
      "Batch:\t1 /13\t:  1.194877 2.419742 \tl:4.419864 \tem:0.288889\n",
      "Batch:\t2 /13\t:  1.318136 3.738188 \tl:4.971457 \tem:0.222222\n",
      "Batch:\t3 /13\t:  1.200259 4.939738 \tl:3.392264 \tem:0.311111\n",
      "Batch:\t4 /13\t:  1.296533 6.236813 \tl:4.321352 \tem:0.311111\n",
      "Batch:\t5 /13\t:  1.212242 7.449623 \tl:4.315129 \tem:0.200000\n",
      "Batch:\t6 /13\t:  1.166293 8.616552 \tl:4.368131 \tem:0.333333\n",
      "Batch:\t7 /13\t:  1.261209 9.878277 \tl:4.598212 \tem:0.288889\n",
      "Batch:\t8 /13\t:  1.138030 11.01740 \tl:5.115384 \tem:0.244444\n",
      "Batch:\t9 /13\t:  1.268779 12.28694 \tl:4.597138 \tem:0.244444\n",
      "Batch:\t10 /13\t:  1.264668 13.55298 \tl:4.283659 \tem:0.311111\n",
      "Batch:\t11 /13\t:  1.198446 14.75192 \tl:3.535449 \tem:0.400000\n",
      "Batch:\t12 /13\t:  1.266032 16.01855 \tl:3.806614 \tem:0.311111\n",
      "\n",
      "Epoch performance:  16.556sec Trl:4.375430 \tTrem:0.282051 \tTeem:0.000000\n",
      "\n",
      "Epoch:  114 / 300\n",
      "Batch:\t0 /13\t:  1.169283 1.169301 \tl:4.610301 \tem:0.311111\n",
      "Batch:\t1 /13\t:  1.199669 2.369463 \tl:4.320383 \tem:0.333333\n",
      "Batch:\t2 /13\t:  1.209964 3.580216 \tl:4.621778 \tem:0.155556\n",
      "Batch:\t3 /13\t:  1.226756 4.807859 \tl:3.987936 \tem:0.311111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t4 /13\t:  1.160604 5.969398 \tl:5.349213 \tem:0.155556\n",
      "Batch:\t5 /13\t:  1.254004 7.223613 \tl:4.194384 \tem:0.311111\n",
      "Batch:\t6 /13\t:  1.178080 8.402513 \tl:4.104772 \tem:0.244444\n",
      "Batch:\t7 /13\t:  1.310388 9.713500 \tl:4.167998 \tem:0.288889\n",
      "Batch:\t8 /13\t:  1.322471 11.03622 \tl:4.045462 \tem:0.355556\n",
      "Batch:\t9 /13\t:  1.232204 12.26859 \tl:3.029608 \tem:0.444444\n",
      "Batch:\t10 /13\t:  1.165148 13.43449 \tl:3.941019 \tem:0.311111\n",
      "Batch:\t11 /13\t:  1.172557 14.60768 \tl:3.625957 \tem:0.355556\n",
      "Batch:\t12 /13\t:  1.222868 15.83127 \tl:3.988453 \tem:0.288889\n",
      "\n",
      "Epoch performance:  16.368sec Trl:4.152866 \tTrem:0.297436 \tTeem:0.000000\n",
      "\n",
      "Epoch:  115 / 300\n",
      "Batch:\t0 /13\t:  1.163897 1.163913 \tl:4.124322 \tem:0.333333\n",
      "Batch:\t1 /13\t:  1.283920 2.448961 \tl:4.636970 \tem:0.266667\n",
      "Batch:\t2 /13\t:  1.582226 4.031661 \tl:4.827706 \tem:0.333333\n",
      "Batch:\t3 /13\t:  1.316382 5.348277 \tl:4.536469 \tem:0.266667\n",
      "Batch:\t4 /13\t:  1.186817 6.535356 \tl:4.082763 \tem:0.266667\n",
      "Batch:\t5 /13\t:  1.280141 7.816418 \tl:4.152645 \tem:0.244444\n",
      "Batch:\t6 /13\t:  1.276002 9.093049 \tl:4.727196 \tem:0.244444\n",
      "Batch:\t7 /13\t:  1.345373 10.43907 \tl:4.920661 \tem:0.266667\n",
      "Batch:\t8 /13\t:  1.180572 11.62024 \tl:4.131875 \tem:0.288889\n",
      "Batch:\t9 /13\t:  1.228168 12.84870 \tl:4.619631 \tem:0.266667\n",
      "Batch:\t10 /13\t:  1.163552 14.01277 \tl:4.025259 \tem:0.311111\n",
      "Batch:\t11 /13\t:  1.197518 15.21092 \tl:4.076667 \tem:0.377778\n",
      "Batch:\t12 /13\t:  1.400219 16.61184 \tl:3.448128 \tem:0.333333\n",
      "\n",
      "Epoch performance:  17.149sec Trl:4.331561 \tTrem:0.292308 \tTeem:0.000000\n",
      "\n",
      "Epoch:  116 / 300\n",
      "Batch:\t0 /13\t:  1.200407 1.200426 \tl:5.109752 \tem:0.222222\n",
      "Batch:\t1 /13\t:  1.188215 2.389296 \tl:3.884830 \tem:0.311111\n",
      "Batch:\t2 /13\t:  1.208037 3.597532 \tl:4.680688 \tem:0.177778\n",
      "Batch:\t3 /13\t:  1.205555 4.803867 \tl:4.377638 \tem:0.244444\n",
      "Batch:\t4 /13\t:  1.422133 6.226514 \tl:4.429369 \tem:0.244444\n",
      "Batch:\t5 /13\t:  1.269705 7.496870 \tl:4.266678 \tem:0.288889\n",
      "Batch:\t6 /13\t:  1.198912 8.696894 \tl:3.894945 \tem:0.333333\n",
      "Batch:\t7 /13\t:  1.542165 10.23928 \tl:4.495239 \tem:0.311111\n",
      "Batch:\t8 /13\t:  1.290124 11.53022 \tl:3.972159 \tem:0.311111\n",
      "Batch:\t9 /13\t:  1.207049 12.73817 \tl:4.621678 \tem:0.244444\n",
      "Batch:\t10 /13\t:  1.159072 13.89768 \tl:3.850358 \tem:0.311111\n",
      "Batch:\t11 /13\t:  1.270499 15.16848 \tl:4.443241 \tem:0.222222\n",
      "Batch:\t12 /13\t:  1.207027 16.37619 \tl:4.283771 \tem:0.222222\n",
      "\n",
      "Epoch performance:  16.916sec Trl:4.331565 \tTrem:0.264957 \tTeem:0.000000\n",
      "\n",
      "Epoch:  117 / 300\n",
      "Batch:\t0 /13\t:  1.287328 1.287346 \tl:4.730518 \tem:0.155556\n",
      "Batch:\t1 /13\t:  1.255599 2.543143 \tl:5.118471 \tem:0.177778\n",
      "Batch:\t2 /13\t:  1.302359 3.846212 \tl:4.729150 \tem:0.244444\n",
      "Batch:\t3 /13\t:  1.300297 5.147095 \tl:3.929274 \tem:0.377778\n",
      "Batch:\t4 /13\t:  1.263334 6.410656 \tl:4.452791 \tem:0.177778\n",
      "Batch:\t5 /13\t:  1.209379 7.620555 \tl:4.031451 \tem:0.266667\n",
      "Batch:\t6 /13\t:  1.284627 8.905955 \tl:4.044231 \tem:0.355556\n",
      "Batch:\t7 /13\t:  1.227669 10.13411 \tl:2.896098 \tem:0.444444\n",
      "Batch:\t8 /13\t:  1.162618 11.29730 \tl:4.964529 \tem:0.155556\n",
      "Batch:\t9 /13\t:  1.237692 12.53601 \tl:3.962426 \tem:0.400000\n",
      "Batch:\t10 /13\t:  1.211312 13.74797 \tl:4.106268 \tem:0.444444\n",
      "Batch:\t11 /13\t:  1.197748 14.94653 \tl:5.031989 \tem:0.333333\n",
      "Batch:\t12 /13\t:  1.152294 16.09953 \tl:4.347852 \tem:0.288889\n",
      "\n",
      "Epoch performance:  16.637sec Trl:4.334235 \tTrem:0.294017 \tTeem:0.000000\n",
      "\n",
      "Epoch:  118 / 300\n",
      "Batch:\t0 /13\t:  1.350483 1.350501 \tl:3.931807 \tem:0.422222\n",
      "Batch:\t1 /13\t:  1.174961 2.525749 \tl:4.238083 \tem:0.333333\n",
      "Batch:\t2 /13\t:  1.326873 3.853615 \tl:4.091774 \tem:0.266667\n",
      "Batch:\t3 /13\t:  1.217448 5.071650 \tl:4.207715 \tem:0.288889\n",
      "Batch:\t4 /13\t:  1.186247 6.258285 \tl:4.136217 \tem:0.355556\n",
      "Batch:\t5 /13\t:  1.361188 7.620085 \tl:4.310705 \tem:0.266667\n",
      "Batch:\t6 /13\t:  1.193065 8.814374 \tl:3.912775 \tem:0.377778\n",
      "Batch:\t7 /13\t:  1.242880 10.05768 \tl:5.152327 \tem:0.222222\n",
      "Batch:\t8 /13\t:  1.140100 11.19899 \tl:3.931850 \tem:0.333333\n",
      "Batch:\t9 /13\t:  1.157506 12.35730 \tl:3.374331 \tem:0.355556\n",
      "Batch:\t10 /13\t:  1.276577 13.63447 \tl:3.175673 \tem:0.422222\n",
      "Batch:\t11 /13\t:  1.280019 14.91466 \tl:3.453615 \tem:0.288889\n",
      "Batch:\t12 /13\t:  1.260822 16.17623 \tl:3.974181 \tem:0.355556\n",
      "\n",
      "Epoch performance:  16.713sec Trl:3.991619 \tTrem:0.329915 \tTeem:0.000000\n",
      "\n",
      "Epoch:  119 / 300\n",
      "Batch:\t0 /13\t:  1.288084 1.288100 \tl:3.540530 \tem:0.444444\n",
      "Batch:\t1 /13\t:  1.272346 2.561037 \tl:4.074112 \tem:0.244444\n",
      "Batch:\t2 /13\t:  1.162505 3.724239 \tl:4.743186 \tem:0.288889\n",
      "Batch:\t3 /13\t:  1.259530 4.984289 \tl:4.611455 \tem:0.311111\n",
      "Batch:\t4 /13\t:  1.225127 6.210726 \tl:4.172944 \tem:0.288889\n",
      "Batch:\t5 /13\t:  1.220484 7.432111 \tl:4.548254 \tem:0.244444\n",
      "Batch:\t6 /13\t:  1.156486 8.589282 \tl:4.920316 \tem:0.333333\n",
      "Batch:\t7 /13\t:  1.352479 9.942131 \tl:4.321625 \tem:0.311111\n",
      "Batch:\t8 /13\t:  1.138436 11.08290 \tl:5.158969 \tem:0.155556\n",
      "Batch:\t9 /13\t:  1.299852 12.38334 \tl:4.061372 \tem:0.311111\n",
      "Batch:\t10 /13\t:  1.192909 13.57683 \tl:4.022328 \tem:0.222222\n",
      "Batch:\t11 /13\t:  1.255650 14.83315 \tl:4.519428 \tem:0.244444\n",
      "Batch:\t12 /13\t:  1.212512 16.04612 \tl:4.072530 \tem:0.311111\n",
      "\n",
      "Epoch performance:  16.582sec Trl:4.366696 \tTrem:0.285470 \tTeem:0.000000\n",
      "\n",
      "Epoch:  120 / 300\n",
      "Batch:\t0 /13\t:  1.207166 1.207185 \tl:4.807211 \tem:0.266667\n",
      "Batch:\t1 /13\t:  1.214729 2.422178 \tl:4.965648 \tem:0.177778\n",
      "Batch:\t2 /13\t:  1.119203 3.541856 \tl:4.258054 \tem:0.333333\n",
      "Batch:\t3 /13\t:  1.231683 4.774183 \tl:4.253693 \tem:0.200000\n",
      "Batch:\t4 /13\t:  1.184870 5.960042 \tl:3.949783 \tem:0.266667\n",
      "Batch:\t5 /13\t:  1.251239 7.212162 \tl:4.017066 \tem:0.355556\n",
      "Batch:\t6 /13\t:  1.160210 8.372822 \tl:4.040372 \tem:0.200000\n",
      "Batch:\t7 /13\t:  1.213860 9.587208 \tl:3.294681 \tem:0.466667\n",
      "Batch:\t8 /13\t:  1.217046 10.80453 \tl:4.566626 \tem:0.244444\n",
      "Batch:\t9 /13\t:  1.177216 11.98287 \tl:3.428094 \tem:0.377778\n",
      "Batch:\t10 /13\t:  1.222512 13.20670 \tl:3.393180 \tem:0.422222\n",
      "Batch:\t11 /13\t:  1.138187 14.34585 \tl:4.391483 \tem:0.288889\n",
      "Batch:\t12 /13\t:  1.218449 15.56654 \tl:3.253799 \tem:0.533333\n",
      "\n",
      "Epoch performance:  16.103sec Trl:4.047668 \tTrem:0.317949 \tTeem:0.000000\n",
      "\n",
      "Epoch:  121 / 300\n",
      "Batch:\t0 /13\t:  1.260361 1.260376 \tl:4.006225 \tem:0.355556\n",
      "Batch:\t1 /13\t:  1.207401 2.468683 \tl:3.156812 \tem:0.422222\n",
      "Batch:\t2 /13\t:  1.137353 3.606301 \tl:4.832939 \tem:0.266667\n",
      "Batch:\t3 /13\t:  1.226788 4.833585 \tl:4.085330 \tem:0.244444\n",
      "Batch:\t4 /13\t:  1.334162 6.168118 \tl:4.455519 \tem:0.311111\n",
      "Batch:\t5 /13\t:  1.168377 7.337069 \tl:3.449567 \tem:0.333333\n",
      "Batch:\t6 /13\t:  1.221845 8.559214 \tl:4.310973 \tem:0.288889\n",
      "Batch:\t7 /13\t:  1.161894 9.721981 \tl:4.240137 \tem:0.333333\n",
      "Batch:\t8 /13\t:  1.219409 10.94217 \tl:4.616107 \tem:0.155556\n",
      "Batch:\t9 /13\t:  1.179670 12.12211 \tl:3.510531 \tem:0.333333\n",
      "Batch:\t10 /13\t:  1.250030 13.37350 \tl:4.296062 \tem:0.311111\n",
      "Batch:\t11 /13\t:  1.224809 14.59848 \tl:3.952812 \tem:0.244444\n",
      "Batch:\t12 /13\t:  1.258574 15.85730 \tl:3.519496 \tem:0.333333\n",
      "\n",
      "Epoch performance:  16.393sec Trl:4.033270 \tTrem:0.302564 \tTeem:0.000000\n",
      "\n",
      "Epoch:  122 / 300\n",
      "Batch:\t0 /13\t:  1.279380 1.279394 \tl:3.093499 \tem:0.400000\n",
      "Batch:\t1 /13\t:  1.220759 2.500689 \tl:4.558616 \tem:0.200000\n",
      "Batch:\t2 /13\t:  1.224445 3.726343 \tl:3.991405 \tem:0.333333\n",
      "Batch:\t3 /13\t:  1.222232 4.949564 \tl:4.211767 \tem:0.177778\n",
      "Batch:\t4 /13\t:  1.273609 6.223855 \tl:3.181463 \tem:0.422222\n",
      "Batch:\t5 /13\t:  1.195624 7.420003 \tl:4.132967 \tem:0.444444\n",
      "Batch:\t6 /13\t:  1.275480 8.695976 \tl:3.729095 \tem:0.222222\n",
      "Batch:\t7 /13\t:  1.254070 9.950565 \tl:3.756172 \tem:0.377778\n",
      "Batch:\t8 /13\t:  1.170773 11.12218 \tl:4.647013 \tem:0.244444\n",
      "Batch:\t9 /13\t:  1.610141 12.73262 \tl:5.301449 \tem:0.222222\n",
      "Batch:\t10 /13\t:  1.471148 14.20419 \tl:3.882124 \tem:0.355556\n",
      "Batch:\t11 /13\t:  1.188197 15.39347 \tl:4.217597 \tem:0.244444\n",
      "Batch:\t12 /13\t:  1.164324 16.55828 \tl:4.487790 \tem:0.288889\n",
      "\n",
      "Epoch performance:  17.096sec Trl:4.091612 \tTrem:0.302564 \tTeem:0.000000\n",
      "\n",
      "Epoch:  123 / 300\n",
      "Batch:\t0 /13\t:  1.193410 1.193429 \tl:4.043043 \tem:0.311111\n",
      "Batch:\t1 /13\t:  1.266277 2.460563 \tl:4.388817 \tem:0.288889\n",
      "Batch:\t2 /13\t:  1.150496 3.611835 \tl:4.052717 \tem:0.377778\n",
      "Batch:\t3 /13\t:  1.180273 4.794425 \tl:3.892574 \tem:0.422222\n",
      "Batch:\t4 /13\t:  1.201955 5.996582 \tl:4.132502 \tem:0.333333\n",
      "Batch:\t5 /13\t:  1.276983 7.274484 \tl:3.906422 \tem:0.266667\n",
      "Batch:\t6 /13\t:  1.378767 8.654263 \tl:3.378721 \tem:0.311111\n",
      "Batch:\t7 /13\t:  1.361996 10.01704 \tl:4.128755 \tem:0.311111\n",
      "Batch:\t8 /13\t:  1.631736 11.64937 \tl:4.032784 \tem:0.422222\n",
      "Batch:\t9 /13\t:  1.252831 12.90242 \tl:4.746211 \tem:0.222222\n",
      "Batch:\t10 /13\t:  1.232992 14.13590 \tl:3.608238 \tem:0.244444\n",
      "Batch:\t11 /13\t:  1.260064 15.39621 \tl:3.148114 \tem:0.400000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t12 /13\t:  1.292616 16.69010 \tl:5.035965 \tem:0.244444\n",
      "\n",
      "Epoch performance:  17.229sec Trl:4.038066 \tTrem:0.319658 \tTeem:0.000000\n",
      "\n",
      "Epoch:  124 / 300\n",
      "Batch:\t0 /13\t:  1.207754 1.207770 \tl:4.465027 \tem:0.288889\n",
      "Batch:\t1 /13\t:  1.245075 2.453475 \tl:4.425053 \tem:0.333333\n",
      "Batch:\t2 /13\t:  1.276560 3.730648 \tl:3.605652 \tem:0.244444\n",
      "Batch:\t3 /13\t:  1.279208 5.010171 \tl:4.128536 \tem:0.244444\n",
      "Batch:\t4 /13\t:  1.172652 6.183339 \tl:3.304970 \tem:0.355556\n",
      "Batch:\t5 /13\t:  1.327805 7.511442 \tl:4.046235 \tem:0.244444\n",
      "Batch:\t6 /13\t:  1.191879 8.703949 \tl:4.165912 \tem:0.311111\n",
      "Batch:\t7 /13\t:  1.250766 9.955603 \tl:4.260337 \tem:0.311111\n",
      "Batch:\t8 /13\t:  1.249651 11.20787 \tl:3.402171 \tem:0.266667\n",
      "Batch:\t9 /13\t:  1.191159 12.40024 \tl:4.313380 \tem:0.266667\n",
      "Batch:\t10 /13\t:  1.265436 13.66617 \tl:3.857375 \tem:0.333333\n",
      "Batch:\t11 /13\t:  1.204451 14.87138 \tl:4.341208 \tem:0.266667\n",
      "Batch:\t12 /13\t:  1.450906 16.32281 \tl:4.332821 \tem:0.288889\n",
      "\n",
      "Epoch performance:  16.860sec Trl:4.049898 \tTrem:0.288889 \tTeem:0.000000\n",
      "\n",
      "Epoch:  125 / 300\n",
      "Batch:\t0 /13\t:  1.310093 1.310108 \tl:3.131904 \tem:0.444444\n",
      "Batch:\t1 /13\t:  1.252424 2.563091 \tl:4.801841 \tem:0.200000\n",
      "Batch:\t2 /13\t:  1.258888 3.822229 \tl:4.336749 \tem:0.288889\n",
      "Batch:\t3 /13\t:  1.223195 5.045889 \tl:4.183260 \tem:0.333333\n",
      "Batch:\t4 /13\t:  1.148942 6.195713 \tl:4.655589 \tem:0.222222\n",
      "Batch:\t5 /13\t:  1.295893 7.492254 \tl:4.044023 \tem:0.288889\n",
      "Batch:\t6 /13\t:  1.178256 8.682934 \tl:3.108033 \tem:0.422222\n",
      "Batch:\t7 /13\t:  1.209828 9.893034 \tl:4.331915 \tem:0.244444\n",
      "Batch:\t8 /13\t:  1.194723 11.08867 \tl:5.082535 \tem:0.155556\n",
      "Batch:\t9 /13\t:  1.204799 12.29368 \tl:4.404493 \tem:0.311111\n",
      "Batch:\t10 /13\t:  1.215911 13.51013 \tl:2.776195 \tem:0.511111\n",
      "Batch:\t11 /13\t:  1.140808 14.65113 \tl:3.709349 \tem:0.266667\n",
      "Batch:\t12 /13\t:  1.285762 15.93718 \tl:5.070625 \tem:0.200000\n",
      "\n",
      "Epoch performance:  16.476sec Trl:4.125885 \tTrem:0.299145 \tTeem:0.000000\n",
      "\n",
      "Epoch:  126 / 300\n",
      "Batch:\t0 /13\t:  1.275243 1.275260 \tl:3.750929 \tem:0.377778\n",
      "Batch:\t1 /13\t:  1.239710 2.515519 \tl:3.398421 \tem:0.377778\n",
      "Batch:\t2 /13\t:  1.204658 3.721420 \tl:4.080595 \tem:0.244444\n",
      "Batch:\t3 /13\t:  1.217648 4.939548 \tl:3.878019 \tem:0.377778\n",
      "Batch:\t4 /13\t:  1.167330 6.107660 \tl:3.630794 \tem:0.244444\n",
      "Batch:\t5 /13\t:  1.336971 7.445845 \tl:3.749757 \tem:0.288889\n",
      "Batch:\t6 /13\t:  1.237602 8.684201 \tl:3.427659 \tem:0.444444\n",
      "Batch:\t7 /13\t:  1.194331 9.879020 \tl:3.481928 \tem:0.511111\n",
      "Batch:\t8 /13\t:  1.317040 11.19685 \tl:3.102995 \tem:0.488889\n",
      "Batch:\t9 /13\t:  1.163950 12.36174 \tl:3.975828 \tem:0.355556\n",
      "Batch:\t10 /13\t:  1.272848 13.63556 \tl:3.509371 \tem:0.355556\n",
      "Batch:\t11 /13\t:  1.121443 14.75790 \tl:4.069569 \tem:0.244444\n",
      "Batch:\t12 /13\t:  1.172290 15.93056 \tl:4.223815 \tem:0.311111\n",
      "\n",
      "Epoch performance:  16.468sec Trl:3.713821 \tTrem:0.355556 \tTeem:0.000000\n",
      "\n",
      "Epoch:  127 / 300\n",
      "Batch:\t0 /13\t:  1.115414 1.115429 \tl:3.867796 \tem:0.333333\n",
      "Batch:\t1 /13\t:  1.234351 2.350792 \tl:4.256964 \tem:0.155556\n",
      "Batch:\t2 /13\t:  1.313847 3.665384 \tl:4.013895 \tem:0.355556\n",
      "Batch:\t3 /13\t:  1.234492 4.900572 \tl:4.117936 \tem:0.333333\n",
      "Batch:\t4 /13\t:  1.232589 6.133707 \tl:4.060049 \tem:0.377778\n",
      "Batch:\t5 /13\t:  1.191079 7.325818 \tl:3.621939 \tem:0.244444\n",
      "Batch:\t6 /13\t:  1.274435 8.600545 \tl:3.261521 \tem:0.444444\n",
      "Batch:\t7 /13\t:  1.218521 9.820136 \tl:4.044600 \tem:0.377778\n",
      "Batch:\t8 /13\t:  1.293375 11.11396 \tl:4.259535 \tem:0.244444\n",
      "Batch:\t9 /13\t:  1.321440 12.43606 \tl:3.360877 \tem:0.355556\n",
      "Batch:\t10 /13\t:  1.323864 13.76073 \tl:3.693261 \tem:0.333333\n",
      "Batch:\t11 /13\t:  1.237281 14.99857 \tl:3.447656 \tem:0.466667\n",
      "Batch:\t12 /13\t:  1.213846 16.21284 \tl:3.682646 \tem:0.288889\n",
      "\n",
      "Epoch performance:  16.752sec Trl:3.822206 \tTrem:0.331624 \tTeem:0.000000\n",
      "\n",
      "Epoch:  128 / 300\n",
      "Batch:\t0 /13\t:  1.191486 1.191500 \tl:4.031631 \tem:0.266667\n",
      "Batch:\t1 /13\t:  1.296962 2.488907 \tl:4.793014 \tem:0.244444\n",
      "Batch:\t2 /13\t:  1.238140 3.729980 \tl:4.463224 \tem:0.400000\n",
      "Batch:\t3 /13\t:  1.184973 4.915940 \tl:4.628102 \tem:0.222222\n",
      "Batch:\t4 /13\t:  1.213892 6.130840 \tl:3.479219 \tem:0.377778\n",
      "Batch:\t5 /13\t:  1.287142 7.419245 \tl:4.338036 \tem:0.266667\n",
      "Batch:\t6 /13\t:  1.272728 8.692965 \tl:3.866457 \tem:0.444444\n",
      "Batch:\t7 /13\t:  1.293021 9.986588 \tl:3.285611 \tem:0.377778\n",
      "Batch:\t8 /13\t:  1.305479 11.29337 \tl:3.853476 \tem:0.355556\n",
      "Batch:\t9 /13\t:  1.306905 12.60092 \tl:4.155917 \tem:0.288889\n",
      "Batch:\t10 /13\t:  1.313258 13.91474 \tl:3.424351 \tem:0.422222\n",
      "Batch:\t11 /13\t:  1.246662 15.16167 \tl:3.601008 \tem:0.333333\n",
      "Batch:\t12 /13\t:  1.632162 16.79468 \tl:4.224255 \tem:0.400000\n",
      "\n",
      "Epoch performance:  17.333sec Trl:4.011100 \tTrem:0.338462 \tTeem:0.000000\n",
      "\n",
      "Epoch:  129 / 300\n",
      "Batch:\t0 /13\t:  1.346153 1.346173 \tl:3.397401 \tem:0.333333\n",
      "Batch:\t1 /13\t:  1.251010 2.597715 \tl:3.964964 \tem:0.377778\n",
      "Batch:\t2 /13\t:  1.239963 3.838101 \tl:3.970714 \tem:0.377778\n",
      "Batch:\t3 /13\t:  1.311400 5.150123 \tl:4.068431 \tem:0.311111\n",
      "Batch:\t4 /13\t:  1.159732 6.310475 \tl:3.087747 \tem:0.422222\n",
      "Batch:\t5 /13\t:  1.194015 7.504996 \tl:3.634631 \tem:0.422222\n",
      "Batch:\t6 /13\t:  1.203906 8.709452 \tl:4.131567 \tem:0.311111\n",
      "Batch:\t7 /13\t:  1.206634 9.916835 \tl:3.337239 \tem:0.333333\n",
      "Batch:\t8 /13\t:  1.253383 11.17129 \tl:4.276325 \tem:0.377778\n",
      "Batch:\t9 /13\t:  1.145395 12.31824 \tl:3.228632 \tem:0.422222\n",
      "Batch:\t10 /13\t:  1.308757 13.62773 \tl:3.380292 \tem:0.444444\n",
      "Batch:\t11 /13\t:  1.163142 14.79208 \tl:4.361046 \tem:0.266667\n",
      "Batch:\t12 /13\t:  1.295847 16.08859 \tl:4.420910 \tem:0.266667\n",
      "\n",
      "Epoch performance:  16.625sec Trl:3.789223 \tTrem:0.358974 \tTeem:0.000000\n",
      "\n",
      "Epoch:  130 / 300\n",
      "Batch:\t0 /13\t:  1.283534 1.283549 \tl:4.018474 \tem:0.355556\n",
      "Batch:\t1 /13\t:  1.219863 2.503617 \tl:3.947040 \tem:0.288889\n",
      "Batch:\t2 /13\t:  1.160640 3.665117 \tl:4.028445 \tem:0.222222\n",
      "Batch:\t3 /13\t:  1.197144 4.862457 \tl:4.313220 \tem:0.333333\n",
      "Batch:\t4 /13\t:  1.214504 6.077510 \tl:4.164093 \tem:0.333333\n",
      "Batch:\t5 /13\t:  1.233247 7.311748 \tl:4.621548 \tem:0.222222\n",
      "Batch:\t6 /13\t:  1.315635 8.627998 \tl:3.473365 \tem:0.333333\n",
      "Batch:\t7 /13\t:  1.315526 9.943983 \tl:3.321222 \tem:0.422222\n",
      "Batch:\t8 /13\t:  1.682081 11.62680 \tl:4.273207 \tem:0.311111\n",
      "Batch:\t9 /13\t:  1.544650 13.17255 \tl:4.261846 \tem:0.244444\n",
      "Batch:\t10 /13\t:  1.329566 14.50324 \tl:3.625803 \tem:0.400000\n",
      "Batch:\t11 /13\t:  1.250545 15.75450 \tl:3.411521 \tem:0.311111\n",
      "Batch:\t12 /13\t:  1.185300 16.94038 \tl:3.423542 \tem:0.422222\n",
      "\n",
      "Epoch performance:  17.480sec Trl:3.914102 \tTrem:0.323077 \tTeem:0.000000\n",
      "\n",
      "Epoch:  131 / 300\n",
      "Batch:\t0 /13\t:  1.248212 1.248228 \tl:3.533484 \tem:0.444444\n",
      "Batch:\t1 /13\t:  1.165879 2.414978 \tl:3.747132 \tem:0.333333\n",
      "Batch:\t2 /13\t:  1.211879 3.627457 \tl:3.565967 \tem:0.200000\n",
      "Batch:\t3 /13\t:  1.239425 4.867132 \tl:3.492306 \tem:0.311111\n",
      "Batch:\t4 /13\t:  1.196191 6.064524 \tl:4.398494 \tem:0.200000\n",
      "Batch:\t5 /13\t:  1.268807 7.333647 \tl:5.005563 \tem:0.244444\n",
      "Batch:\t6 /13\t:  1.139621 8.474250 \tl:4.499105 \tem:0.222222\n",
      "Batch:\t7 /13\t:  1.196893 9.671662 \tl:4.600648 \tem:0.244444\n",
      "Batch:\t8 /13\t:  1.139961 10.81266 \tl:4.810575 \tem:0.244444\n",
      "Batch:\t9 /13\t:  1.236651 12.04989 \tl:3.580616 \tem:0.333333\n",
      "Batch:\t10 /13\t:  1.303674 13.35424 \tl:4.565017 \tem:0.355556\n",
      "Batch:\t11 /13\t:  1.305691 14.66108 \tl:3.866045 \tem:0.311111\n",
      "Batch:\t12 /13\t:  1.202494 15.86399 \tl:4.122798 \tem:0.288889\n",
      "\n",
      "Epoch performance:  16.401sec Trl:4.137519 \tTrem:0.287179 \tTeem:0.000000\n",
      "\n",
      "Epoch:  132 / 300\n",
      "Batch:\t0 /13\t:  1.183173 1.183189 \tl:4.281974 \tem:0.200000\n",
      "Batch:\t1 /13\t:  1.270221 2.455914 \tl:3.899994 \tem:0.333333\n",
      "Batch:\t2 /13\t:  1.221140 3.677515 \tl:4.017724 \tem:0.311111\n",
      "Batch:\t3 /13\t:  1.174319 4.852398 \tl:3.453299 \tem:0.377778\n",
      "Batch:\t4 /13\t:  1.197615 6.051324 \tl:3.590983 \tem:0.200000\n",
      "Batch:\t5 /13\t:  1.288342 7.340756 \tl:2.883876 \tem:0.333333\n",
      "Batch:\t6 /13\t:  1.101269 8.442680 \tl:4.584264 \tem:0.311111\n",
      "Batch:\t7 /13\t:  1.292535 9.735493 \tl:3.584038 \tem:0.377778\n",
      "Batch:\t8 /13\t:  1.512295 11.24823 \tl:3.726584 \tem:0.311111\n",
      "Batch:\t9 /13\t:  1.322217 12.57130 \tl:4.129749 \tem:0.355556\n",
      "Batch:\t10 /13\t:  1.202014 13.77362 \tl:3.901974 \tem:0.311111\n",
      "Batch:\t11 /13\t:  1.287381 15.06169 \tl:4.008059 \tem:0.244444\n",
      "Batch:\t12 /13\t:  1.239560 16.30178 \tl:3.650264 \tem:0.400000\n",
      "\n",
      "Epoch performance:  16.839sec Trl:3.824060 \tTrem:0.312821 \tTeem:0.000000\n",
      "\n",
      "Epoch:  133 / 300\n",
      "Batch:\t0 /13\t:  1.346761 1.346793 \tl:3.365516 \tem:0.333333\n",
      "Batch:\t1 /13\t:  1.216326 2.564257 \tl:4.235279 \tem:0.266667\n",
      "Batch:\t2 /13\t:  1.174659 3.739388 \tl:4.491648 \tem:0.333333\n",
      "Batch:\t3 /13\t:  1.241850 4.981705 \tl:3.640826 \tem:0.400000\n",
      "Batch:\t4 /13\t:  1.201081 6.183653 \tl:4.260013 \tem:0.377778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t5 /13\t:  1.225028 7.410030 \tl:3.768864 \tem:0.333333\n",
      "Batch:\t6 /13\t:  1.189455 8.600041 \tl:3.606367 \tem:0.266667\n",
      "Batch:\t7 /13\t:  1.266394 9.866741 \tl:3.637654 \tem:0.311111\n",
      "Batch:\t8 /13\t:  1.145547 11.01297 \tl:3.910313 \tem:0.266667\n",
      "Batch:\t9 /13\t:  1.180571 12.19449 \tl:4.326208 \tem:0.288889\n",
      "Batch:\t10 /13\t:  1.202994 13.39808 \tl:3.938955 \tem:0.266667\n",
      "Batch:\t11 /13\t:  1.174896 14.57340 \tl:3.752193 \tem:0.355556\n",
      "Batch:\t12 /13\t:  1.224117 15.79812 \tl:4.138731 \tem:0.288889\n",
      "\n",
      "Epoch performance:  16.335sec Trl:3.928659 \tTrem:0.314530 \tTeem:0.000000\n",
      "\n",
      "Epoch:  134 / 300\n",
      "Batch:\t0 /13\t:  1.178073 1.178090 \tl:3.476314 \tem:0.422222\n",
      "Batch:\t1 /13\t:  1.277471 2.456722 \tl:4.279392 \tem:0.333333\n",
      "Batch:\t2 /13\t:  1.289844 3.747673 \tl:4.260713 \tem:0.355556\n",
      "Batch:\t3 /13\t:  1.244493 4.992980 \tl:4.241673 \tem:0.377778\n",
      "Batch:\t4 /13\t:  1.233747 6.227230 \tl:3.318808 \tem:0.377778\n",
      "Batch:\t5 /13\t:  1.280781 7.509170 \tl:3.320836 \tem:0.244444\n",
      "Batch:\t6 /13\t:  1.330099 8.839671 \tl:3.321091 \tem:0.355556\n",
      "Batch:\t7 /13\t:  1.193645 10.03431 \tl:3.268858 \tem:0.466667\n",
      "Batch:\t8 /13\t:  1.181221 11.21571 \tl:3.067504 \tem:0.422222\n",
      "Batch:\t9 /13\t:  1.219049 12.43527 \tl:3.167822 \tem:0.400000\n",
      "Batch:\t10 /13\t:  1.212693 13.64844 \tl:3.344138 \tem:0.400000\n",
      "Batch:\t11 /13\t:  1.258416 14.90707 \tl:3.365440 \tem:0.466667\n",
      "Batch:\t12 /13\t:  1.271619 16.17920 \tl:3.519152 \tem:0.377778\n",
      "\n",
      "Epoch performance:  16.716sec Trl:3.534749 \tTrem:0.384615 \tTeem:0.000000\n",
      "\n",
      "Epoch:  135 / 300\n",
      "Batch:\t0 /13\t:  1.324969 1.324983 \tl:3.752284 \tem:0.311111\n",
      "Batch:\t1 /13\t:  1.260215 2.586374 \tl:3.224228 \tem:0.355556\n",
      "Batch:\t2 /13\t:  1.250565 3.837438 \tl:3.145607 \tem:0.444444\n",
      "Batch:\t3 /13\t:  1.555796 5.393788 \tl:4.579361 \tem:0.288889\n",
      "Batch:\t4 /13\t:  1.256817 6.651300 \tl:3.976366 \tem:0.400000\n",
      "Batch:\t5 /13\t:  1.692668 8.344834 \tl:3.878224 \tem:0.311111\n",
      "Batch:\t6 /13\t:  1.283372 9.629003 \tl:3.511321 \tem:0.355556\n",
      "Batch:\t7 /13\t:  1.269863 10.89907 \tl:4.114394 \tem:0.377778\n",
      "Batch:\t8 /13\t:  1.079741 11.97936 \tl:3.032514 \tem:0.444444\n",
      "Batch:\t9 /13\t:  1.210283 13.19020 \tl:4.182718 \tem:0.400000\n",
      "Batch:\t10 /13\t:  1.241933 14.43269 \tl:4.687780 \tem:0.155556\n",
      "Batch:\t11 /13\t:  1.123311 15.55686 \tl:3.345672 \tem:0.311111\n",
      "Batch:\t12 /13\t:  1.184508 16.74180 \tl:3.186272 \tem:0.377778\n",
      "\n",
      "Epoch performance:  17.279sec Trl:3.739749 \tTrem:0.348718 \tTeem:0.000000\n",
      "\n",
      "Epoch:  136 / 300\n",
      "Batch:\t0 /13\t:  1.324345 1.324363 \tl:3.920325 \tem:0.333333\n",
      "Batch:\t1 /13\t:  1.179960 2.504575 \tl:4.006792 \tem:0.377778\n",
      "Batch:\t2 /13\t:  1.194960 3.700860 \tl:4.008358 \tem:0.244444\n",
      "Batch:\t3 /13\t:  1.237595 4.939447 \tl:4.383054 \tem:0.266667\n",
      "Batch:\t4 /13\t:  1.216084 6.156846 \tl:3.805450 \tem:0.444444\n",
      "Batch:\t5 /13\t:  1.242339 7.400218 \tl:3.341201 \tem:0.400000\n",
      "Batch:\t6 /13\t:  1.135481 8.536129 \tl:2.660688 \tem:0.533333\n",
      "Batch:\t7 /13\t:  1.317044 9.854372 \tl:2.960379 \tem:0.311111\n",
      "Batch:\t8 /13\t:  1.251484 11.10662 \tl:3.754797 \tem:0.333333\n",
      "Batch:\t9 /13\t:  1.168090 12.27492 \tl:3.271942 \tem:0.333333\n",
      "Batch:\t10 /13\t:  1.192851 13.46866 \tl:3.287151 \tem:0.422222\n",
      "Batch:\t11 /13\t:  1.202936 14.67241 \tl:3.478768 \tem:0.400000\n",
      "Batch:\t12 /13\t:  1.263649 15.93687 \tl:3.843537 \tem:0.444444\n",
      "\n",
      "Epoch performance:  16.474sec Trl:3.594034 \tTrem:0.372650 \tTeem:0.000000\n",
      "\n",
      "Epoch:  137 / 300\n",
      "Batch:\t0 /13\t:  1.329092 1.329113 \tl:3.222348 \tem:0.533333\n",
      "Batch:\t1 /13\t:  1.244475 2.574621 \tl:4.251965 \tem:0.355556\n",
      "Batch:\t2 /13\t:  1.213339 3.788383 \tl:3.962309 \tem:0.311111\n",
      "Batch:\t3 /13\t:  1.243368 5.031960 \tl:4.259713 \tem:0.266667\n",
      "Batch:\t4 /13\t:  1.247737 6.280177 \tl:3.495625 \tem:0.333333\n",
      "Batch:\t5 /13\t:  1.278272 7.559015 \tl:3.720947 \tem:0.244444\n",
      "Batch:\t6 /13\t:  1.133016 8.692325 \tl:3.500942 \tem:0.333333\n",
      "Batch:\t7 /13\t:  1.212876 9.905962 \tl:3.201711 \tem:0.444444\n",
      "Batch:\t8 /13\t:  1.189527 11.09601 \tl:3.601904 \tem:0.377778\n",
      "Batch:\t9 /13\t:  1.223687 12.32023 \tl:3.718942 \tem:0.400000\n",
      "Batch:\t10 /13\t:  1.307481 13.62838 \tl:3.607624 \tem:0.311111\n",
      "Batch:\t11 /13\t:  1.233561 14.86259 \tl:3.605094 \tem:0.377778\n",
      "Batch:\t12 /13\t:  1.232075 16.09528 \tl:3.961161 \tem:0.222222\n",
      "\n",
      "Epoch performance:  16.632sec Trl:3.700791 \tTrem:0.347009 \tTeem:0.000000\n",
      "\n",
      "Epoch:  138 / 300\n",
      "Batch:\t0 /13\t:  1.326443 1.326457 \tl:3.302655 \tem:0.422222\n",
      "Batch:\t1 /13\t:  1.197803 2.524727 \tl:3.848482 \tem:0.355556\n",
      "Batch:\t2 /13\t:  1.201540 3.727226 \tl:4.303571 \tem:0.377778\n",
      "Batch:\t3 /13\t:  1.208517 4.936233 \tl:4.193056 \tem:0.355556\n",
      "Batch:\t4 /13\t:  1.225239 6.162158 \tl:3.622770 \tem:0.422222\n",
      "Batch:\t5 /13\t:  1.213897 7.377158 \tl:3.939101 \tem:0.377778\n",
      "Batch:\t6 /13\t:  1.268409 8.645884 \tl:3.154905 \tem:0.400000\n",
      "Batch:\t7 /13\t:  1.189296 9.846019 \tl:3.997916 \tem:0.333333\n",
      "Batch:\t8 /13\t:  1.231844 11.07834 \tl:3.230265 \tem:0.466667\n",
      "Batch:\t9 /13\t:  1.179522 12.25876 \tl:4.054357 \tem:0.422222\n",
      "Batch:\t10 /13\t:  1.242224 13.50151 \tl:4.001132 \tem:0.377778\n",
      "Batch:\t11 /13\t:  1.111234 14.61350 \tl:3.999885 \tem:0.311111\n",
      "Batch:\t12 /13\t:  1.209033 15.82305 \tl:3.351056 \tem:0.377778\n",
      "\n",
      "Epoch performance:  16.360sec Trl:3.769165 \tTrem:0.384615 \tTeem:0.000000\n",
      "\n",
      "Epoch:  139 / 300\n",
      "Batch:\t0 /13\t:  1.259637 1.259653 \tl:3.177212 \tem:0.355556\n",
      "Batch:\t1 /13\t:  1.531553 2.792380 \tl:3.692412 \tem:0.377778\n",
      "Batch:\t2 /13\t:  1.268553 4.062012 \tl:5.211226 \tem:0.244444\n",
      "Batch:\t3 /13\t:  1.399943 5.462389 \tl:2.615276 \tem:0.488889\n",
      "Batch:\t4 /13\t:  1.189762 6.652692 \tl:3.916476 \tem:0.377778\n",
      "Batch:\t5 /13\t:  1.611160 8.264339 \tl:3.409951 \tem:0.333333\n",
      "Batch:\t6 /13\t:  1.274911 9.540414 \tl:4.211035 \tem:0.244444\n",
      "Batch:\t7 /13\t:  1.269670 10.81040 \tl:3.366421 \tem:0.444444\n",
      "Batch:\t8 /13\t:  1.291200 12.10210 \tl:3.228975 \tem:0.355556\n",
      "Batch:\t9 /13\t:  1.269387 13.37209 \tl:3.292696 \tem:0.355556\n",
      "Batch:\t10 /13\t:  1.199978 14.57298 \tl:3.327816 \tem:0.377778\n",
      "Batch:\t11 /13\t:  1.179290 15.75330 \tl:3.365299 \tem:0.333333\n",
      "Batch:\t12 /13\t:  1.275130 17.02872 \tl:3.778570 \tem:0.400000\n",
      "\n",
      "Epoch performance:  17.566sec Trl:3.584105 \tTrem:0.360684 \tTeem:0.000000\n",
      "\n",
      "Epoch:  140 / 300\n",
      "Batch:\t0 /13\t:  1.271643 1.271661 \tl:3.531876 \tem:0.377778\n",
      "Batch:\t1 /13\t:  1.251353 2.523497 \tl:2.687068 \tem:0.400000\n",
      "Batch:\t2 /13\t:  1.222194 3.746443 \tl:3.638530 \tem:0.311111\n",
      "Batch:\t3 /13\t:  1.205567 4.952775 \tl:3.311897 \tem:0.400000\n",
      "Batch:\t4 /13\t:  1.183099 6.136812 \tl:3.889830 \tem:0.355556\n",
      "Batch:\t5 /13\t:  1.273343 7.410672 \tl:4.020993 \tem:0.288889\n",
      "Batch:\t6 /13\t:  1.265118 8.676324 \tl:3.486138 \tem:0.444444\n",
      "Batch:\t7 /13\t:  1.158363 9.834874 \tl:4.013971 \tem:0.333333\n",
      "Batch:\t8 /13\t:  1.143229 10.97837 \tl:4.241782 \tem:0.288889\n",
      "Batch:\t9 /13\t:  1.252774 12.23211 \tl:4.531083 \tem:0.244444\n",
      "Batch:\t10 /13\t:  1.256004 13.48864 \tl:3.781098 \tem:0.288889\n",
      "Batch:\t11 /13\t:  1.208222 14.69770 \tl:3.788855 \tem:0.288889\n",
      "Batch:\t12 /13\t:  1.200581 15.89872 \tl:3.715801 \tem:0.311111\n",
      "\n",
      "Epoch performance:  16.435sec Trl:3.741456 \tTrem:0.333333 \tTeem:0.000000\n",
      "\n",
      "Epoch:  141 / 300\n",
      "Batch:\t0 /13\t:  1.209734 1.209750 \tl:3.567094 \tem:0.400000\n",
      "Batch:\t1 /13\t:  1.250310 2.460627 \tl:2.895346 \tem:0.377778\n",
      "Batch:\t2 /13\t:  1.270917 3.731784 \tl:3.253676 \tem:0.444444\n",
      "Batch:\t3 /13\t:  1.208221 4.940786 \tl:3.519306 \tem:0.311111\n",
      "Batch:\t4 /13\t:  1.220266 6.161864 \tl:4.479556 \tem:0.200000\n",
      "Batch:\t5 /13\t:  1.219327 7.381757 \tl:3.482992 \tem:0.444444\n",
      "Batch:\t6 /13\t:  1.215973 8.598227 \tl:3.372197 \tem:0.355556\n",
      "Batch:\t7 /13\t:  1.212202 9.811405 \tl:3.641745 \tem:0.266667\n",
      "Batch:\t8 /13\t:  1.263190 11.07499 \tl:3.390204 \tem:0.444444\n",
      "Batch:\t9 /13\t:  1.267772 12.34337 \tl:3.091067 \tem:0.400000\n",
      "Batch:\t10 /13\t:  1.303540 13.64749 \tl:3.606628 \tem:0.333333\n",
      "Batch:\t11 /13\t:  1.366844 15.01465 \tl:5.001977 \tem:0.266667\n",
      "Batch:\t12 /13\t:  1.279602 16.29457 \tl:3.598324 \tem:0.444444\n",
      "\n",
      "Epoch performance:  16.832sec Trl:3.607701 \tTrem:0.360684 \tTeem:0.000000\n",
      "\n",
      "Epoch:  142 / 300\n",
      "Batch:\t0 /13\t:  1.481621 1.481638 \tl:4.762076 \tem:0.177778\n",
      "Batch:\t1 /13\t:  1.540212 3.022047 \tl:3.242140 \tem:0.422222\n",
      "Batch:\t2 /13\t:  1.534995 4.557760 \tl:3.566252 \tem:0.355556\n",
      "Batch:\t3 /13\t:  1.224484 5.782474 \tl:3.718435 \tem:0.266667\n",
      "Batch:\t4 /13\t:  1.204089 6.987367 \tl:3.062127 \tem:0.466667\n",
      "Batch:\t5 /13\t:  1.229253 8.217321 \tl:3.819201 \tem:0.266667\n",
      "Batch:\t6 /13\t:  1.218150 9.435992 \tl:4.229141 \tem:0.311111\n",
      "Batch:\t7 /13\t:  1.211211 10.64770 \tl:4.160124 \tem:0.333333\n",
      "Batch:\t8 /13\t:  1.233908 11.88216 \tl:3.811044 \tem:0.400000\n",
      "Batch:\t9 /13\t:  1.251379 13.13403 \tl:3.613617 \tem:0.333333\n",
      "Batch:\t10 /13\t:  1.145869 14.28052 \tl:3.686170 \tem:0.266667\n",
      "Batch:\t11 /13\t:  1.198742 15.48053 \tl:3.646104 \tem:0.311111\n",
      "Batch:\t12 /13\t:  1.160638 16.64207 \tl:3.579378 \tem:0.511111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch performance:  17.179sec Trl:3.761216 \tTrem:0.340171 \tTeem:0.000000\n",
      "\n",
      "Epoch:  143 / 300\n",
      "Batch:\t0 /13\t:  1.284596 1.284622 \tl:4.031110 \tem:0.333333\n",
      "Batch:\t1 /13\t:  1.259681 2.545379 \tl:3.617400 \tem:0.333333\n",
      "Batch:\t2 /13\t:  1.209380 3.755601 \tl:4.355881 \tem:0.333333\n",
      "Batch:\t3 /13\t:  1.236431 4.993077 \tl:2.478930 \tem:0.577778\n",
      "Batch:\t4 /13\t:  1.200852 6.194401 \tl:3.868932 \tem:0.311111\n",
      "Batch:\t5 /13\t:  1.205169 7.400103 \tl:4.042234 \tem:0.333333\n",
      "Batch:\t6 /13\t:  1.309128 8.710102 \tl:3.524354 \tem:0.444444\n",
      "Batch:\t7 /13\t:  1.233728 9.944656 \tl:3.589455 \tem:0.444444\n",
      "Batch:\t8 /13\t:  1.174327 11.11987 \tl:3.227040 \tem:0.444444\n",
      "Batch:\t9 /13\t:  1.216033 12.33613 \tl:2.966758 \tem:0.377778\n",
      "Batch:\t10 /13\t:  1.362843 13.69991 \tl:3.297104 \tem:0.422222\n",
      "Batch:\t11 /13\t:  1.305873 15.02625 \tl:3.862188 \tem:0.355556\n",
      "Batch:\t12 /13\t:  1.294677 16.32157 \tl:4.005174 \tem:0.333333\n",
      "\n",
      "Epoch performance:  16.861sec Trl:3.605120 \tTrem:0.388034 \tTeem:0.000000\n",
      "\n",
      "Epoch:  144 / 300\n",
      "Batch:\t0 /13\t:  1.336136 1.336153 \tl:3.683916 \tem:0.333333\n",
      "Batch:\t1 /13\t:  1.213203 2.550117 \tl:3.721116 \tem:0.288889\n",
      "Batch:\t2 /13\t:  1.192015 3.742987 \tl:3.856655 \tem:0.377778\n",
      "Batch:\t3 /13\t:  1.174807 4.918385 \tl:4.132235 \tem:0.200000\n",
      "Batch:\t4 /13\t:  1.215576 6.134706 \tl:3.578219 \tem:0.355556\n",
      "Batch:\t5 /13\t:  1.289483 7.425116 \tl:2.922910 \tem:0.422222\n",
      "Batch:\t6 /13\t:  1.152099 8.578523 \tl:3.514401 \tem:0.311111\n",
      "Batch:\t7 /13\t:  1.263903 9.843031 \tl:3.603960 \tem:0.355556\n",
      "Batch:\t8 /13\t:  1.162609 11.00613 \tl:4.282544 \tem:0.266667\n",
      "Batch:\t9 /13\t:  1.200366 12.20734 \tl:3.293967 \tem:0.466667\n",
      "Batch:\t10 /13\t:  1.202862 13.41099 \tl:3.583387 \tem:0.266667\n",
      "Batch:\t11 /13\t:  1.214835 14.62632 \tl:3.617785 \tem:0.377778\n",
      "Batch:\t12 /13\t:  1.185162 15.81167 \tl:3.146998 \tem:0.377778\n",
      "\n",
      "Epoch performance:  16.349sec Trl:3.610622 \tTrem:0.338462 \tTeem:0.000000\n",
      "\n",
      "Epoch:  145 / 300\n",
      "Batch:\t0 /13\t:  1.304374 1.304389 \tl:2.962471 \tem:0.377778\n",
      "Batch:\t1 /13\t:  1.239603 2.544432 \tl:3.529467 \tem:0.355556\n",
      "Batch:\t2 /13\t:  1.219675 3.765233 \tl:3.402815 \tem:0.377778\n",
      "Batch:\t3 /13\t:  1.222630 4.988354 \tl:3.486065 \tem:0.311111\n",
      "Batch:\t4 /13\t:  1.205140 6.194051 \tl:3.162945 \tem:0.466667\n",
      "Batch:\t5 /13\t:  1.273571 7.468152 \tl:3.759765 \tem:0.400000\n",
      "Batch:\t6 /13\t:  1.209120 8.679681 \tl:3.971446 \tem:0.266667\n",
      "Batch:\t7 /13\t:  1.198299 9.882282 \tl:3.232597 \tem:0.377778\n",
      "Batch:\t8 /13\t:  1.324190 11.20718 \tl:2.272514 \tem:0.600000\n",
      "Batch:\t9 /13\t:  1.275402 12.48349 \tl:3.019206 \tem:0.466667\n",
      "Batch:\t10 /13\t:  1.236176 13.72094 \tl:3.291198 \tem:0.400000\n",
      "Batch:\t11 /13\t:  1.205623 14.92735 \tl:3.520731 \tem:0.444444\n",
      "Batch:\t12 /13\t:  1.277971 16.20657 \tl:2.494532 \tem:0.511111\n",
      "\n",
      "Epoch performance:  16.745sec Trl:3.238904 \tTrem:0.411966 \tTeem:0.000000\n",
      "\n",
      "Epoch:  146 / 300\n",
      "Batch:\t0 /13\t:  1.256154 1.256170 \tl:3.586466 \tem:0.355556\n",
      "Batch:\t1 /13\t:  1.216603 2.473318 \tl:4.373688 \tem:0.311111\n",
      "Batch:\t2 /13\t:  1.274195 3.748019 \tl:3.351121 \tem:0.400000\n",
      "Batch:\t3 /13\t:  1.615859 5.364423 \tl:3.787267 \tem:0.400000\n",
      "Batch:\t4 /13\t:  1.531955 6.897006 \tl:2.891465 \tem:0.511111\n",
      "Batch:\t5 /13\t:  1.217927 8.117321 \tl:2.884495 \tem:0.422222\n",
      "Batch:\t6 /13\t:  1.260905 9.380052 \tl:3.494816 \tem:0.355556\n",
      "Batch:\t7 /13\t:  1.239888 10.62050 \tl:3.928222 \tem:0.355556\n",
      "Batch:\t8 /13\t:  1.157504 11.77864 \tl:2.748800 \tem:0.400000\n",
      "Batch:\t9 /13\t:  1.267815 13.04711 \tl:3.984794 \tem:0.244444\n",
      "Batch:\t10 /13\t:  1.241128 14.28931 \tl:4.209888 \tem:0.333333\n",
      "Batch:\t11 /13\t:  1.214797 15.50479 \tl:4.160926 \tem:0.288889\n",
      "Batch:\t12 /13\t:  1.171705 16.67712 \tl:3.757917 \tem:0.355556\n",
      "\n",
      "Epoch performance:  17.217sec Trl:3.627682 \tTrem:0.364103 \tTeem:0.000000\n",
      "\n",
      "Epoch:  147 / 300\n",
      "Batch:\t0 /13\t:  1.207086 1.207100 \tl:3.301469 \tem:0.466667\n",
      "Batch:\t1 /13\t:  1.154817 2.362357 \tl:2.759130 \tem:0.488889\n",
      "Batch:\t2 /13\t:  1.355340 3.718905 \tl:3.266008 \tem:0.422222\n",
      "Batch:\t3 /13\t:  1.160222 4.879605 \tl:2.839185 \tem:0.577778\n",
      "Batch:\t4 /13\t:  1.192440 6.073127 \tl:3.029591 \tem:0.444444\n",
      "Batch:\t5 /13\t:  1.335702 7.409308 \tl:3.257087 \tem:0.444444\n",
      "Batch:\t6 /13\t:  1.182478 8.592672 \tl:3.542101 \tem:0.400000\n",
      "Batch:\t7 /13\t:  1.452804 10.04591 \tl:2.699733 \tem:0.511111\n",
      "Batch:\t8 /13\t:  1.182562 11.22895 \tl:4.664725 \tem:0.177778\n",
      "Batch:\t9 /13\t:  1.184044 12.41344 \tl:3.806783 \tem:0.222222\n",
      "Batch:\t10 /13\t:  1.260808 13.67524 \tl:4.011623 \tem:0.311111\n",
      "Batch:\t11 /13\t:  1.232218 14.90825 \tl:3.679658 \tem:0.333333\n",
      "Batch:\t12 /13\t:  1.270595 16.17956 \tl:4.346926 \tem:0.288889\n",
      "\n",
      "Epoch performance:  16.717sec Trl:3.477232 \tTrem:0.391453 \tTeem:0.000000\n",
      "\n",
      "Epoch:  148 / 300\n",
      "Batch:\t0 /13\t:  1.217221 1.217235 \tl:3.856519 \tem:0.333333\n",
      "Batch:\t1 /13\t:  1.257203 2.474879 \tl:2.410703 \tem:0.555556\n",
      "Batch:\t2 /13\t:  1.197171 3.672520 \tl:3.166261 \tem:0.400000\n",
      "Batch:\t3 /13\t:  1.634060 5.307516 \tl:3.768623 \tem:0.311111\n",
      "Batch:\t4 /13\t:  1.561617 6.869418 \tl:3.216582 \tem:0.400000\n",
      "Batch:\t5 /13\t:  1.330221 8.200222 \tl:3.146292 \tem:0.488889\n",
      "Batch:\t6 /13\t:  1.171824 9.372241 \tl:3.762071 \tem:0.422222\n",
      "Batch:\t7 /13\t:  1.207836 10.58049 \tl:3.187952 \tem:0.244444\n",
      "Batch:\t8 /13\t:  1.190969 11.77168 \tl:2.960997 \tem:0.466667\n",
      "Batch:\t9 /13\t:  1.258609 13.03097 \tl:3.302097 \tem:0.311111\n",
      "Batch:\t10 /13\t:  1.231321 14.26291 \tl:3.779642 \tem:0.422222\n",
      "Batch:\t11 /13\t:  1.244003 15.50749 \tl:3.862781 \tem:0.355556\n",
      "Batch:\t12 /13\t:  1.243205 16.75131 \tl:2.860648 \tem:0.488889\n",
      "\n",
      "Epoch performance:  17.289sec Trl:3.329321 \tTrem:0.400000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  149 / 300\n",
      "Batch:\t0 /13\t:  1.295958 1.295973 \tl:3.996968 \tem:0.377778\n",
      "Batch:\t1 /13\t:  1.284702 2.581653 \tl:3.095081 \tem:0.488889\n",
      "Batch:\t2 /13\t:  1.226106 3.808809 \tl:3.389177 \tem:0.400000\n",
      "Batch:\t3 /13\t:  1.241027 5.051032 \tl:2.700819 \tem:0.444444\n",
      "Batch:\t4 /13\t:  1.221238 6.272760 \tl:2.776848 \tem:0.488889\n",
      "Batch:\t5 /13\t:  1.320909 7.594339 \tl:3.141070 \tem:0.400000\n",
      "Batch:\t6 /13\t:  1.246258 8.841134 \tl:3.814594 \tem:0.400000\n",
      "Batch:\t7 /13\t:  1.427456 10.26940 \tl:3.730535 \tem:0.288889\n",
      "Batch:\t8 /13\t:  1.641709 11.91193 \tl:3.709289 \tem:0.400000\n",
      "Batch:\t9 /13\t:  1.510905 13.42386 \tl:3.278316 \tem:0.422222\n",
      "Batch:\t10 /13\t:  1.305931 14.73028 \tl:2.576718 \tem:0.533333\n",
      "Batch:\t11 /13\t:  1.185679 15.91640 \tl:2.099265 \tem:0.511111\n",
      "Batch:\t12 /13\t:  1.381909 17.29950 \tl:3.040312 \tem:0.377778\n",
      "\n",
      "Epoch performance:  17.836sec Trl:3.180692 \tTrem:0.425641 \tTeem:0.000000\n",
      "\n",
      "Epoch:  150 / 300\n",
      "Batch:\t0 /13\t:  1.156914 1.156942 \tl:3.344771 \tem:0.377778\n",
      "Batch:\t1 /13\t:  1.242272 2.400637 \tl:3.625317 \tem:0.333333\n",
      "Batch:\t2 /13\t:  1.165624 3.566743 \tl:3.287210 \tem:0.355556\n",
      "Batch:\t3 /13\t:  1.242500 4.809693 \tl:3.151118 \tem:0.422222\n",
      "Batch:\t4 /13\t:  1.256301 6.066459 \tl:3.807613 \tem:0.266667\n",
      "Batch:\t5 /13\t:  1.202366 7.269545 \tl:3.400487 \tem:0.400000\n",
      "Batch:\t6 /13\t:  1.288532 8.559151 \tl:3.719205 \tem:0.377778\n",
      "Batch:\t7 /13\t:  1.297793 9.858062 \tl:3.638788 \tem:0.288889\n",
      "Batch:\t8 /13\t:  1.349178 11.20857 \tl:2.817025 \tem:0.400000\n",
      "Batch:\t9 /13\t:  1.123640 12.33264 \tl:3.804282 \tem:0.244444\n",
      "Batch:\t10 /13\t:  1.305222 13.63909 \tl:2.942511 \tem:0.488889\n",
      "Batch:\t11 /13\t:  1.385032 15.02481 \tl:3.093293 \tem:0.444444\n",
      "Batch:\t12 /13\t:  1.306277 16.33187 \tl:3.503225 \tem:0.355556\n",
      "\n",
      "Epoch performance:  16.868sec Trl:3.394988 \tTrem:0.365812 \tTeem:0.000000\n",
      "\n",
      "Epoch:  151 / 300\n",
      "Batch:\t0 /13\t:  1.194878 1.194895 \tl:2.756490 \tem:0.400000\n",
      "Batch:\t1 /13\t:  1.220422 2.415897 \tl:2.985301 \tem:0.444444\n",
      "Batch:\t2 /13\t:  1.347092 3.765553 \tl:3.642423 \tem:0.355556\n",
      "Batch:\t3 /13\t:  1.246708 5.012789 \tl:3.214330 \tem:0.444444\n",
      "Batch:\t4 /13\t:  1.255630 6.269224 \tl:3.226663 \tem:0.466667\n",
      "Batch:\t5 /13\t:  1.211643 7.481813 \tl:3.177071 \tem:0.488889\n",
      "Batch:\t6 /13\t:  1.287848 8.770674 \tl:4.712492 \tem:0.288889\n",
      "Batch:\t7 /13\t:  1.228554 9.999727 \tl:3.458354 \tem:0.400000\n",
      "Batch:\t8 /13\t:  1.217151 11.21755 \tl:3.268173 \tem:0.466667\n",
      "Batch:\t9 /13\t:  1.182293 12.40010 \tl:3.556335 \tem:0.400000\n",
      "Batch:\t10 /13\t:  1.344404 13.74479 \tl:3.390215 \tem:0.355556\n",
      "Batch:\t11 /13\t:  1.179613 14.92546 \tl:3.435248 \tem:0.444444\n",
      "Batch:\t12 /13\t:  1.267910 16.20986 \tl:3.783286 \tem:0.266667\n",
      "\n",
      "Epoch performance:  16.752sec Trl:3.431260 \tTrem:0.401709 \tTeem:0.000000\n",
      "\n",
      "Epoch:  152 / 300\n",
      "Batch:\t0 /13\t:  1.581454 1.581470 \tl:3.431293 \tem:0.444444\n",
      "Batch:\t1 /13\t:  1.299937 2.881611 \tl:3.153621 \tem:0.444444\n",
      "Batch:\t2 /13\t:  1.194100 4.076165 \tl:3.317171 \tem:0.355556\n",
      "Batch:\t3 /13\t:  1.243967 5.321125 \tl:3.816102 \tem:0.400000\n",
      "Batch:\t4 /13\t:  1.260557 6.582717 \tl:2.912060 \tem:0.555556\n",
      "Batch:\t5 /13\t:  1.302757 7.886301 \tl:3.579774 \tem:0.400000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t6 /13\t:  1.169108 9.056172 \tl:3.430406 \tem:0.311111\n",
      "Batch:\t7 /13\t:  1.145946 10.20289 \tl:3.185239 \tem:0.288889\n",
      "Batch:\t8 /13\t:  1.217787 11.42167 \tl:2.815740 \tem:0.466667\n",
      "Batch:\t9 /13\t:  1.191339 12.61394 \tl:2.950340 \tem:0.466667\n",
      "Batch:\t10 /13\t:  1.266872 13.88143 \tl:4.225254 \tem:0.266667\n",
      "Batch:\t11 /13\t:  1.275022 15.15707 \tl:3.093291 \tem:0.466667\n",
      "Batch:\t12 /13\t:  1.315347 16.47270 \tl:3.026332 \tem:0.533333\n",
      "\n",
      "Epoch performance:  17.008sec Trl:3.302817 \tTrem:0.415385 \tTeem:0.000000\n",
      "\n",
      "Epoch:  153 / 300\n",
      "Batch:\t0 /13\t:  1.237254 1.237272 \tl:2.924236 \tem:0.444444\n",
      "Batch:\t1 /13\t:  1.263589 2.501828 \tl:3.592330 \tem:0.400000\n",
      "Batch:\t2 /13\t:  1.208928 3.710993 \tl:2.490006 \tem:0.444444\n",
      "Batch:\t3 /13\t:  1.312824 5.025115 \tl:3.585414 \tem:0.355556\n",
      "Batch:\t4 /13\t:  1.180083 6.205634 \tl:3.538699 \tem:0.377778\n",
      "Batch:\t5 /13\t:  1.230077 7.436008 \tl:2.042181 \tem:0.511111\n",
      "Batch:\t6 /13\t:  1.203047 8.639847 \tl:3.441142 \tem:0.377778\n",
      "Batch:\t7 /13\t:  1.219883 9.860626 \tl:2.974392 \tem:0.400000\n",
      "Batch:\t8 /13\t:  1.211637 11.07276 \tl:3.855798 \tem:0.355556\n",
      "Batch:\t9 /13\t:  1.193889 12.26713 \tl:3.017232 \tem:0.422222\n",
      "Batch:\t10 /13\t:  1.264724 13.53244 \tl:2.989621 \tem:0.422222\n",
      "Batch:\t11 /13\t:  1.168546 14.70186 \tl:3.227233 \tem:0.400000\n",
      "Batch:\t12 /13\t:  1.213259 15.91534 \tl:3.911303 \tem:0.177778\n",
      "\n",
      "Epoch performance:  16.453sec Trl:3.199199 \tTrem:0.391453 \tTeem:0.000000\n",
      "\n",
      "Epoch:  154 / 300\n",
      "Batch:\t0 /13\t:  1.268784 1.268801 \tl:3.167700 \tem:0.422222\n",
      "Batch:\t1 /13\t:  1.180640 2.449635 \tl:3.508378 \tem:0.377778\n",
      "Batch:\t2 /13\t:  1.223323 3.673426 \tl:3.321100 \tem:0.377778\n",
      "Batch:\t3 /13\t:  1.142144 4.815786 \tl:2.827786 \tem:0.511111\n",
      "Batch:\t4 /13\t:  1.293497 6.110217 \tl:2.650755 \tem:0.488889\n",
      "Batch:\t5 /13\t:  1.616444 7.726874 \tl:2.376244 \tem:0.466667\n",
      "Batch:\t6 /13\t:  1.288711 9.016093 \tl:3.035869 \tem:0.311111\n",
      "Batch:\t7 /13\t:  1.640774 10.65767 \tl:3.795172 \tem:0.400000\n",
      "Batch:\t8 /13\t:  1.518433 12.17722 \tl:2.952659 \tem:0.444444\n",
      "Batch:\t9 /13\t:  1.320683 13.49875 \tl:3.013967 \tem:0.488889\n",
      "Batch:\t10 /13\t:  1.322566 14.82172 \tl:2.980446 \tem:0.466667\n",
      "Batch:\t11 /13\t:  1.195079 16.01799 \tl:3.087835 \tem:0.444444\n",
      "Batch:\t12 /13\t:  1.222851 17.24141 \tl:2.581796 \tem:0.555556\n",
      "Saving new model on epoch 154\n",
      "\n",
      "Epoch performance:  19.870sec Trl:3.023054 \tTrem:0.442735 \tTeem:0.001709\n",
      "\n",
      "Epoch:  155 / 300\n",
      "Batch:\t0 /13\t:  1.254637 1.254654 \tl:2.889716 \tem:0.488889\n",
      "Batch:\t1 /13\t:  1.359060 2.614222 \tl:3.413762 \tem:0.488889\n",
      "Batch:\t2 /13\t:  1.198141 3.813258 \tl:3.457016 \tem:0.355556\n",
      "Batch:\t3 /13\t:  1.306997 5.120543 \tl:3.267050 \tem:0.333333\n",
      "Batch:\t4 /13\t:  1.229918 6.350962 \tl:3.899980 \tem:0.311111\n",
      "Batch:\t5 /13\t:  1.296139 7.647571 \tl:2.367302 \tem:0.533333\n",
      "Batch:\t6 /13\t:  1.217918 8.866851 \tl:3.727277 \tem:0.333333\n",
      "Batch:\t7 /13\t:  1.674633 10.54167 \tl:2.764542 \tem:0.466667\n",
      "Batch:\t8 /13\t:  1.326785 11.86897 \tl:3.192112 \tem:0.422222\n",
      "Batch:\t9 /13\t:  1.263007 13.13314 \tl:3.311329 \tem:0.355556\n",
      "Batch:\t10 /13\t:  1.352922 14.48626 \tl:2.208155 \tem:0.533333\n",
      "Batch:\t11 /13\t:  1.248578 15.73593 \tl:2.758917 \tem:0.444444\n",
      "Batch:\t12 /13\t:  1.279634 17.01633 \tl:3.552949 \tem:0.466667\n",
      "\n",
      "Epoch performance:  17.554sec Trl:3.139239 \tTrem:0.425641 \tTeem:0.000000\n",
      "\n",
      "Epoch:  156 / 300\n",
      "Batch:\t0 /13\t:  1.401608 1.401629 \tl:3.510158 \tem:0.400000\n",
      "Batch:\t1 /13\t:  1.262310 2.666263 \tl:3.454248 \tem:0.422222\n",
      "Batch:\t2 /13\t:  1.138265 3.804944 \tl:2.383751 \tem:0.444444\n",
      "Batch:\t3 /13\t:  1.225085 5.030549 \tl:3.846035 \tem:0.333333\n",
      "Batch:\t4 /13\t:  1.354659 6.386464 \tl:2.382567 \tem:0.533333\n",
      "Batch:\t5 /13\t:  1.259247 7.646615 \tl:3.330418 \tem:0.422222\n",
      "Batch:\t6 /13\t:  1.188661 8.835700 \tl:2.976926 \tem:0.400000\n",
      "Batch:\t7 /13\t:  1.354120 10.19047 \tl:3.100615 \tem:0.422222\n",
      "Batch:\t8 /13\t:  1.268822 11.46016 \tl:3.483845 \tem:0.422222\n",
      "Batch:\t9 /13\t:  1.271239 12.73191 \tl:4.342210 \tem:0.266667\n",
      "Batch:\t10 /13\t:  1.147330 13.88012 \tl:3.912576 \tem:0.311111\n",
      "Batch:\t11 /13\t:  1.430712 15.31229 \tl:3.862398 \tem:0.355556\n",
      "Batch:\t12 /13\t:  1.158107 16.47089 \tl:3.147428 \tem:0.533333\n",
      "\n",
      "Epoch performance:  17.009sec Trl:3.364090 \tTrem:0.405128 \tTeem:0.001709\n",
      "\n",
      "Epoch:  157 / 300\n",
      "Batch:\t0 /13\t:  1.353677 1.353693 \tl:3.370837 \tem:0.444444\n",
      "Batch:\t1 /13\t:  1.270466 2.624558 \tl:2.861333 \tem:0.444444\n",
      "Batch:\t2 /13\t:  1.187727 3.812685 \tl:2.970069 \tem:0.355556\n",
      "Batch:\t3 /13\t:  1.165446 4.978976 \tl:3.604912 \tem:0.355556\n",
      "Batch:\t4 /13\t:  1.205235 6.191520 \tl:2.873847 \tem:0.466667\n",
      "Batch:\t5 /13\t:  1.252395 7.444607 \tl:3.211544 \tem:0.377778\n",
      "Batch:\t6 /13\t:  1.125802 8.570855 \tl:3.381570 \tem:0.488889\n",
      "Batch:\t7 /13\t:  1.335424 9.906980 \tl:3.734970 \tem:0.400000\n",
      "Batch:\t8 /13\t:  1.168929 11.07650 \tl:3.852409 \tem:0.288889\n",
      "Batch:\t9 /13\t:  1.271027 12.34861 \tl:3.061572 \tem:0.400000\n",
      "Batch:\t10 /13\t:  1.260735 13.61164 \tl:2.902923 \tem:0.466667\n",
      "Batch:\t11 /13\t:  1.241544 14.85444 \tl:2.766295 \tem:0.488889\n",
      "Batch:\t12 /13\t:  1.181118 16.03628 \tl:2.795161 \tem:0.444444\n",
      "\n",
      "Epoch performance:  16.575sec Trl:3.183649 \tTrem:0.417094 \tTeem:0.000000\n",
      "\n",
      "Epoch:  158 / 300\n",
      "Batch:\t0 /13\t:  1.227243 1.227262 \tl:2.775710 \tem:0.400000\n",
      "Batch:\t1 /13\t:  1.229918 2.457958 \tl:3.600953 \tem:0.422222\n",
      "Batch:\t2 /13\t:  1.205363 3.664337 \tl:2.808511 \tem:0.422222\n",
      "Batch:\t3 /13\t:  1.599301 5.264554 \tl:3.252326 \tem:0.355556\n",
      "Batch:\t4 /13\t:  1.244272 6.509459 \tl:3.005096 \tem:0.422222\n",
      "Batch:\t5 /13\t:  1.184845 7.694929 \tl:2.968303 \tem:0.488889\n",
      "Batch:\t6 /13\t:  1.245731 8.941123 \tl:2.884991 \tem:0.422222\n",
      "Batch:\t7 /13\t:  1.251252 10.19286 \tl:3.207614 \tem:0.355556\n",
      "Batch:\t8 /13\t:  1.189028 11.38262 \tl:3.855254 \tem:0.422222\n",
      "Batch:\t9 /13\t:  1.313425 12.69666 \tl:2.333384 \tem:0.555556\n",
      "Batch:\t10 /13\t:  1.160748 13.85761 \tl:4.010820 \tem:0.288889\n",
      "Batch:\t11 /13\t:  1.199568 15.05787 \tl:3.792493 \tem:0.400000\n",
      "Batch:\t12 /13\t:  1.324391 16.38306 \tl:2.839598 \tem:0.466667\n",
      "\n",
      "Epoch performance:  16.920sec Trl:3.179619 \tTrem:0.417094 \tTeem:0.000000\n",
      "\n",
      "Epoch:  159 / 300\n",
      "Batch:\t0 /13\t:  1.305752 1.305768 \tl:2.945448 \tem:0.444444\n",
      "Batch:\t1 /13\t:  1.382128 2.689450 \tl:2.800496 \tem:0.466667\n",
      "Batch:\t2 /13\t:  1.276658 3.966899 \tl:3.857956 \tem:0.311111\n",
      "Batch:\t3 /13\t:  1.204758 5.172170 \tl:2.580908 \tem:0.533333\n",
      "Batch:\t4 /13\t:  1.359196 6.532597 \tl:3.535938 \tem:0.288889\n",
      "Batch:\t5 /13\t:  1.247552 7.780691 \tl:3.138557 \tem:0.466667\n",
      "Batch:\t6 /13\t:  1.296734 9.077715 \tl:3.519867 \tem:0.355556\n",
      "Batch:\t7 /13\t:  1.290978 10.37176 \tl:2.923862 \tem:0.466667\n",
      "Batch:\t8 /13\t:  1.288151 11.66053 \tl:2.910898 \tem:0.488889\n",
      "Batch:\t9 /13\t:  1.206807 12.86786 \tl:3.085362 \tem:0.466667\n",
      "Batch:\t10 /13\t:  1.230412 14.09898 \tl:3.279427 \tem:0.488889\n",
      "Batch:\t11 /13\t:  1.333518 15.43342 \tl:3.545100 \tem:0.377778\n",
      "Batch:\t12 /13\t:  1.125085 16.55961 \tl:2.302692 \tem:0.555556\n",
      "\n",
      "Epoch performance:  17.096sec Trl:3.109732 \tTrem:0.439316 \tTeem:0.000000\n",
      "\n",
      "Epoch:  160 / 300\n",
      "Batch:\t0 /13\t:  1.412544 1.412569 \tl:2.644756 \tem:0.422222\n",
      "Batch:\t1 /13\t:  1.264209 2.677598 \tl:2.837090 \tem:0.466667\n",
      "Batch:\t2 /13\t:  1.222455 3.900949 \tl:3.645356 \tem:0.400000\n",
      "Batch:\t3 /13\t:  1.180381 5.082306 \tl:3.458616 \tem:0.444444\n",
      "Batch:\t4 /13\t:  1.226161 6.308765 \tl:3.544902 \tem:0.333333\n",
      "Batch:\t5 /13\t:  1.154820 7.464236 \tl:2.846857 \tem:0.400000\n",
      "Batch:\t6 /13\t:  1.210702 8.676187 \tl:2.808360 \tem:0.511111\n",
      "Batch:\t7 /13\t:  1.197786 9.875 \tl:2.133480 \tem:0.533333\n",
      "Batch:\t8 /13\t:  1.157683 11.03399 \tl:2.233089 \tem:0.555556\n",
      "Batch:\t9 /13\t:  1.291738 12.32647 \tl:2.781018 \tem:0.511111\n",
      "Batch:\t10 /13\t:  1.162521 13.48935 \tl:2.591629 \tem:0.511111\n",
      "Batch:\t11 /13\t:  1.329745 14.81992 \tl:3.309910 \tem:0.377778\n",
      "Batch:\t12 /13\t:  1.244205 16.06503 \tl:3.139333 \tem:0.466667\n",
      "\n",
      "Epoch performance:  16.602sec Trl:2.921107 \tTrem:0.456410 \tTeem:0.000000\n",
      "\n",
      "Epoch:  161 / 300\n",
      "Batch:\t0 /13\t:  1.401710 1.401727 \tl:3.225698 \tem:0.333333\n",
      "Batch:\t1 /13\t:  1.153486 2.555427 \tl:3.469756 \tem:0.422222\n",
      "Batch:\t2 /13\t:  1.210968 3.766961 \tl:3.165710 \tem:0.488889\n",
      "Batch:\t3 /13\t:  1.200028 4.968115 \tl:3.564548 \tem:0.400000\n",
      "Batch:\t4 /13\t:  1.201419 6.170593 \tl:3.612464 \tem:0.333333\n",
      "Batch:\t5 /13\t:  1.223221 7.394369 \tl:2.957931 \tem:0.422222\n",
      "Batch:\t6 /13\t:  1.188994 8.584203 \tl:3.181428 \tem:0.400000\n",
      "Batch:\t7 /13\t:  1.269505 9.854907 \tl:3.305711 \tem:0.466667\n",
      "Batch:\t8 /13\t:  1.237148 11.09261 \tl:3.341126 \tem:0.355556\n",
      "Batch:\t9 /13\t:  1.340070 12.43358 \tl:2.942462 \tem:0.511111\n",
      "Batch:\t10 /13\t:  1.213917 13.64800 \tl:3.264118 \tem:0.333333\n",
      "Batch:\t11 /13\t:  1.259819 14.90862 \tl:3.419216 \tem:0.377778\n",
      "Batch:\t12 /13\t:  1.339548 16.24907 \tl:2.803676 \tem:0.444444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch performance:  16.784sec Trl:3.250296 \tTrem:0.406838 \tTeem:0.000000\n",
      "\n",
      "Epoch:  162 / 300\n",
      "Batch:\t0 /13\t:  1.541599 1.541617 \tl:2.677903 \tem:0.444444\n",
      "Batch:\t1 /13\t:  1.262814 2.805188 \tl:3.035177 \tem:0.400000\n",
      "Batch:\t2 /13\t:  1.250750 4.056433 \tl:3.386211 \tem:0.377778\n",
      "Batch:\t3 /13\t:  1.144889 5.202807 \tl:3.338682 \tem:0.311111\n",
      "Batch:\t4 /13\t:  1.169532 6.372839 \tl:3.074247 \tem:0.422222\n",
      "Batch:\t5 /13\t:  1.208364 7.581678 \tl:2.802576 \tem:0.488889\n",
      "Batch:\t6 /13\t:  1.185503 8.779905 \tl:3.233363 \tem:0.400000\n",
      "Batch:\t7 /13\t:  1.251642 10.03255 \tl:3.366601 \tem:0.511111\n",
      "Batch:\t8 /13\t:  1.306156 11.33955 \tl:3.358422 \tem:0.400000\n",
      "Batch:\t9 /13\t:  1.258062 12.59815 \tl:3.341055 \tem:0.355556\n",
      "Batch:\t10 /13\t:  1.184144 13.78272 \tl:2.589741 \tem:0.466667\n",
      "Batch:\t11 /13\t:  1.210204 14.99320 \tl:2.833583 \tem:0.333333\n",
      "Batch:\t12 /13\t:  1.339417 16.33281 \tl:3.167778 \tem:0.355556\n",
      "\n",
      "Epoch performance:  16.869sec Trl:3.092718 \tTrem:0.405128 \tTeem:0.000000\n",
      "\n",
      "Epoch:  163 / 300\n",
      "Batch:\t0 /13\t:  1.345446 1.345461 \tl:3.516815 \tem:0.266667\n",
      "Batch:\t1 /13\t:  1.161953 2.508371 \tl:2.682917 \tem:0.355556\n",
      "Batch:\t2 /13\t:  1.206251 3.715733 \tl:2.939016 \tem:0.400000\n",
      "Batch:\t3 /13\t:  1.180459 4.897359 \tl:3.354749 \tem:0.333333\n",
      "Batch:\t4 /13\t:  1.306262 6.204699 \tl:3.944737 \tem:0.333333\n",
      "Batch:\t5 /13\t:  1.290081 7.495813 \tl:3.615057 \tem:0.400000\n",
      "Batch:\t6 /13\t:  1.256525 8.753274 \tl:3.914535 \tem:0.288889\n",
      "Batch:\t7 /13\t:  1.287168 10.04185 \tl:3.470768 \tem:0.288889\n",
      "Batch:\t8 /13\t:  1.254582 11.29696 \tl:2.903588 \tem:0.400000\n",
      "Batch:\t9 /13\t:  1.304859 12.60256 \tl:3.050434 \tem:0.422222\n",
      "Batch:\t10 /13\t:  1.220152 13.82368 \tl:3.452148 \tem:0.422222\n",
      "Batch:\t11 /13\t:  1.211119 15.03550 \tl:2.535109 \tem:0.466667\n",
      "Batch:\t12 /13\t:  1.248581 16.28467 \tl:3.335807 \tem:0.333333\n",
      "\n",
      "Epoch performance:  16.820sec Trl:3.285821 \tTrem:0.362393 \tTeem:0.000000\n",
      "\n",
      "Epoch:  164 / 300\n",
      "Batch:\t0 /13\t:  1.373733 1.373754 \tl:2.668238 \tem:0.466667\n",
      "Batch:\t1 /13\t:  1.212390 2.586622 \tl:2.267907 \tem:0.533333\n",
      "Batch:\t2 /13\t:  1.268210 3.855458 \tl:4.252371 \tem:0.377778\n",
      "Batch:\t3 /13\t:  1.148228 5.004189 \tl:2.681967 \tem:0.511111\n",
      "Batch:\t4 /13\t:  1.142132 6.146925 \tl:2.761924 \tem:0.488889\n",
      "Batch:\t5 /13\t:  1.287146 7.434299 \tl:2.147112 \tem:0.488889\n",
      "Batch:\t6 /13\t:  1.172927 8.607805 \tl:3.695079 \tem:0.311111\n",
      "Batch:\t7 /13\t:  1.373723 9.982603 \tl:3.260145 \tem:0.377778\n",
      "Batch:\t8 /13\t:  1.345073 11.32819 \tl:3.675241 \tem:0.400000\n",
      "Batch:\t9 /13\t:  1.381845 12.71059 \tl:3.097730 \tem:0.400000\n",
      "Batch:\t10 /13\t:  1.269749 13.98124 \tl:2.771048 \tem:0.488889\n",
      "Batch:\t11 /13\t:  1.267798 15.24961 \tl:3.671502 \tem:0.400000\n",
      "Batch:\t12 /13\t:  1.128606 16.37875 \tl:3.329097 \tem:0.422222\n",
      "\n",
      "Epoch performance:  16.914sec Trl:3.098412 \tTrem:0.435897 \tTeem:0.000000\n",
      "\n",
      "Epoch:  165 / 300\n",
      "Batch:\t0 /13\t:  1.204101 1.204116 \tl:2.657952 \tem:0.488889\n",
      "Batch:\t1 /13\t:  1.219538 2.424368 \tl:2.613069 \tem:0.444444\n",
      "Batch:\t2 /13\t:  1.292309 3.717231 \tl:3.517406 \tem:0.400000\n",
      "Batch:\t3 /13\t:  1.185807 4.903923 \tl:3.300970 \tem:0.466667\n",
      "Batch:\t4 /13\t:  1.202378 6.107187 \tl:2.800572 \tem:0.355556\n",
      "Batch:\t5 /13\t:  1.257723 7.365539 \tl:3.388329 \tem:0.466667\n",
      "Batch:\t6 /13\t:  1.259076 8.625931 \tl:3.207799 \tem:0.488889\n",
      "Batch:\t7 /13\t:  1.309878 9.936350 \tl:2.771147 \tem:0.466667\n",
      "Batch:\t8 /13\t:  1.177383 11.11393 \tl:4.202001 \tem:0.355556\n",
      "Batch:\t9 /13\t:  1.233808 12.34873 \tl:2.942402 \tem:0.377778\n",
      "Batch:\t10 /13\t:  1.157788 13.50714 \tl:3.370357 \tem:0.288889\n",
      "Batch:\t11 /13\t:  1.206681 14.71475 \tl:3.330110 \tem:0.444444\n",
      "Batch:\t12 /13\t:  1.219971 15.93593 \tl:2.707810 \tem:0.466667\n",
      "\n",
      "Epoch performance:  16.475sec Trl:3.139225 \tTrem:0.423932 \tTeem:0.000000\n",
      "\n",
      "Epoch:  166 / 300\n",
      "Batch:\t0 /13\t:  1.374227 1.374241 \tl:3.449699 \tem:0.377778\n",
      "Batch:\t1 /13\t:  1.283385 2.658133 \tl:3.539675 \tem:0.355556\n",
      "Batch:\t2 /13\t:  1.312797 3.971246 \tl:3.662590 \tem:0.200000\n",
      "Batch:\t3 /13\t:  1.301302 5.273280 \tl:3.891577 \tem:0.311111\n",
      "Batch:\t4 /13\t:  1.323018 6.596869 \tl:2.678933 \tem:0.444444\n",
      "Batch:\t5 /13\t:  1.174309 7.771562 \tl:3.128988 \tem:0.400000\n",
      "Batch:\t6 /13\t:  1.197306 8.969167 \tl:3.095139 \tem:0.422222\n",
      "Batch:\t7 /13\t:  1.323805 10.29350 \tl:3.240199 \tem:0.488889\n",
      "Batch:\t8 /13\t:  1.258385 11.55247 \tl:3.002463 \tem:0.466667\n",
      "Batch:\t9 /13\t:  1.452647 13.00567 \tl:2.825697 \tem:0.488889\n",
      "Batch:\t10 /13\t:  1.292881 14.29920 \tl:2.601982 \tem:0.422222\n",
      "Batch:\t11 /13\t:  1.396147 15.69567 \tl:3.265102 \tem:0.355556\n",
      "Batch:\t12 /13\t:  1.182689 16.87912 \tl:2.939383 \tem:0.466667\n",
      "\n",
      "Epoch performance:  17.415sec Trl:3.178571 \tTrem:0.400000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  167 / 300\n",
      "Batch:\t0 /13\t:  1.356208 1.356227 \tl:2.821463 \tem:0.488889\n",
      "Batch:\t1 /13\t:  1.600996 2.959205 \tl:2.729771 \tem:0.488889\n",
      "Batch:\t2 /13\t:  1.408913 4.369459 \tl:2.861296 \tem:0.422222\n",
      "Batch:\t3 /13\t:  1.524891 5.894900 \tl:2.314916 \tem:0.555556\n",
      "Batch:\t4 /13\t:  1.497187 7.392548 \tl:2.950879 \tem:0.466667\n",
      "Batch:\t5 /13\t:  1.540224 8.933483 \tl:3.184514 \tem:0.444444\n",
      "Batch:\t6 /13\t:  1.231938 10.16627 \tl:2.296191 \tem:0.511111\n",
      "Batch:\t7 /13\t:  1.183638 11.35064 \tl:2.340415 \tem:0.555556\n",
      "Batch:\t8 /13\t:  1.248545 12.59979 \tl:2.436702 \tem:0.600000\n",
      "Batch:\t9 /13\t:  1.215590 13.81605 \tl:2.871232 \tem:0.444444\n",
      "Batch:\t10 /13\t:  1.245385 15.06232 \tl:3.743402 \tem:0.400000\n",
      "Batch:\t11 /13\t:  1.324918 16.38767 \tl:2.845564 \tem:0.444444\n",
      "Batch:\t12 /13\t:  1.214463 17.60257 \tl:2.470651 \tem:0.511111\n",
      "\n",
      "Epoch performance:  18.140sec Trl:2.759000 \tTrem:0.487179 \tTeem:0.000000\n",
      "\n",
      "Epoch:  168 / 300\n",
      "Batch:\t0 /13\t:  1.245124 1.245141 \tl:3.598065 \tem:0.377778\n",
      "Batch:\t1 /13\t:  1.235093 2.480984 \tl:1.911268 \tem:0.555556\n",
      "Batch:\t2 /13\t:  1.274867 3.756608 \tl:1.955415 \tem:0.577778\n",
      "Batch:\t3 /13\t:  1.244174 5.003993 \tl:2.858992 \tem:0.488889\n",
      "Batch:\t4 /13\t:  1.213741 6.217973 \tl:3.604551 \tem:0.333333\n",
      "Batch:\t5 /13\t:  1.247900 7.466767 \tl:3.594845 \tem:0.444444\n",
      "Batch:\t6 /13\t:  1.281301 8.748631 \tl:2.510000 \tem:0.488889\n",
      "Batch:\t7 /13\t:  1.148764 9.898393 \tl:3.111084 \tem:0.422222\n",
      "Batch:\t8 /13\t:  1.253026 11.15234 \tl:2.722862 \tem:0.400000\n",
      "Batch:\t9 /13\t:  1.201597 12.35521 \tl:2.712879 \tem:0.511111\n",
      "Batch:\t10 /13\t:  1.336751 13.69293 \tl:3.331286 \tem:0.422222\n",
      "Batch:\t11 /13\t:  1.235760 14.92943 \tl:2.994502 \tem:0.511111\n",
      "Batch:\t12 /13\t:  1.214267 16.14438 \tl:2.823413 \tem:0.444444\n",
      "\n",
      "Epoch performance:  16.681sec Trl:2.902243 \tTrem:0.459829 \tTeem:0.000000\n",
      "\n",
      "Epoch:  169 / 300\n",
      "Batch:\t0 /13\t:  1.276608 1.276626 \tl:2.946391 \tem:0.488889\n",
      "Batch:\t1 /13\t:  1.363888 2.641118 \tl:4.241958 \tem:0.266667\n",
      "Batch:\t2 /13\t:  1.167907 3.809734 \tl:3.246490 \tem:0.400000\n",
      "Batch:\t3 /13\t:  1.269248 5.079452 \tl:3.373363 \tem:0.422222\n",
      "Batch:\t4 /13\t:  1.219105 6.299450 \tl:3.299699 \tem:0.444444\n",
      "Batch:\t5 /13\t:  1.303852 7.604521 \tl:2.251917 \tem:0.533333\n",
      "Batch:\t6 /13\t:  1.321410 8.926561 \tl:2.516574 \tem:0.533333\n",
      "Batch:\t7 /13\t:  1.187539 10.11461 \tl:2.612664 \tem:0.511111\n",
      "Batch:\t8 /13\t:  1.242708 11.35815 \tl:2.256117 \tem:0.577778\n",
      "Batch:\t9 /13\t:  1.215597 12.57468 \tl:3.097125 \tem:0.377778\n",
      "Batch:\t10 /13\t:  1.328812 13.90374 \tl:3.191022 \tem:0.266667\n",
      "Batch:\t11 /13\t:  1.627219 15.53156 \tl:2.711254 \tem:0.533333\n",
      "Batch:\t12 /13\t:  1.398923 16.93140 \tl:2.774300 \tem:0.444444\n",
      "\n",
      "Epoch performance:  17.470sec Trl:2.962990 \tTrem:0.446154 \tTeem:0.000000\n",
      "\n",
      "Epoch:  170 / 300\n",
      "Batch:\t0 /13\t:  1.143671 1.143686 \tl:3.689199 \tem:0.444444\n",
      "Batch:\t1 /13\t:  1.208491 2.352610 \tl:3.452313 \tem:0.400000\n",
      "Batch:\t2 /13\t:  1.165892 3.519541 \tl:2.860855 \tem:0.466667\n",
      "Batch:\t3 /13\t:  1.310269 4.830444 \tl:2.635739 \tem:0.555556\n",
      "Batch:\t4 /13\t:  1.263791 6.094794 \tl:2.836956 \tem:0.400000\n",
      "Batch:\t5 /13\t:  1.216495 7.311762 \tl:3.543625 \tem:0.222222\n",
      "Batch:\t6 /13\t:  1.248893 8.561639 \tl:2.195277 \tem:0.511111\n",
      "Batch:\t7 /13\t:  1.187346 9.749850 \tl:2.360194 \tem:0.466667\n",
      "Batch:\t8 /13\t:  1.187454 10.93805 \tl:3.460357 \tem:0.422222\n",
      "Batch:\t9 /13\t:  1.187998 12.12669 \tl:3.604227 \tem:0.422222\n",
      "Batch:\t10 /13\t:  1.258836 13.38613 \tl:3.126245 \tem:0.422222\n",
      "Batch:\t11 /13\t:  1.581671 14.96916 \tl:3.261170 \tem:0.377778\n",
      "Batch:\t12 /13\t:  1.377778 16.34748 \tl:2.868597 \tem:0.466667\n",
      "\n",
      "Epoch performance:  16.884sec Trl:3.068827 \tTrem:0.429060 \tTeem:0.000000\n",
      "\n",
      "Epoch:  171 / 300\n",
      "Batch:\t0 /13\t:  1.208791 1.208810 \tl:3.199437 \tem:0.377778\n",
      "Batch:\t1 /13\t:  1.345715 2.555107 \tl:3.517891 \tem:0.422222\n",
      "Batch:\t2 /13\t:  1.303221 3.858628 \tl:3.139964 \tem:0.400000\n",
      "Batch:\t3 /13\t:  1.361483 5.221111 \tl:3.333756 \tem:0.400000\n",
      "Batch:\t4 /13\t:  1.169791 6.391461 \tl:2.838735 \tem:0.400000\n",
      "Batch:\t5 /13\t:  1.262983 7.655779 \tl:2.395644 \tem:0.511111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t6 /13\t:  1.203603 8.860396 \tl:2.900811 \tem:0.488889\n",
      "Batch:\t7 /13\t:  1.290712 10.15155 \tl:3.157223 \tem:0.444444\n",
      "Batch:\t8 /13\t:  1.280004 11.43270 \tl:3.017657 \tem:0.377778\n",
      "Batch:\t9 /13\t:  1.197978 12.63176 \tl:2.393865 \tem:0.466667\n",
      "Batch:\t10 /13\t:  1.223631 13.85664 \tl:3.680150 \tem:0.377778\n",
      "Batch:\t11 /13\t:  1.215806 15.07301 \tl:2.962487 \tem:0.466667\n",
      "Batch:\t12 /13\t:  1.291309 16.36517 \tl:2.154743 \tem:0.533333\n",
      "\n",
      "Epoch performance:  16.905sec Trl:2.976336 \tTrem:0.435897 \tTeem:0.000000\n",
      "\n",
      "Epoch:  172 / 300\n",
      "Batch:\t0 /13\t:  1.220772 1.220790 \tl:2.996270 \tem:0.333333\n",
      "Batch:\t1 /13\t:  1.201365 2.422387 \tl:2.422991 \tem:0.533333\n",
      "Batch:\t2 /13\t:  1.159075 3.582309 \tl:3.068401 \tem:0.400000\n",
      "Batch:\t3 /13\t:  1.282635 4.865514 \tl:2.139909 \tem:0.488889\n",
      "Batch:\t4 /13\t:  1.207340 6.073531 \tl:2.909626 \tem:0.533333\n",
      "Batch:\t5 /13\t:  1.245405 7.319589 \tl:3.402286 \tem:0.466667\n",
      "Batch:\t6 /13\t:  1.188973 8.509588 \tl:2.176204 \tem:0.466667\n",
      "Batch:\t7 /13\t:  1.262614 9.772964 \tl:3.248294 \tem:0.400000\n",
      "Batch:\t8 /13\t:  1.253427 11.02706 \tl:3.196914 \tem:0.355556\n",
      "Batch:\t9 /13\t:  1.193423 12.22092 \tl:3.033515 \tem:0.377778\n",
      "Batch:\t10 /13\t:  1.318970 13.54099 \tl:2.903138 \tem:0.400000\n",
      "Batch:\t11 /13\t:  1.209440 14.75133 \tl:3.646971 \tem:0.400000\n",
      "Batch:\t12 /13\t:  1.219641 15.97136 \tl:3.173637 \tem:0.400000\n",
      "\n",
      "Epoch performance:  16.510sec Trl:2.947550 \tTrem:0.427350 \tTeem:0.000000\n",
      "\n",
      "Epoch:  173 / 300\n",
      "Batch:\t0 /13\t:  1.231326 1.231343 \tl:2.767354 \tem:0.466667\n",
      "Batch:\t1 /13\t:  1.323728 2.555871 \tl:3.355748 \tem:0.422222\n",
      "Batch:\t2 /13\t:  1.197595 3.754300 \tl:2.276483 \tem:0.622222\n",
      "Batch:\t3 /13\t:  1.341230 5.095970 \tl:3.099528 \tem:0.400000\n",
      "Batch:\t4 /13\t:  1.190521 6.287109 \tl:3.501249 \tem:0.377778\n",
      "Batch:\t5 /13\t:  1.239046 7.527048 \tl:2.424271 \tem:0.555556\n",
      "Batch:\t6 /13\t:  1.235470 8.763018 \tl:2.841223 \tem:0.466667\n",
      "Batch:\t7 /13\t:  1.251286 10.01480 \tl:3.274395 \tem:0.400000\n",
      "Batch:\t8 /13\t:  1.158424 11.17399 \tl:2.963900 \tem:0.400000\n",
      "Batch:\t9 /13\t:  1.242889 12.41713 \tl:2.820658 \tem:0.400000\n",
      "Batch:\t10 /13\t:  1.169051 13.58644 \tl:2.866539 \tem:0.422222\n",
      "Batch:\t11 /13\t:  1.205586 14.79242 \tl:3.172713 \tem:0.333333\n",
      "Batch:\t12 /13\t:  1.190953 15.98394 \tl:2.507247 \tem:0.422222\n",
      "\n",
      "Epoch performance:  16.521sec Trl:2.913177 \tTrem:0.437607 \tTeem:0.000000\n",
      "\n",
      "Epoch:  174 / 300\n",
      "Batch:\t0 /13\t:  1.272284 1.272300 \tl:3.123266 \tem:0.333333\n",
      "Batch:\t1 /13\t:  1.250241 2.523463 \tl:2.699290 \tem:0.400000\n",
      "Batch:\t2 /13\t:  1.233006 3.757040 \tl:3.022867 \tem:0.377778\n",
      "Batch:\t3 /13\t:  1.196398 4.953871 \tl:2.652755 \tem:0.555556\n",
      "Batch:\t4 /13\t:  1.182011 6.136388 \tl:3.584595 \tem:0.311111\n",
      "Batch:\t5 /13\t:  1.243026 7.380254 \tl:3.204130 \tem:0.444444\n",
      "Batch:\t6 /13\t:  1.181235 8.561748 \tl:2.384069 \tem:0.488889\n",
      "Batch:\t7 /13\t:  1.201333 9.763556 \tl:2.914845 \tem:0.488889\n",
      "Batch:\t8 /13\t:  1.222538 10.98640 \tl:2.997647 \tem:0.422222\n",
      "Batch:\t9 /13\t:  1.218966 12.20568 \tl:2.731528 \tem:0.422222\n",
      "Batch:\t10 /13\t:  1.319672 13.52663 \tl:2.682869 \tem:0.400000\n",
      "Batch:\t11 /13\t:  1.221629 14.74843 \tl:3.318693 \tem:0.488889\n",
      "Batch:\t12 /13\t:  1.177008 15.92628 \tl:3.090877 \tem:0.444444\n",
      "\n",
      "Epoch performance:  16.466sec Trl:2.954418 \tTrem:0.429060 \tTeem:0.000000\n",
      "\n",
      "Epoch:  175 / 300\n",
      "Batch:\t0 /13\t:  1.137360 1.137389 \tl:2.957975 \tem:0.422222\n"
     ]
    }
   ],
   "source": [
    "op = training_loop(_models=[ques_model, para_model, mlstm_model, pointer_decoder_model],\n",
    "                   _data=data,\n",
    "                   _debug=macros['debug'],\n",
    "                   _save=-1,\n",
    "                   _test_eval=1,\n",
    "                   _train_eval=1,\n",
    "                   _epochs=EPOCHS,\n",
    "                   _macros=macros)    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# See if gradients are being passed\n",
    "p = list(filter(lambda p: p.requires_grad, ques_model.parameters())) + \\\n",
    "    list(filter(lambda p: p.requires_grad, para_model.parameters())) + \\\n",
    "    list(para_model.parameters()) + \\\n",
    "    list(pointer_decoder_model.parameters())\n",
    "                       \n",
    "print([ x.grad.sum().item() for x in p])                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "\n",
    "So far, we plot the training losss. \n",
    "Shall we superimpose test loss on it too? We don't calculate test loss per batch though (fortunately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "print(\"Training Loss\")\n",
    "visualize_loss(op[0], \"train loss\", _only_epoch=True)\n",
    "\n",
    "# if len(op[1]) > 0:\n",
    "\n",
    "print(\"Training EM\")\n",
    "visualize_loss(op[1], \"train em\", _only_epoch=True)\n",
    "\n",
    "print(\"Testing EM\")\n",
    "visualize_loss(op[3], \"test em\")\n",
    "\n",
    "\n",
    "# print(op[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing (temp)\n",
    "# models = { 'ques_model': ques_model,\n",
    "#            'para_model': para_model,\n",
    "#            'mlstm_model':  mlstm_model,\n",
    "#            'pointer_decoder_model': pointer_decoder_model\n",
    "#          }\n",
    "# save_model(loc=macros['save_model_loc'], models=models, epochs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading the model\n",
    "ques_model = torch.load(os.path.join(macros['save_model_loc'], 'ques_model.torch'))\n",
    "print(\"Ques Model\\n\", ques_model)\n",
    "\n",
    "para_model = torch.load(os.path.join(macros['save_model_loc'], 'para_model.torch'))\n",
    "print(\"Para Model\\n\", para_model)\n",
    "\n",
    "mlstm_model = torch.load(os.path.join(macros['save_model_loc'], 'mlstm_model.torch'))\n",
    "print(\"MLSTM Model\\n\", mlstm_model)\n",
    "\n",
    "pointer_decoder_model = torch.load(os.path.join(macros['save_model_loc'], 'pointer_decoder_model.torch'))\n",
    "print(\"Pointer Decoder model\\n\", pointer_decoder_model)\n",
    "\n",
    "# Create dummy data for testing the predict fn\n",
    "q = np.random.randint(0, len(vectors), (3, 30))\n",
    "p = np.random.randint(0, len(vectors), (3, 200))\n",
    "\n",
    "y_cap_start, y_cap_end, _ = predict(torch.tensor(p, dtype=torch.long, device=device), \n",
    "                                   torch.tensor(q, dtype=torch.long, device=device),\n",
    "                                   ques_model=ques_model,\n",
    "                                   para_model=para_model,\n",
    "                                   mlstm_model=mlstm_model,\n",
    "                                   pointer_decoder_model=pointer_decoder_model,\n",
    "                                    macros=macros,\n",
    "                                    debug=macros['debug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(y_cap_start.squeeze(), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
