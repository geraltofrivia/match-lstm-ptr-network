{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA over unstructured data\n",
    "\n",
    "Using Match LSTM, Pointer Networks, as mentioned in paper https://arxiv.org/pdf/1608.07905.pdf\n",
    "\n",
    "We start with the pre-processing provided by https://github.com/MurtyShikhar/Question-Answering to clean up the data and make neat para, ques files.\n",
    "\n",
    "\n",
    "### @TODOs:\n",
    "\n",
    "1. [done] _Figure out how to put in real, pre-trained embeddings in embeddings layer._\n",
    "2. [done] _Explicitly provide batch size when instantiating model_\n",
    "3. [done] is ./val.ids.* validation set or test set?: **validation**\n",
    "4. [done:em] emInstead of test loss, calculate test acc metrics\n",
    "    1. todo: new metrics like P, R, F1\n",
    "5. [done] Update unit test codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Codeblock to pull up embeddings. Needs to run before following imports\n",
    "# import numpy as np\n",
    "\n",
    "# # Macros \n",
    "# DATA_LOC = './data/squad/'\n",
    "# EMBEDDING_FILE = 'glove.trimmed.300.npz'\n",
    "# VOCAB_FILE = 'vocab.dat'\n",
    "\n",
    "# file_loc = DATA_LOC + EMBEDDING_FILE\n",
    "# glove_file = np.load(open(file_loc))['glove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "from io import open\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import traceback\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from networks import Encoder, MatchLSTMEncoder, PointerDecoder\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug Legend\n",
    "\n",
    "- 5: Print everything that goes in every tensor.\n",
    "- 4: ??\n",
    "- 3: Check every model individually\n",
    "- 2: Print things in training loops\n",
    "- 1: ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macros \n",
    "DATA_LOC = './data/domain/'\n",
    "MODEL_LOC = './models/mlstms/domain/'\n",
    "DEBUG = 1\n",
    "\n",
    "# nn Macros\n",
    "QUES_LEN, PARA_LEN =  30, 200\n",
    "VOCAB_SIZE = 120000\n",
    "# VOCAB_SIZE = glove_file.shape[1]               # @TODO: get actual size\n",
    "HIDDEN_DIM = 150\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 100                  # Might have total 100 batches.\n",
    "EPOCHS = 300\n",
    "TEST_EVERY_ = 1\n",
    "LR = 0.001\n",
    "CROP = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder \n",
    "Use a simple lstm class to have encoder for question and paragraph. \n",
    "The output of these will be used in the match lstm\n",
    "\n",
    "$H^p = LSTM(P)$ \n",
    "\n",
    "\n",
    "$H^q = LSTM(Q)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "    \n",
    "#     def __init__(self, inputlen, macros, glove_file, device):\n",
    "#         super(Encoder, self).__init__()\n",
    "        \n",
    "#         # Catch dim\n",
    "#         self.inputlen = inputlen\n",
    "#         self.hiddendim = macros['hidden_dim']\n",
    "#         self.embeddingdim =  macros['embedding_dim']\n",
    "#         self.vocablen = macros['vocab_size']\n",
    "# #         self.device = macros['device']\n",
    "#         self.batch_size = macros['batch_size']\n",
    "#         self.debug = macros['debug']\n",
    "        \n",
    "#         # Embedding Layer\n",
    "# #         self.embedding = nn.Embedding(self.vocablen, self.embeddingdim)\n",
    "#         self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(glove_file))\n",
    "#         self.embedding.weight.requires_grad = True\n",
    "       \n",
    "#         # LSTM Layer\n",
    "#         self.lstm = nn.LSTM(self.embeddingdim, self.hiddendim, bidirectional=True)\n",
    "        \n",
    "#     def init_hidden(self, batch_size, device):\n",
    "        \n",
    "#         # Returns a new hidden layer var for LSTM\n",
    "#         return (torch.zeros((2, batch_size, self.hiddendim), device=device), \n",
    "#                 torch.zeros((2, batch_size, self.hiddendim), device=device))\n",
    "    \n",
    "#     def forward(self, x, h):\n",
    "        \n",
    "#         # Input: x (batch, len ) (current input)\n",
    "#         # Hidden: h (1, batch, hiddendim) (last hidden state)\n",
    "        \n",
    "#         # Batchsize: b int (inferred)\n",
    "#         b = x.shape[0]\n",
    "        \n",
    "#         if self.debug > 4: print(\"x:\\t\", x.shape)\n",
    "#         if self.debug > 4: print(\"h:\\t\", h[0].shape, h[1].shape)\n",
    "        \n",
    "#         x_emb = self.embedding(x)\n",
    "#         if self.debug > 4: print(\"x_emb:\\t\", x_emb.shape)\n",
    "            \n",
    "#         ycap, h = self.lstm(x_emb.view(-1, b, self.embeddingdim), h)\n",
    "#         if self.debug > 4: print(\"ycap:\\t\", ycap.shape)\n",
    "        \n",
    "#         return ycap, h\n",
    "    \n",
    "    \n",
    "# # with torch.no_grad():\n",
    "# #     print (\"Trying out question encoder LSTM\")\n",
    "# #     model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "# #     dummy_x = torch.tensor([22,45,12], dtype=torch.long)\n",
    "# #     hidden = model.init_hidden()\n",
    "# #     ycap, h = model(dummy_x, hidden)\n",
    "    \n",
    "# #     print(ycap.shape)\n",
    "# #     print(h[0].shape, h[1].shape)\n",
    "\n",
    "\n",
    "# if DEBUG >= 4:\n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         macros = {\n",
    "#         \"ques_len\": QUES_LEN,\n",
    "#         \"hidden_dim\": HIDDEN_DIM, \n",
    "#         \"vocab_size\": VOCAB_SIZE, \n",
    "#         \"batch_size\": BATCH_SIZE,\n",
    "#         \"para_len\": PARA_LEN,\n",
    "#         \"embedding_dim\": EMBEDDING_DIM,\n",
    "#         \"lr\": LR,\n",
    "#         \"debug\":4,\n",
    "#         \"device\":device\n",
    "#     }\n",
    "\n",
    "#         dummy_para = torch.randint(0,VOCAB_SIZE-1,(PARA_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,PARA_LEN).long()\n",
    "#     #     print (dummy_para.shape)\n",
    "#         dummy_question = torch.randint(0,VOCAB_SIZE-1,(QUES_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,QUES_LEN).long()\n",
    "#     #     print (dummy_question.shape)\n",
    "#         glove_file = torch.randn((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "#     #     print(\"LSTM with batches\")\n",
    "#         ques_model = Encoder(QUES_LEN, macros, glove_file).cuda(device)\n",
    "#         para_model = Encoder(QUES_LEN, macros, glove_file).cuda(device)\n",
    "#         ques_hidden = ques_model.init_hidden(BATCH_SIZE)\n",
    "#         para_hidden = para_model.init_hidden(BATCH_SIZE)\n",
    "#         ques_embedded,hidden_ques = ques_model(dummy_question,ques_hidden)\n",
    "#         para_embedded,hidden_para = para_model(dummy_para,para_hidden)\n",
    "        \n",
    "#         print (ques_embedded.shape) # question_length,batch,embedding_dim\n",
    "#         print (para_embedded.shape) # para_length,batch,embedding_dim\n",
    "#         print (hidden_para[0].shape,hidden_para[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match LSTM\n",
    "\n",
    "Use a match LSTM to compute a **summarized sequential vector** for the paragraph w.r.t the question.\n",
    "\n",
    "Consider the summarized vector ($H^r$) as the output of a new decoder, where the inputs are $H^p, H^q$ computed above. \n",
    "\n",
    "1. Attend the para word $i$ with the entire question ($H^q$)\n",
    "  \n",
    "    1. $\\vec{G}_i = tanh(W^qH^q + repeat(W^ph^p_i + W^r\\vec{h^r_{i-1} + b^p}))$\n",
    "    \n",
    "    2. *Computing it*: Here, $\\vec{G}_i$ is equivalent to `energy`, computed differently.\n",
    "    \n",
    "    3. Use a linear layer to compute the content within the $repeat$ fn.\n",
    "    \n",
    "    4. Add with another linear (without bias) with $H_q$\n",
    "    \n",
    "    5. $tanh$ the bloody thing\n",
    "  \n",
    "  \n",
    "2. Softmax over it to get $\\alpha$ weights.\n",
    "\n",
    "    1. $\\vec{\\alpha_i} = softmax(w^t\\vec{G}_i + repeat(b))$\n",
    "    \n",
    "3. Use the attention weight vector $\\vec{\\alpha_i}$ to obtain a weighted version of the question and concat it with the current token of the passage to form a vector $\\vec{z_i}$\n",
    "\n",
    "4. Use $\\vec{z_i}$ to compute the desired $h^r_i$:\n",
    "\n",
    "    1. $ h^r_i = LSTM(\\vec{z_i}, h^r_{i-1}) $\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MatchLSTMEncoder(nn.Module):\n",
    "    \n",
    "#     def __init__(self, macros, device):\n",
    "        \n",
    "#         super(MatchLSTMEncoder, self).__init__()\n",
    "        \n",
    "#         self.hidden_dim = macros['hidden_dim']\n",
    "#         self.ques_len = macros['ques_len']\n",
    "#         self.batch_size = macros['batch_size']\n",
    "#         self.debug = macros['debug']    \n",
    "        \n",
    "#         # Catch lens and params\n",
    "#         self.lin_g_repeat_a_dense = nn.Linear(2*self.hidden_dim, self.hidden_dim)\n",
    "#         self.lin_g_repeat_b_dense = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "#         self.lin_g_nobias = nn.Linear(2*self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        \n",
    "#         self.alpha_i_w = nn.Parameter(torch.rand((self.hidden_dim, 1)))\n",
    "#         self.alpha_i_b = nn.Parameter(torch.rand((1)))\n",
    "        \n",
    "#         self.lstm_summary = nn.LSTM((self.ques_len+1)*2*self.hidden_dim, self.hidden_dim)\n",
    "                                      \n",
    "    \n",
    "#     def forward(self, H_p, h_ri, H_q, hidden, device):\n",
    "#         \"\"\"\n",
    "#             Ideally, we would have manually unrolled the lstm \n",
    "#             but due to memory constraints, we do it in the module.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Find the batchsize\n",
    "#         batch_size = H_p.shape[1]\n",
    "        \n",
    "#         H_r = torch.empty((0, batch_size, self.hidden_dim), device=device, dtype=torch.float)\n",
    "#         H_r = torch.cat((H_r, h_ri), dim=0)\n",
    "        \n",
    "#         if self.debug > 4:\n",
    "#             print( \"H_p:\\t\\t\\t\", H_p.shape)\n",
    "#             print( \"H_q:\\t\\t\\t\", H_q.shape)\n",
    "#             print( \"h_ri:\\t\\t\\t\", h_ri.shape)\n",
    "#             print( \"H_r:\\t\\t\\t\", H_r.shape)\n",
    "#             print( \"hid:\\t\\t\\t\", hidden.shape)\n",
    "        \n",
    "#         for i in range(H_p.shape[0]):\n",
    "            \n",
    "#             # We call the (W^P.H^P + W^rh^r_i-1 + b^P) as lin_repeat_input.\n",
    "            \n",
    "#             # We first write out its two components as\n",
    "#             lin_repeat_input_a = self.lin_g_repeat_a_dense(H_p[i].view(1, batch_size, -1))\n",
    "#             if self.debug > 4: print(\"lin_repeat_input_a:\\t\", lin_repeat_input_a.shape)\n",
    "            \n",
    "#             lin_repeat_input_b = self.lin_g_repeat_b_dense(H_r[i].view(1, batch_size, -1))\n",
    "#             if self.debug > 4: print(\"lin_repeat_input_b:\\t\", lin_repeat_input_b.shape)\n",
    "            \n",
    "#             # Add the two terms up\n",
    "#             lin_repeat_input_a.add_(lin_repeat_input_b)\n",
    "# #             if self.debug > 4: print(\"lin_g_input_b unrepeated:\", lin_g_input_b.shape)\n",
    "\n",
    "#             lin_g_input_b = lin_repeat_input_a.repeat(H_q.shape[0], 1, 1)\n",
    "#             if self.debug > 4: print(\"lin_g_input_b:\\t\\t\", lin_g_input_b.shape)\n",
    "\n",
    "#             # lin_g_input_a = self.lin_g_nobias.matmul(H_q.view(-1, self.ques_len, self.hidden_dim)) #self.lin_g_nobias(H_q)\n",
    "#             lin_g_input_a =  self.lin_g_nobias(H_q)\n",
    "#             if self.debug > 4: print(\"lin_g_input_a:\\t\\t\", lin_g_input_a.shape)\n",
    "\n",
    "#             G_i = F.tanh(lin_g_input_a + lin_g_input_b)\n",
    "#             if self.debug > 4: print(\"G_i:\\t\\t\\t\", G_i.shape)\n",
    "#             # Note; G_i should be a 1D vector over ques_len\n",
    "\n",
    "#             # Attention weights\n",
    "#             alpha_i_input_a = G_i.view(batch_size, -1, self.hidden_dim).matmul(self.alpha_i_w).view(batch_size, 1, -1)\n",
    "#             if self.debug > 4: print(\"alpha_i_input_a:\\t\", alpha_i_input_a.shape)\n",
    "\n",
    "#             alpha_i_input = alpha_i_input_a.add_(self.alpha_i_b.view(-1,1,1).repeat(1,1,self.ques_len))\n",
    "#             if self.debug > 4: print(\"alpha_i_input:\\t\\t\", alpha_i_input.shape)\n",
    "\n",
    "#             # Softmax over alpha inputs\n",
    "#             alpha_i = F.softmax(alpha_i_input, dim=-1)\n",
    "#             if self.debug > 4: print(\"alpha_i:\\t\\t\", alpha_i.shape)\n",
    "\n",
    "#             # Weighted summary of question with alpha    \n",
    "#             z_i_input_b = (\n",
    "#                             H_q.view(batch_size, self.ques_len, -1) *\n",
    "#                            (alpha_i.view(batch_size, self.ques_len, -1).repeat(1, 1, 2*self.hidden_dim))\n",
    "#                           ).view(self.ques_len,batch_size, -1)\n",
    "#             if self.debug > 4: print(\"z_i_input_b:\\t\\t\", z_i_input_b.shape)\n",
    "\n",
    "#             z_i = torch.cat((H_p[i].view(1, batch_size, -1), z_i_input_b), dim=0)\n",
    "#             if self.debug > 4: print(\"z_i:\\t\\t\\t\", z_i.shape)\n",
    "\n",
    "#             # Pass z_i, h_ri to the LSTM \n",
    "# #             lstm_input = torch.cat((z_i.view(1, batch_size,-1), H_r[i].view(1, batch_size, -1)), dim=2)\n",
    "# #             if self.debug > 4: print(\"lstm_input:\\t\\t\", lstm_input.shape)\n",
    "\n",
    "#             # Take input from LSTM, concat in H_r and nullify the temp var.\n",
    "#             h_ri, (_, hidden) = self.lstm_summary(z_i.view(1, batch_size, -1), \n",
    "#                                              (H_r[i].view(1,batch_size, -1), hidden))\n",
    "#             if self.debug > 4:\n",
    "#                 print(\"newh_ri:\\t\\t\", h_ri.shape)\n",
    "#                 print(\"newhidden:\\t\\t\", hidden.shape)\n",
    "#             H_r = torch.cat((H_r, h_ri), dim=0)\n",
    "# #             h_ri = None\n",
    "            \n",
    "#             if self.debug > 4:\n",
    "#                 print(\"\\tH_r:\\t\\t\\t\", H_r.shape)\n",
    "# #                 print(\"hidden new:\\t\\t\", hidden[0].shape, hidden[1].shape)\n",
    "\n",
    "#         return H_r[1:]\n",
    "    \n",
    "#     def init_hidden(self, batch_size, device):\n",
    "#         # Before we've done anything, we dont have any hidden state.\n",
    "#         # Refer to the Pytorch documentation to see exactly\n",
    "#         # why they have this dimensionality.\n",
    "#         # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "#         return torch.zeros((1, batch_size, self.hidden_dim), device=device)\n",
    "# #                 torch.zeros((1, batch_size, self.hidden_dim), device=device))\n",
    "\n",
    "# # with torch.no_grad():\n",
    "# #     model = MatchLSTMEncoder(HIDDEN_DIM, QUES_LEN)\n",
    "# #     h_pi = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "# #     h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM)\n",
    "# #     hidden = model.init_hidden()\n",
    "# #     H_q = torch.randn(QUES_LEN, BATCH_SIZE, HIDDEN_DIM)\n",
    "    \n",
    "# #     op, hid = model(h_pi, h_ri, H_q, hidden)\n",
    "    \n",
    "# #     print(\"\\nDone:op\", op.shape)\n",
    "# #     print(\"Done:hid\", hid[0].shape, hid[1].shape)\n",
    "\n",
    "# if DEBUG >= 4:\n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         macros = {\n",
    "#             \"ques_len\": QUES_LEN,\n",
    "#             \"hidden_dim\": HIDDEN_DIM, \n",
    "#             \"vocab_size\": VOCAB_SIZE, \n",
    "#             \"batch_size\": BATCH_SIZE,\n",
    "#             \"para_len\": PARA_LEN,\n",
    "#             \"embedding_dim\": EMBEDDING_DIM,\n",
    "#             \"lr\": LR,\n",
    "#             \"debug\":5,\n",
    "#             \"device\":device\n",
    "#         }\n",
    "            \n",
    "#         matchLSTMEncoder = MatchLSTMEncoder(macros).cuda(device)\n",
    "#         hidden = matchLSTMEncoder.init_hidden(BATCH_SIZE)\n",
    "#         para_embedded = torch.rand((PARA_LEN, BATCH_SIZE, 2*HIDDEN_DIM), device=device)\n",
    "#         ques_embedded = torch.rand((QUES_LEN, BATCH_SIZE, 2*HIDDEN_DIM), device=device)\n",
    "#         h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM, device=self.device)\n",
    "#     #     if DEBUG:\n",
    "#     #         print (\"init h_ri shape is: \", h_ri.shape)\n",
    "#     #         print (\"the para length is \", len(para_embedded))\n",
    "#         H_r = matchLSTMEncoder(para_embedded.view(-1,BATCH_SIZE,2*HIDDEN_DIM),\n",
    "#                                h_ri, \n",
    "#                                ques_embedded, \n",
    "#                                hidden)\n",
    "#         print(\"H_r: \", H_r.shape)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network\n",
    "\n",
    "Using a ptrnet over $H_r$ to unfold and get most probable spans.\n",
    "We use the **boundry model** to do that (predict start and end of seq).\n",
    "\n",
    "A simple energy -> softmax -> decoder. Where softmaxed energy is supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# class PointerDecoder(nn.Module):\n",
    "    \n",
    "#     def __init__(self, macros, device):\n",
    "#         super(PointerDecoder, self).__init__()\n",
    "        \n",
    "#         # Keep args\n",
    "#         self.hidden_dim = macros['hidden_dim']\n",
    "#         self.batch_size = macros['batch_size']\n",
    "#         self.para_len = macros['para_len']\n",
    "#         self.debug = macros['debug']\n",
    "        \n",
    "#         self.lin_f_repeat = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "#         self.lin_f_nobias = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        \n",
    "#         self.beta_k_w = nn.Parameter(torch.randn(self.hidden_dim, 1))\n",
    "#         self.beta_k_b = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "#         self.lstm = nn.LSTM(self.hidden_dim*self.para_len, self.hidden_dim)\n",
    "\n",
    "    \n",
    "#     def init_hidden(self, batch_size, device):\n",
    "#         # Before we've done anything, we dont have any hidden state.\n",
    "#         # Refer to the Pytorch documentation to see exactly\n",
    "#         # why they have this dimensionality.\n",
    "#         # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "#         return torch.zeros((1, batch_size, self.hidden_dim), device=device)\n",
    "# #                 torch.zeros((1, batch_size, self.hidden_dim), device=device))\n",
    "    \n",
    "#     def forward(self, h_ak, H_r, hidden):\n",
    "        \n",
    "#         # h_ak (current decoder's last op) (1,batch,hiddendim)\n",
    "#         # H_r (weighted summary of para) (P, batch, hiddendim)\n",
    "#         batch_size = H_r.shape[1]\n",
    "        \n",
    "#         if self.debug > 4:\n",
    "#             print(\"h_ak:\\t\\t\\t\", h_ak.shape)\n",
    "#             print(\"H_r:\\t\\t\\t\", H_r.shape)\n",
    "#             print(\"hidden:\\t\\t\\t\", hidden.shape)\n",
    "            \n",
    "#         # Prepare inputs for the tanh used to compute energy\n",
    "#         f_input_b = self.lin_f_repeat(h_ak)\n",
    "#         if self.debug > 4: print(\"f_input_b unrepeated:  \", f_input_b.shape)\n",
    "        \n",
    "#         #H_r shape is ([PARA_LEN, BATCHSIZE, EmbeddingDIM])\n",
    "#         f_input_b = f_input_b.repeat(H_r.shape[0], 1, 1)\n",
    "#         if self.debug > 4: print(\"f_input_b repeated:\\t\", f_input_b.shape)\n",
    "            \n",
    "#         f_input_a = self.lin_f_nobias(H_r)\n",
    "#         if self.debug > 4: print(\"f_input_a:\\t\\t\", f_input_a.shape)\n",
    "            \n",
    "#         # Send it off to tanh now\n",
    "#         F_k = F.tanh(f_input_a+f_input_b)\n",
    "#         if self.debug > 4: print(\"F_k:\\t\\t\\t\", F_k.shape) #PARA_LEN,BATCHSIZE,EmbeddingDim\n",
    "            \n",
    "#         # Attention weights\n",
    "#         beta_k_input_a = F_k.view(batch_size, -1, self.hidden_dim).matmul(self.beta_k_w).view(batch_size, 1, -1)\n",
    "#         if self.debug > 4: print(\"beta_k_input_a:\\t\\t\", beta_k_input_a.shape)\n",
    "            \n",
    "#         beta_k_input = beta_k_input_a.add_(self.beta_k_b.repeat(1,1,self.para_len))\n",
    "#         if self.debug > 4: print(\"beta_k_input:\\t\\t\", beta_k_input.shape)\n",
    "            \n",
    "#         beta_k = F.softmax(beta_k_input, dim=-1)\n",
    "#         if self.debug > 4: print(\"beta_k:\\t\\t\\t\", beta_k.shape)\n",
    "            \n",
    "#         lstm_input_a = H_r.view(batch_size, self.para_len, -1) * (beta_k.view(batch_size, self.para_len, -1).repeat(1,1,self.hidden_dim))\n",
    "#         if self.debug > 4: print(\"lstm_input_a:\\t\\t\", lstm_input_a.shape)\n",
    "            \n",
    "# #         lstm_input = torch.cat((lstm_input_a.view(1, batch_size,-1), h_ak.view(1, batch_size, -1)), dim=2)\n",
    "# #         if self.debug > 4: print(\"lstm_input:\\t\\t\", lstm_input.shape)\n",
    "        \n",
    "#         h_ak, (_, hidden) = self.lstm(lstm_input_a.view(1, batch_size, -1), (h_ak, hidden))\n",
    "        \n",
    "#         return h_ak, hidden, F.log_softmax(beta_k_input, dim=-1)\n",
    "            \n",
    "# if DEBUG > 4:\n",
    "#     with torch.no_grad():\n",
    "#         macros = {\n",
    "#             \"ques_len\": QUES_LEN,\n",
    "#             \"hidden_dim\": HIDDEN_DIM, \n",
    "#             \"vocab_size\": VOCAB_SIZE, \n",
    "#             \"batch_size\": BATCH_SIZE,\n",
    "#             \"para_len\": PARA_LEN,\n",
    "#             \"embedding_dim\": EMBEDDING_DIM,\n",
    "#             \"lr\": LR,\n",
    "#             \"debug\":5,\n",
    "#             \"device\":device\n",
    "#         }\n",
    "        \n",
    "#         pointerDecoder = PointerDecoder(macros).cuda(device)\n",
    "#         h_ak = torch.randn(1,BATCH_SIZE,HIDDEN_DIM, device=device)\n",
    "#         H_r = torch.randn(PARA_LEN, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "#         hidden = pointerDecoder.init_hidden(BATCH_SIZE)\n",
    "#         h_ak, hidden, beta_k = pointerDecoder(h_ak, H_r, hidden)\n",
    "#         print (beta_k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull the real data from disk.\n",
    "\n",
    "Files stored in `./data/squad/train.ids.*`\n",
    "Pull both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_loc, macros, crop=None):\n",
    "    \"\"\"\n",
    "        Given the dataloc and the data available in a specific format, it would pick the data up, and make trainable matrices,\n",
    "        Harvest train_P, train_Q, train_Y, test_P, test_Q, test_Y matrices in this format\n",
    "        \n",
    "        If crop given, will trim the data at a certain length\n",
    "        \n",
    "        **return_type**: np matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpacking macros\n",
    "    PARA_LEN = macros['para_len']\n",
    "    QUES_LEN = macros['ques_len']\n",
    "    \n",
    "    train_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.ids.question')))])\n",
    "    train_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.ids.context')))])\n",
    "    train_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.span')))])\n",
    "\n",
    "    test_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.ids.question')))])\n",
    "    test_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.ids.context')))])\n",
    "    test_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.span')))])\n",
    "\n",
    "    if macros['debug'] > 3:\n",
    "        print(\"Train Q: \", train_q.shape)\n",
    "        print(\"Train P: \", train_p.shape)\n",
    "        print(\"Train Y: \", train_y.shape)\n",
    "        print(\"Test Q: \", test_q.shape)\n",
    "        print(\"Test P: \", test_p.shape)\n",
    "        print(\"Test Y: \", test_y.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "        Parse the semi-raw data:\n",
    "            - shuffle\n",
    "            - pad, prepare\n",
    "            - dump useless vars\n",
    "    \"\"\"\n",
    "    # Shuffle data\n",
    "    \n",
    "    if crop:\n",
    "        index_train, index_test = np.random.choice(np.arange(len(train_p)), crop), \\\n",
    "                                  np.random.choice(np.arange(len(test_p)), crop)\n",
    "    else:\n",
    "        index_train, index_test = np.arange(len(train_p)), np.arange(len(test_p))\n",
    "        np.random.shuffle(index_train)\n",
    "        np.random.shuffle(index_test)\n",
    "\n",
    "    train_p, train_q, train_y = train_p[index_train], train_q[index_train], train_y[index_train]\n",
    "    test_p, test_q, test_y = test_p[index_test], test_q[index_test], test_y[index_test]\n",
    "\n",
    "#     sanity_check(train_p, train_y)\n",
    "\n",
    "    if macros['debug'] >= 5:\n",
    "        print(\"Max q len: \", max(len(q) for q in train_q))\n",
    "        \n",
    "    \n",
    "    # Pad and prepare\n",
    "    train_P = np.zeros((len(train_p), PARA_LEN))\n",
    "    train_Q = np.zeros((len(train_q), QUES_LEN))\n",
    "    train_Y_start = np.zeros((len(train_p), PARA_LEN))\n",
    "    train_Y_end = np.zeros((len(train_p), PARA_LEN))\n",
    "\n",
    "    test_P = np.zeros((len(test_p), PARA_LEN))\n",
    "    test_Q = np.zeros((len(test_q), QUES_LEN))\n",
    "    test_Y_start = np.zeros((len(test_p), PARA_LEN))\n",
    "    test_Y_end = np.zeros((len(test_p), PARA_LEN))\n",
    "    \n",
    "#     print(train_P.shape)\n",
    "\n",
    "    crop_train = []    # Remove these rows from training\n",
    "    for i in range(len(train_p)):\n",
    "        p = train_p[i]\n",
    "        q = train_q[i]\n",
    "        y = train_y[i]\n",
    "        \n",
    "        # First see if you can keep this example or not (due to size)\n",
    "        if y[0] >= PARA_LEN or y[1] >= PARA_LEN:\n",
    "            crop_train.append(i)\n",
    "            continue\n",
    "\n",
    "\n",
    "        train_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "        train_Q[i, :min(QUES_LEN, len(q))] = q[:min(QUES_LEN, len(q))]\n",
    "        train_Y_start[i, y[0]] = 1\n",
    "        train_Y_end[i, y[1]] = 1\n",
    "\n",
    "    crop_test = []\n",
    "    for i in range(len(test_p)):\n",
    "        p = test_p[i]\n",
    "        q = test_q[i]\n",
    "        y = test_y[i]\n",
    "\n",
    "        # First see if you can keep this example or not (due to size)\n",
    "        if y[0] >= PARA_LEN or y[1] >= PARA_LEN:\n",
    "            crop_test.append(i)\n",
    "            continue\n",
    "\n",
    "        test_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "        test_Q[i, :min(QUES_LEN, len(q))] = q[:min(QUES_LEN, len(q))]\n",
    "        test_Y_start[i, y[0]] = 1\n",
    "        test_Y_end[i, y[1]] = 1\n",
    "        \n",
    "        \n",
    "    # Remove the instances which are in crop_train\n",
    "    train_P = np.delete(train_P, crop_train, axis=0)\n",
    "    train_Q = np.delete(train_Q, crop_train, axis=0)\n",
    "    train_Y_start = np.delete(train_Y_start, crop_train, axis=0)\n",
    "    train_Y_end = np.delete(train_Y_end, crop_train, axis=0)\n",
    "    \n",
    "    test_P = np.delete(test_P, crop_test, axis=0)\n",
    "    test_Q = np.delete(test_Q, crop_test, axis=0)\n",
    "    test_Y_start = np.delete(test_Y_start, crop_test, axis=0)\n",
    "    test_Y_end = np.delete(test_Y_end, crop_test, axis=0)\n",
    "\n",
    "    if macros['debug'] >= 1:\n",
    "        print(\"Train Q: \", train_Q.shape)\n",
    "        print(\"Train P: \", train_P.shape)\n",
    "        print(\"Train Y: \", train_Y_start.shape)\n",
    "        print(\"Test Q: \", test_Q.shape)\n",
    "        print(\"Test P: \", test_P.shape)\n",
    "        print(\"Test Y: \", test_Y_start.shape)\n",
    "        print(\"Crop_train: \", len(crop_train))\n",
    "        print(\"Crop_test: \", len(crop_test))\n",
    "    # Let's free up some memory now\n",
    "    train_p, train_q, train_y, test_p, test_q, test_y = None, None, None, None, None, None\n",
    "    \n",
    "    # Load embedding matrics\n",
    "    vectors = np.load(os.path.join(data_loc, 'glove.new.trimmed.300.npy'))\n",
    "    \n",
    "    return train_P, train_Q, train_Y_start, train_Y_end, test_P, test_Q, test_Y_start, test_Y_end, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macros = {\n",
    "#     \"ques_len\": QUES_LEN,\n",
    "#     \"hidden_dim\": HIDDEN_DIM, \n",
    "#     \"vocab_size\": VOCAB_SIZE, \n",
    "#     \"batch_size\": BATCH_SIZE,\n",
    "#     \"para_len\": PARA_LEN,\n",
    "#     \"embedding_dim\": EMBEDDING_DIM,\n",
    "#     \"debug\": 5\n",
    "# } \n",
    "\n",
    "# a = prepare_data(DATA_LOC, macros=macros, crop=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, and running the model\n",
    "- Write a train fn\n",
    "- Write a training loop invoking it\n",
    "- Fill in real data\n",
    "\n",
    "----------\n",
    "\n",
    "Feats:\n",
    "- Function to test every n epochs.\n",
    "- Report train accuracy every epoch\n",
    "- Store the train, test accuracy for every instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def save_model(loc, models, epochs=0, optimizer=None):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            loc: str of the folder where the models are to be saved\n",
    "            models: dict of 'model_name': model_object\n",
    "            epochs, optimizers are int, torch.optims (discarded right now).\n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(models) is dict and len(models.keys()) == 4\n",
    "    \n",
    "    # Assumes four models. Doesn't save device/epochs/optimizer right now.\n",
    "    \n",
    "    for name in models:\n",
    "        torch.save(models[name], os.path.join(loc, name+'.torch'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(para_batch,\n",
    "          ques_batch,\n",
    "          answer_start_batch,\n",
    "          answer_end_batch,\n",
    "          ques_model,\n",
    "          para_model,\n",
    "          mlstm_model,\n",
    "          pointer_decoder_model,\n",
    "          optimizer, \n",
    "          loss_fn,\n",
    "          macros,\n",
    "          debug=2):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    :param para_batch: paragraphs (batch, max_seq_len_para) \n",
    "    :param ques_batch: questions corresponding to para (batch, max_seq_len_ques)\n",
    "    :param answer_start_batch: one-hot vector denoting pos of span start (batch, max_seq_len_para)\n",
    "    :param answer_end_batch: one-hot vector denoting pos of span end (batch, max_seq_len_para)\n",
    "    \n",
    "    # Models\n",
    "    :param ques_model: model to encode ques\n",
    "    :param para_model: model to encode para\n",
    "    :param mlstm_model: model to match para, ques to get para summary\n",
    "    :param pointer_decoder_model: model to get a pointer over start and end span pointer\n",
    "    \n",
    "    # Loss and Optimizer.\n",
    "    :param loss_fn: \n",
    "    :param optimizer: \n",
    "    \n",
    "    :return: \n",
    "    \n",
    "    \n",
    "    NOTE: When using MSE, \n",
    "        - target labels are one-hot\n",
    "        - target label is float tensor\n",
    "        - shape (batch, 1, len)\n",
    "        \n",
    "        When using CrossEntropy\n",
    "        - target is not onehot\n",
    "        - long\n",
    "        - shape (batch, )\n",
    "    \"\"\"\n",
    "    try:    \n",
    "    #     DEBUG = debug\n",
    "    #     BATCH_SIZE = macros['batch_size']\n",
    "    #     HIDDEN_DIM = macros['hidden_dim']\n",
    "\n",
    "        if debug >=2: \n",
    "            print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "            print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "            print(\"\\tanswer_start_batch:\\t\", answer_start_batch.shape)\n",
    "            print(\"\\tanswer_end_batch:\\t\\t\", answer_end_batch.shape)\n",
    "\n",
    "        # Wiping all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initializing all hidden states.\n",
    "        hidden_quesenc = ques_model.init_hidden(macros['batch_size'], device)\n",
    "        hidden_paraenc = para_model.init_hidden(macros['batch_size'], device)\n",
    "        hidden_mlstm = mlstm_model.init_hidden(macros['batch_size'], device)\n",
    "        hidden_ptrnet = pointer_decoder_model.init_hidden(macros['batch_size'], device)\n",
    "        h_ri = torch.zeros((1, macros['batch_size'], macros['hidden_dim']), dtype=torch.float, device=device)\n",
    "        h_ak = torch.zeros((1, macros['batch_size'], macros['hidden_dim']), dtype=torch.float, device=device)\n",
    "        if debug >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "\n",
    "        #passing the data through LSTM pre-processing layer\n",
    "        H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc, device=device)\n",
    "        H_p, para_model_hidden = para_model(para_batch, hidden_paraenc, device=device)\n",
    "        if debug >= 2: \n",
    "            print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "            print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "            print(\"\\tH_ri:\\t\\t\", h_ri.shape)\n",
    "    #         raw_input(\"Check memory and ye shall continue\")\n",
    "            print(\"------------Encoded hidden states------------\")\n",
    "\n",
    "        H_r = mlstm_model(H_p.view(-1, macros['batch_size'], 2*macros['hidden_dim']), h_ri, H_q, hidden_mlstm, device=device)\n",
    "        if debug >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "\n",
    "        #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "        h_ak, hidden_ptrnet, beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device=device)\n",
    "        h_ak, hidden_ptrnet, beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device=device)\n",
    "        if debug >= 2: print(\"------------Passed through pointernet------------\")\n",
    "\n",
    "\n",
    "        # For crossentropy\n",
    "        _, answer_start_batch = answer_start_batch.max(dim=2)\n",
    "        _, answer_end_batch = answer_end_batch.max(dim=2)\n",
    "        answer_start_batch = answer_start_batch.view(-1).long()\n",
    "        answer_end_batch = answer_end_batch.view(-1).long()\n",
    "#         print(beta_k_start.view(-1, macros['para_len']).shape, answer_start_batch.view(-1).shape)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = loss_fn(beta_k_start.view(-1, macros['para_len']), answer_start_batch)\n",
    "        loss += loss_fn(beta_k_end.view(-1, macros['para_len']), answer_end_batch)\n",
    "#         loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "#         loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "        if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "\n",
    "        loss.backward()\n",
    "        if debug >= 2: print(\"------------Calculated Gradients------------\")\n",
    "\n",
    "        #optimization step\n",
    "        optimizer.step()\n",
    "        if debug >= 2: print(\"------------Updated weights.------------\")\n",
    "            \n",
    "        return beta_k_start, beta_k_end, loss\n",
    "    \n",
    "    except: \n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function (no grad, no eval)\n",
    "def predict(para_batch,\n",
    "            ques_batch,\n",
    "            ques_model,\n",
    "            para_model,\n",
    "            mlstm_model,\n",
    "            pointer_decoder_model,\n",
    "            macros,\n",
    "            loss_fn=None,\n",
    "            debug=DEBUG):\n",
    "    \"\"\"\n",
    "        Function which returns the model's output based on a given set of P&Q's. \n",
    "        Does not convert to strings, gives the direct model output.\n",
    "        \n",
    "        Expects:\n",
    "            four models\n",
    "            data\n",
    "            misc macros\n",
    "    \"\"\"\n",
    "    \n",
    "#     BATCH_SIZE = macros['batch_size']\n",
    "    BATCH_SIZE = ques_batch.shape[0]\n",
    "    HIDDEN_DIM = macros['hidden_dim']\n",
    "    DEBUG = debug\n",
    "    \n",
    "    if debug >=2: \n",
    "        print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "        print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "        \n",
    "    with torch.no_grad():    \n",
    "\n",
    "        # Initializing all hidden states.\n",
    "        hidden_quesenc = ques_model.init_hidden(BATCH_SIZE, device)\n",
    "        hidden_paraenc = para_model.init_hidden(BATCH_SIZE, device)\n",
    "        hidden_mlstm = mlstm_model.init_hidden(BATCH_SIZE, device)\n",
    "        hidden_ptrnet = pointer_decoder_model.init_hidden(BATCH_SIZE, device)\n",
    "        h_ri = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        h_ak = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        if DEBUG >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "            \n",
    "        #passing the data through LSTM pre-processing layer\n",
    "        H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc, device)\n",
    "        H_p, para_model_hidden = para_model(para_batch, hidden_paraenc, device)\n",
    "        if DEBUG >= 2: \n",
    "            print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "            print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "            print(\"\\tH_ri:\\t\\t\", h_ri.shape)\n",
    "#             raw_input(\"Check memory and ye shall continue\")\n",
    "            print(\"------------Encoded hidden states------------\")\n",
    "\n",
    "        H_r = mlstm_model(H_p.view(-1, BATCH_SIZE, 2*HIDDEN_DIM), h_ri, H_q, hidden_mlstm, device)\n",
    "        if DEBUG >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "\n",
    "        #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "        h_ak, hidden_ptrnet, beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device)\n",
    "        _, _, beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device)\n",
    "        if DEBUG >= 2: print(\"------------Passed through pointernet------------\")\n",
    "                            \n",
    "        # For crossentropy\n",
    "#         _, answer_start_batch = answer_start_batch.max(dim=2)[1]\n",
    "#         _, answer_end_batch = answer_end_batch.max(dim=2)[1]\n",
    "#         print(\"labels: \", answer_start_batch.shape)[1]\n",
    "            \n",
    "#         #How will we manage batches for loss.\n",
    "#         loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "#         loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "#         if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "            \n",
    "        return (beta_k_start, beta_k_end, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval function (no grad no eval no nothing)\n",
    "def eval(y_cap, y, metrics={'em':None}):\n",
    "    \"\"\" \n",
    "        Returns the exact-match (em) metric by default.\n",
    "        Can specifiy more in a list (TODO)\n",
    "        \n",
    "        Inputs:\n",
    "        - y_cap: list of two tensors (start, end) of dim [BATCH_SIZE, PARA_LEN] each\n",
    "        - y: list of two tensors (start, end) of dim [BATCH_SIZE, 1] each\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(y[0].shape, y[1].shape, y_cap[0].shape, y_cap[1].shape)\n",
    "    \n",
    "    y_cap= torch.argmax(y_cap[0], dim=1).float(), torch.argmax(y_cap[1], dim=1).float()\n",
    "    y = torch.argmax(y[0], dim=1).float(), torch.argmax(y[1], dim=1).float()\n",
    "    \n",
    "    if \"em\" in metrics.keys():\n",
    "        metrics['em'] = (y[0].eq(y_cap[0]) & y[1].eq(y_cap[1])).sum().item()/ float(y[0].shape[0])\n",
    "        \n",
    "    if DEBUG >= 3: \n",
    "        print(\"Test performance: \", metrics)\n",
    "        print(\"------------Evaluated------------\")\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "if DEBUG >=5:\n",
    "    # Testing this function\n",
    "    metrics = {'em':None}\n",
    "#     y = torch.tensor([[3]]).float(), torch.tensor([[4]]).float()\n",
    "    y = torch.tensor([[0,0,3,0], [0,2,0,0]]), torch.tensor([[0,0,0,3], [0,0,0,3]])\n",
    "    y_cap = torch.tensor([[0,0,3,0],[0,0,3,0]]), torch.tensor([[0,0,0,3],[0,0,0,3]])\n",
    "#     y = torch.randint(0, PARA_LEN, (BATCH_SIZE,)).float(), torch.randint(0, PARA_LEN, (BATCH_SIZE,)).float()\n",
    "#     y_cap = torch.rand((BATCH_SIZE, PARA_LEN)), torch.rand((BATCH_SIZE, PARA_LEN))\n",
    "    print(eval(y_cap, y))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(_models, _data, _macros, _epochs, _save=0, _test_eval=0, _train_eval=0, _debug=2):\n",
    "    \"\"\"\n",
    "        > Instantiate models\n",
    "        > Instantiate loss, optimizer\n",
    "        > Instantiate ways to store loss\n",
    "\n",
    "        > Per epoch\n",
    "            > sample batch and give to train fn\n",
    "            > get loss\n",
    "            > if epoch %k ==0: get test accuracy\n",
    "\n",
    "        > have fn to calculate test accuracy\n",
    "        \n",
    "        > _save: int\n",
    "            > 0: dont\n",
    "            > 1+: save every _save epoch (overwrite)\n",
    "            > -1 -> save best (turned to 1 if test evals dont happen.)\n",
    "        \n",
    "        > Save the model at every epoch if we don't test on test. \n",
    "            > else save on the best performning mode\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack data\n",
    "    DEBUG = _debug\n",
    "    train_P = _data['train']['P']\n",
    "    train_Q = _data['train']['Q']\n",
    "    train_Y_start = _data['train']['Ys']\n",
    "    train_Y_end = _data['train']['Ye']\n",
    "    test_P = _data['test']['P']\n",
    "    test_Q = _data['test']['Q']\n",
    "    test_Y_start = _data['test']['Ys']\n",
    "    test_Y_end = _data['test']['Ye']\n",
    "\n",
    "    ques_model, para_model, mlstm_model, pointer_decoder_model = _models\n",
    "    _data = None\n",
    "\n",
    "    # Instantiate Loss\n",
    "#         loss_fn = nn.MSELoss()\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, ques_model.parameters())) + \n",
    "                             list(filter(lambda p: p.requires_grad, para_model.parameters())) + \n",
    "                             list(mlstm_model.parameters()) + \n",
    "                             list(pointer_decoder_model.parameters()), lr=macros['lr'])\n",
    "#         optimizer = optim.Adam(list(ques_model.parameters()) + \\\n",
    "#                                list(para_model.parameters()) + \\\n",
    "#                                list(mlstm_model.parameters()) + \\\n",
    "#                               list(pointer_decoder_model.parameters()), lr=macros['lr'])\n",
    "\n",
    "    # Losses\n",
    "    train_losses = []\n",
    "    train_em = []\n",
    "    test_losses = []\n",
    "    test_em = []\n",
    "    best_test_em = 0.0\n",
    "    found_best_test_em = False\n",
    "    \n",
    "    try: \n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(_epochs):\n",
    "            print(\"Epoch: \", epoch, \"/\", _epochs)\n",
    "\n",
    "            epoch_loss = []\n",
    "            epoch_train_em = []\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            for iter in range(int(len(train_P)/BATCH_SIZE)):\n",
    "    #         for iter in range(2):\n",
    "\n",
    "                batch_time = time.time()\n",
    "\n",
    "                # Sample batch and train on it\n",
    "                sample_index = np.random.randint(0, len(train_P), _macros['batch_size'])\n",
    "            \n",
    "#                 grad_old = sum([x.grad.sum().item() for x in params])\n",
    "\n",
    "                y_cap_start, y_cap_end, loss = train(\n",
    "                    para_batch = torch.tensor(train_P[sample_index], dtype=torch.long, device=device),\n",
    "                    ques_batch = torch.tensor(train_Q[sample_index], dtype=torch.long, device=device),\n",
    "                    answer_start_batch = torch.tensor(train_Y_start[sample_index], dtype=torch.float, device=device).view( _macros['batch_size'], 1, _macros['para_len']),\n",
    "                    answer_end_batch = torch.tensor(train_Y_end[sample_index], dtype=torch.float, device=device).view(_macros['batch_size'], 1, _macros['para_len']),\n",
    "                    ques_model = ques_model,\n",
    "                    para_model = para_model,\n",
    "                    mlstm_model = mlstm_model,\n",
    "                    pointer_decoder_model = pointer_decoder_model,\n",
    "                    optimizer = optimizer, \n",
    "                    loss_fn= loss_fn,\n",
    "                    macros=_macros,\n",
    "                    debug=_macros['debug']\n",
    "                )\n",
    "\n",
    "                if _train_eval: \n",
    "\n",
    "                    # Calculate train accuracy for this minibatch\n",
    "                    metrics = eval(\n",
    "                        y=(torch.tensor(train_Y_start[sample_index], dtype=torch.long, device=device).view( -1, _macros['para_len']),\n",
    "                            torch.tensor(train_Y_end[sample_index], dtype=torch.long, device=device).view(-1, _macros['para_len'])),\n",
    "                        y_cap=[y_cap_start.squeeze(), y_cap_end.squeeze()])\n",
    "\n",
    "                    epoch_train_em.append(metrics['em'])\n",
    "    \n",
    "                epoch_loss.append(loss.item())\n",
    "    \n",
    "#                 grad_new = sum([x.grad.sum().item() for x in params])\n",
    "\n",
    "                print(\"Batch:\\t%d\" % iter,\"/%d\\t: \" % (len(train_P)/_macros['batch_size']),\n",
    "                      str(\"%s\" % (time.time() - batch_time))[:8], \n",
    "                      str(\"\\t\\b%s\" % (time.time() - epoch_time))[:10], \n",
    "                      \"\\tl:%f\" % loss.item(),\n",
    "                      \"\\tem:%f\" % epoch_train_em[-1] if _train_eval else \"\")\n",
    "#                      \"\\t\\b\\b%s\" % grad_new - grad_old)\n",
    "#                      end=None if iter+1 == int(len(train_P)/BATCH_SIZE) else \"\\r\")\n",
    "\n",
    "            train_losses.append(epoch_loss)\n",
    "        \n",
    "            if _train_eval: train_em.append(epoch_train_em)\n",
    "            if _test_eval and epoch % _test_eval == 0:\n",
    "\n",
    "                y_cap_start, y_cap_end, test_loss = predict(\n",
    "                    para_batch = torch.tensor(test_P, dtype=torch.long, device=device),\n",
    "                    ques_batch = torch.tensor(test_Q, dtype=torch.long, device=device),\n",
    "                    ques_model = ques_model,\n",
    "                    para_model = para_model,\n",
    "                    mlstm_model = mlstm_model,\n",
    "                    pointer_decoder_model = pointer_decoder_model,\n",
    "                    macros = _macros,\n",
    "                    loss_fn= loss_fn,\n",
    "                    debug = _macros['debug']\n",
    "                )\n",
    "                metrics = eval(\n",
    "                    y=(torch.tensor(test_Y_start, dtype=torch.long, device=device).view( -1, _macros['para_len']),\n",
    "                        torch.tensor(test_Y_end, dtype=torch.long, device=device).view(-1, _macros['para_len'])),\n",
    "                    y_cap=[y_cap_start.squeeze(), y_cap_end.squeeze()])\n",
    "\n",
    "                test_losses.append(test_loss)\n",
    "                test_em.append(metrics['em'])\n",
    "                \n",
    "                # Check if we outperformed the best one.\n",
    "                if metrics['em'] >= best_test_em:\n",
    "                    \n",
    "                    # Set flag\n",
    "                    found_best_test_em = True\n",
    "                    \n",
    "                    # Update value\n",
    "                    best_test_em = metrics['em']   \n",
    "                \n",
    "            # Saving logic\n",
    "            if _save == 0:\n",
    "                pass\n",
    "            elif ( _save>0 and epoch % _save == 0) or \\\n",
    "            ( _save == -1 and found_best_test_em ):\n",
    "                models = { 'ques_model': ques_model,\n",
    "                           'para_model': para_model,\n",
    "                           'mlstm_model':  mlstm_model,\n",
    "                           'pointer_decoder_model': pointer_decoder_model\n",
    "                         }\n",
    "                \n",
    "                save_model(macros['save_model_loc'], models,\n",
    "                          epochs=epoch,\n",
    "                           optimizer=optimizer)\n",
    "                \n",
    "                print(\"Saving new model on epoch %d\" % epoch)\n",
    "            \n",
    "            # Reset flags\n",
    "            found_best_test_em = False\n",
    "            \n",
    "            # At the end of every epoch, do print the average epoch loss, and other stat\n",
    "            print(\"\\nEpoch performance: \",\n",
    "                  \"%ssec\" % str(time.time() - epoch_time)[:6],\n",
    "                  \"Trl:%f\" % np.mean(epoch_loss, axis=0),\n",
    "                  \"\\tTrem:%f\" % np.mean(epoch_train_em) if _train_eval and epoch % _train_eval == 0 else \"\",\n",
    "                  \"\\tTeem:%f\\n\" % test_em[-1] if _test_eval and epoch % _test_eval == 0 else \"\\n\")\n",
    "\n",
    "#         return train_losses, train_em, test_losses, test_em\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        # someone called a ctrl+c on it. Let' return the things computed so far atlest.\n",
    "        print(\"Found keyboard interrupt. Stopping training loop\")\n",
    "        \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:       \n",
    "        return train_losses, train_em, test_losses, test_em\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "    Saving said models.\n",
    "    TODO\n",
    "\"\"\"\n",
    "# ques_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(loss, _label=\"Some label\", _only_epoch=True):\n",
    "    \"\"\"\n",
    "        Fn to visualize loss.\n",
    "        Expects either\n",
    "            - [int, int] for epoch level stuff\n",
    "            - [ [int, int], [int, int] ] for batch level data. \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = [15, 8] \n",
    "    \n",
    "    # Detect input format\n",
    "    if type(loss[0]) in [int, float, long]:\n",
    "        \n",
    "#         print(\"here\")\n",
    "        \n",
    "        plt.plot(loss)\n",
    "        plt.ylabel(_label)\n",
    "        plt.show()\n",
    "        \n",
    "    elif type(loss[0]) == list:\n",
    "        \n",
    "        if _only_epoch:\n",
    "            loss = [ sum(x) for x in loss ]\n",
    "            \n",
    "        else:\n",
    "            loss = [ y for x in loss for y in x ]\n",
    "            \n",
    "        plt.plot(loss)\n",
    "        plt.ylabel(_label)\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator\n",
    "\n",
    "One cell which instantiates and runs everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Q:  (585, 30)\n",
      "Train P:  (585, 200)\n",
      "Train Y:  (585, 200)\n",
      "Test Q:  (585, 30)\n",
      "Test P:  (585, 200)\n",
      "Test Y:  (585, 200)\n",
      "Crop_train:  7\n",
      "Crop_test:  7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Cell which pulls everything together.\n",
    "\n",
    "    > init models\n",
    "    > get data prepared\n",
    "    > pass models and data to training loop\n",
    "    > gets trained models and loss\n",
    "    > saves models\n",
    "    > visualizes loss?\n",
    "\n",
    "No other function but this one ever sees global macros!\n",
    "\"\"\"\n",
    "macros = {\n",
    "    \"ques_len\": QUES_LEN,\n",
    "    \"hidden_dim\": HIDDEN_DIM, \n",
    "    \"vocab_size\": VOCAB_SIZE, \n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"para_len\": PARA_LEN,\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"lr\": LR,\n",
    "    \"debug\":DEBUG,\n",
    "    \"save_model_loc\": MODEL_LOC\n",
    "#     \"device\": device\n",
    "} \n",
    "\n",
    "data = {'train':{}, 'test':{}}\n",
    "data['train']['P'], data['train']['Q'], data['train']['Ys'], data['train']['Ye'], \\\n",
    "data['test']['P'], data['test']['Q'], data['test']['Ys'], data['test']['Ye'], vectors = \\\n",
    "    prepare_data(DATA_LOC, macros, crop=CROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate modelshttp://localhost:8888/notebooks/model.ipynb#\n",
    "ques_model = Encoder(QUES_LEN, macros, vectors, device).cuda(device)\n",
    "para_model = Encoder(PARA_LEN, macros, vectors, device).cuda(device)\n",
    "mlstm_model = MatchLSTMEncoder(macros, device).cuda(device)\n",
    "pointer_decoder_model = PointerDecoder(macros, device).cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 / 300\n",
      "Batch:\t0 /5\t:  1.101634 1.101654 \tl:11.143613 \tem:0.000000\n",
      "Batch:\t1 /5\t:  1.100969 2.203655 \tl:30.767282 \tem:0.000000\n",
      "Batch:\t2 /5\t:  1.115385 3.320277 \tl:13.192310 \tem:0.000000\n",
      "Batch:\t3 /5\t:  1.125669 4.446677 \tl:10.815127 \tem:0.000000\n",
      "Batch:\t4 /5\t:  1.033458 5.481040 \tl:11.822186 \tem:0.000000\n",
      "Saving new model on epoch 0\n",
      "\n",
      "Epoch performance:  8.2896sec Trl:15.548104 \tTrem:0.000000 \tTeem:0.000000\n",
      "\n",
      "Epoch:  1 / 300\n",
      "Batch:\t0 /5\t:  1.298332 1.298367 \tl:11.676325 \tem:0.000000\n",
      "Batch:\t1 /5\t:  1.254957 2.554641 \tl:11.274551 \tem:0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-f205e19f0955>\", line 100, in train\n",
      "    loss.backward()\n",
      "  File \"/data/priyansh/virtualenvironment/pwc-reg-m/local/lib/python2.7/site-packages/torch/tensor.py\", line 93, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/data/priyansh/virtualenvironment/pwc-reg-m/local/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 89, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-13-4c0bcefbdfa4>\", line 89, in training_loop\n",
      "    debug=_macros['debug']\n",
      "TypeError: 'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "op = training_loop(_models=[ques_model, para_model, mlstm_model, pointer_decoder_model],\n",
    "                   _data=data,\n",
    "                   _debug=macros['debug'],\n",
    "                   _save=1,\n",
    "                   _test_eval=1,\n",
    "                   _train_eval=1,\n",
    "                   _epochs=EPOCHS,\n",
    "                   _macros=macros)    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# See if gradients are being passed\n",
    "p = list(filter(lambda p: p.requires_grad, ques_model.parameters())) + \\\n",
    "    list(filter(lambda p: p.requires_grad, para_model.parameters())) + \\\n",
    "    list(para_model.parameters()) + \\\n",
    "    list(pointer_decoder_model.parameters())\n",
    "                       \n",
    "print([ x.grad.sum().item() for x in p])                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "\n",
    "So far, we plot the training losss. \n",
    "Shall we superimpose test loss on it too? We don't calculate test loss per batch though (fortunately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EM\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4oAAAHVCAYAAAC3ygXzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGKhJREFUeJzt3X+MZXd53/HPE7u4QTRggzGOF2ed2CoyIgVpapeGVJQfxk4DpsF/mFbKVgK5P0AiQZFwhBrAoBaiBFAUiGRBJAspMQiJshUiljG4lWgCHju0YMDxxg6yHQMLdt04CFPD0z/muppnNctOvL5zd2deL2k095zznbnPSEf2vufce6a6OwAAAPCYn1j1AAAAAJxYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYTl31ADvpGc94Ru/fv3/VYwAAAKzErbfe+p3uPvNY6/ZUKO7fvz/r6+urHgMAAGAlquob21nnpacAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwCAUAQAAGIQiAAAAg1AEAABgEIoAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwCAUAQAAGIQiAAAAg1AEAABgEIoAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwCAUAQAAGIQiAAAAg1AEAABgEIoAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwCAUAQAAGIQiAAAAg1AEAABgEIoAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwCAUAQAAGIQiAAAAg1AEAABgEIoAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwCAUAQAAGIQiAAAAg1AEAABgEIoAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwCAUAQAAGFYailV1aVXdUVWHqurqLY6fVlUfXRz/QlXtP+L4uVX1cFX9xk7NDAAAsNutLBSr6pQkH0hyWZILk7y2qi48YtnrkjzY3ecneV+S9xxx/L1JPr3sWQEAAPaSVV5RvCjJoe6+q7t/kOT6JJcfsebyJNctHn88yUurqpKkql6d5O4kt+/QvAAAAHvCKkPxnCT3bNq+d7FvyzXd/WiSh5I8vaqekuQtSd5xrCepqquqar2q1g8fPvyEDA4AALCbnaw3s3l7kvd198PHWtjd13b3WnevnXnmmcufDAAA4CR36gqf+74kz960vW+xb6s191bVqUmemuS7SS5OckVV/XaSpyX5UVV9v7t/f/ljAwAA7G6rDMVbklxQVedlIwivTPKvjlhzMMmBJH+a5Iokn+3uTvKLjy2oqrcneVgkAgAAPDFWFord/WhVvTHJDUlOSfKH3X17VV2TZL27Dyb5cJKPVNWhJA9kIyYBAABYotq4QLc3rK2t9fr6+qrHAAAAWImqurW714617mS9mQ0AAABLIhQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgGGloVhVl1bVHVV1qKqu3uL4aVX10cXxL1TV/sX+l1fVrVX15cXnl+z07AAAALvVykKxqk5J8oEklyW5MMlrq+rCI5a9LsmD3X1+kvclec9i/3eSvLK7n5fkQJKP7MzUAAAAu98qryhelORQd9/V3T9Icn2Sy49Yc3mS6xaPP57kpVVV3f3n3f3Xi/23J/nJqjptR6YGAADY5VYZiuckuWfT9r2LfVuu6e5HkzyU5OlHrHlNktu6+5GtnqSqrqqq9apaP3z48BMyOAAAwG52Ut/Mpqqem42Xo/7bo63p7mu7e627184888ydGw4AAOAktcpQvC/Jszdt71vs23JNVZ2a5KlJvrvY3pfkE0l+tbv/cunTAgAA7BGrDMVbklxQVedV1ZOSXJnk4BFrDmbjZjVJckWSz3Z3V9XTknwqydXd/fkdmxgAAGAPWFkoLt5z+MYkNyT5WpKPdfftVXVNVb1qsezDSZ5eVYeSvDnJY39C441Jzk/yW1X1pcXHM3f4RwAAANiVqrtXPcOOWVtb6/X19VWPAQAAsBJVdWt3rx1r3Ul9MxsAAACeeEIRAACAQSgCAAAwCEUAAAAGoQgAAMAgFAEAABiEIgAAAINQBAAAYBCKAAAADEIRAACAQSgCAAAwCEUAAAAGoQgAAMAgFAEAABiEIgAAAINQBAAAYBCKAAAADEIRAACAQSgCAAAwCEUAAAAGoQgAAMAgFAEAABiEIgAAAINQBAAAYBCKAAAADEIRAACAQSgCAAAwCEUAAAAGoQgAAMAgFAEAABiEIgAAAINQBAAAYBCKAAAADEIRAACAQSgCAAAwCEUAAAAGoQgAAMAgFAEAABiEIgAAAINQBAAAYBCKAAAADEIRAACAQSgCAAAwCEUAAAAGoQgAAMAgFAEAABiEIgAAAINQBAAAYBCKAAAADEIRAACAQSgCAAAwCEUAAAAGoQgAAMAgFAEAABhOPdaCqjolyb9Isn/z+u5+7/LGAgAAYFWOGYpJ/muS7yf5cpIfLXccAAAAVm07obivu39+6ZMAAABwQtjOexQ/XVWXLH0SAAAATgjbuaL4Z0k+UVU/keT/Jqkk3d0/tdTJAAAAWInthOJ7k7wwyZe7u5c8DwAAACu2nZee3pPkKyIRAABgb9jOFcW7ktxcVZ9O8shjO/15DAAAgN1pO6F49+LjSYsPAAAAdrFjhmJ3vyNJqurJ3f295Y8EAADAKh3zPYpV9cKq+mqSry+2/1FVfXDpkwEAALAS27mZzfuTvCLJd5Oku/9nkn+2zKEAAABYne2EYrr7niN2/XAJswAAAHAC2M7NbO6pqn+apKvq7yV5U5KvLXcsAAAAVmU7VxT/XZI3JDknyX1Jnr/YBgAAYBfazl1Pv5PkX+/ALAAAAJwAtvUexWWpqkur6o6qOlRVV29x/LSq+uji+Beqav+mY7+52H9HVb1iJ+cGAADYzVYWilV1SpIPJLksyYVJXltVFx6x7HVJHuzu85O8L8l7Fl97YZIrkzw3yaVJPrj4fgAAABynVV5RvCjJoe6+q7t/kOT6JJcfsebyJNctHn88yUurqhb7r+/uR7r77iSHFt8PAACA43TM9yhW1WlJXpNk/+b13X3NcT73OUk2/9mNe5NcfLQ13f1oVT2U5OmL/X92xNees9WTVNVVSa5KknPPPfc4RwYAANj9tnNF8ZPZuIL3aJK/3fRxUujua7t7rbvXzjzzzFWPAwAAcMLbzt9R3Nfdly7hue9L8uzNz7PYt9Wae6vq1CRPTfLdbX4tAAAAj8N2rij+j6p63hKe+5YkF1TVeVX1pGzcnObgEWsOJjmweHxFks92dy/2X7m4K+p5SS5I8sUlzAgAALDnbOeK4ouS/JuqujvJI0kqSXf3zx/PEy/ec/jGJDckOSXJH3b37VV1TZL17j6Y5MNJPlJVh5I8kI2YzGLdx5J8NRsviX1Dd//weOYBAABgQ21coPsxC6p+Zqv93f2NpUy0RGtra72+vr7qMQAAAFaiqm7t7rVjrTvqFcWq+qnu/j9J/uYJnQwAAIAT2o976ekfJfnlJLcm6Wy85PQxneRnlzgXAAAAK3LUUOzuX158Pm/nxgEAAGDVtnMzm1TV6dm4s+jff2xfd//3ZQ0FAADA6hwzFKvq9UnelI2/VfilJP8kyZ8meclyRwMAAGAVtvN3FN+U5B8n+UZ3//MkL0jyv5c6FQAAACuznVD8fnd/P0mq6rTu/nqSf7jcsQAAAFiV7bxH8d6qelqS/5Lkxqp6MMlJ9zcUAQAA2J5jhmJ3/8vFw7dX1eeSPDXJnyx1KgAAAFbmx4ZiVZ2S5Pbufk6SdPd/25GpAAAAWJkf+x7F7v5hkjuq6twdmgcAAIAV2857FE9PcntVfTHJ3z62s7tftbSpAAAAWJnthOJ/XPoUAAAAnDC2E4q/1N1v2byjqt6TxPsVAQAAdqHt/B3Fl2+x77InehAAAABODEe9olhV/z7Jf0jys1X1vzYd+gdJPr/swQAAAFiNH/fS0z9K8ukk/znJ1Zv2/013P7DUqQAAAFiZo4Zidz+U5KEkr925cQAAAFi17bxHEQAAgD1EKAIAADAIRQAAAAahCAAAwCAUAQAAGIQiAAAAg1AEAABgEIoAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwCAUAQAAGIQiAAAAg1AEAABgEIoAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwCAUAQAAGIQiAAAAg1AEAABgEIoAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwCAUAQAAGIQiAAAAg1AEAABgEIoAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwCAUAQAAGIQiAAAAg1AEAABgEIoAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwCAUAQAAGIQiAAAAg1AEAABgEIoAAAAMQhEAAIBBKAIAADAIRQAAAAahCAAAwLCSUKyqM6rqxqq6c/H59KOsO7BYc2dVHVjse3JVfaqqvl5Vt1fVu3d2egAAgN1tVVcUr05yU3dfkOSmxfZQVWckeVuSi5NclORtm4Lyd7r7OUlekOQXquqynRkbAABg91tVKF6e5LrF4+uSvHqLNa9IcmN3P9DdDya5Mcml3f297v5cknT3D5LclmTfDswMAACwJ6wqFM/q7vsXj7+Z5Kwt1pyT5J5N2/cu9v1/VfW0JK/MxlXJLVXVVVW1XlXrhw8fPr6pAQAA9oBTl/WNq+ozSZ61xaG3bt7o7q6qfhzf/9Qkf5zk97r7rqOt6+5rk1ybJGtra3/n5wEAANhrlhaK3f2yox2rqm9V1dndfX9VnZ3k21ssuy/Jizdt70ty86bta5Pc2d3vfwLGBQAAYGFVLz09mOTA4vGBJJ/cYs0NSS6pqtMXN7G5ZLEvVfWuJE9N8ms7MCsAAMCesqpQfHeSl1fVnUletthOVa1V1YeSpLsfSPLOJLcsPq7p7geqal82Xr56YZLbqupLVfX6VfwQAAAAu1F175237a2trfX6+vqqxwAAAFiJqrq1u9eOtW5VVxQBAAA4QQlFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGBYSShW1RlVdWNV3bn4fPpR1h1YrLmzqg5scfxgVX1l+RMDAADsHau6onh1kpu6+4IkNy22h6o6I8nbklyc5KIkb9sclFX1K0ke3plxAQAA9o5VheLlSa5bPL4uyau3WPOKJDd29wPd/WCSG5NcmiRV9ZQkb07yrh2YFQAAYE9ZVSie1d33Lx5/M8lZW6w5J8k9m7bvXexLkncm+d0k3zvWE1XVVVW1XlXrhw8fPo6RAQAA9oZTl/WNq+ozSZ61xaG3bt7o7q6q/jt83+cn+bnu/vWq2n+s9d19bZJrk2RtbW3bzwMAALBXLS0Uu/tlRztWVd+qqrO7+/6qOjvJt7dYdl+SF2/a3pfk5iQvTLJWVX+VjfmfWVU3d/eLAwAAwHFb1UtPDyZ57C6mB5J8cos1NyS5pKpOX9zE5pIkN3T3H3T3T3f3/iQvSvIXIhEAAOCJs6pQfHeSl1fVnUletthOVa1V1YeSpLsfyMZ7EW9ZfFyz2AcAAMASVffeedve2tpar6+vr3oMAACAlaiqW7t77VjrVnVFEQAAgBOUUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAYhCIAAACDUAQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAAhuruVc+wY6rqcJJvrHoOjtszknxn1UOwazm/WCbnF8vk/GLZnGO7w89095nHWrSnQpHdoarWu3tt1XOwOzm/WCbnF8vk/GLZnGN7i5eeAgAAMAhFAAAABqHIyejaVQ/Arub8YpmcXyyT84tlc47tId6jCAAAwOCKIgAAAINQBAAAYBCKnJCq6oyqurGq7lx8Pv0o6w4s1txZVQe2OH6wqr6y/Ik5mRzP+VVVT66qT1XV16vq9qp6985Oz4mqqi6tqjuq6lBVXb3F8dOq6qOL41+oqv2bjv3mYv8dVfWKnZybk8PjPb+q6uVVdWtVfXnx+SU7PTsnvuP579fi+LlV9XBV/cZOzczyCUVOVFcnuam7L0hy02J7qKozkrwtycVJLkryts3/4K+qX0ny8M6My0nmeM+v3+nu5yR5QZJfqKrLdmZsTlRVdUqSDyS5LMmFSV5bVRcesex1SR7s7vOTvC/JexZfe2GSK5M8N8mlST64+H6Q5PjOr2z8cfRXdvfzkhxI8pGdmZqTxXGeX495b5JPL3tWdpZQ5ER1eZLrFo+vS/LqLda8IsmN3f1Adz+Y5MZs/CMrVfWUJG9O8q4dmJWTz+M+v7r7e939uSTp7h8kuS3Jvh2YmRPbRUkOdfddi/Pi+mycZ5ttPu8+nuSlVVWL/dd39yPdfXeSQ4vvB4953OdXd/95d//1Yv/tSX6yqk7bkak5WRzPf79SVa9Ocnc2zi92EaHIieqs7r5/8fibSc7aYs05Se7ZtH3vYl+SvDPJ7yb53tIm5GR2vOdXkqSqnpbkldm4KsnedszzZfOa7n40yUNJnr7Nr2VvO57za7PXJLmtux9Z0pycnB73+bX4xfxbkrxjB+Zkh5266gHYu6rqM0metcWht27e6O6uqm3/HZeqen6Sn+vuXz/yNfTsHcs6vzZ9/1OT/HGS3+vuux7flAA7o6qem42XC16y6lnYVd6e5H3d/fDiAiO7iFBkZbr7ZUc7VlXfqqqzu/v+qjo7ybe3WHZfkhdv2t6X5OYkL0yyVlV/lY1z/JlVdXN3vzjsGUs8vx5zbZI7u/v9T8C4nPzuS/LsTdv7Fvu2WnPv4hcNT03y3W1+LXvb8Zxfqap9ST6R5Fe7+y+XPy4nmeM5vy5OckVV/XaSpyX5UVV9v7t/f/ljs2xeesqJ6mA23nSfxedPbrHmhiSXVNXpi5uMXJLkhu7+g+7+6e7en+RFSf5CJHKEx31+JUlVvSsb/5P8tR2YlZPDLUkuqKrzqupJ2bg5zcEj1mw+765I8tnu7sX+Kxd3FTwvyQVJvrhDc3NyeNzn1+Il8p9KcnV3f37HJuZk8rjPr+7+xe7ev/g31/uT/CeRuHsIRU5U707y8qq6M8nLFtupqrWq+lCSdPcD2Xgv4i2Lj2sW++BYHvf5tfjN/FuzcWe426rqS1X1+lX8EJw4Fu/ZeWM2fpnwtSQf6+7bq+qaqnrVYtmHs/GenkPZuNnW1YuvvT3Jx5J8NcmfJHlDd/9wp38GTlzHc34tvu78JL+1+O/Vl6rqmTv8I3ACO87zi12sNn6ZCQAAABtcUQQAAGAQigAAAAxCEQAAgEEoAgAAMAhFAAAABqEIAADAIBQBAAAY/h+LvRhfK7HBuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizations\n",
    "print(\"Training Loss\")\n",
    "visualize_loss(op[0], \"train loss\", _only_epoch=True)\n",
    "\n",
    "# if len(op[1]) > 0:\n",
    "\n",
    "print(\"Training EM\")\n",
    "visualize_loss(op[1], \"train em\", _only_epoch=True)\n",
    "\n",
    "print(\"Testing EM\")\n",
    "visualize_loss(op[3], \"test em\")\n",
    "\n",
    "\n",
    "# print(op[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing (temp)\n",
    "# models = { 'ques_model': ques_model,\n",
    "#            'para_model': para_model,\n",
    "#            'mlstm_model':  mlstm_model,\n",
    "#            'pointer_decoder_model': pointer_decoder_model\n",
    "#          }\n",
    "# save_model(loc=macros['save_model_loc'], models=models, epochs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try loading the model\n",
    "# ques_model = torch.load(os.path.join(macros['save_model_loc'], 'ques_model.torch'))\n",
    "# print(\"Ques Model\\n\", ques_model)\n",
    "\n",
    "# para_model = torch.load(os.path.join(macros['save_model_loc'], 'para_model.torch'))\n",
    "# print(\"Para Model\\n\", para_model)\n",
    "\n",
    "# mlstm_model = torch.load(os.path.join(macros['save_model_loc'], 'mlstm_model.torch'))\n",
    "# print(\"MLSTM Model\\n\", mlstm_model)\n",
    "\n",
    "# pointer_decoder_model = torch.load(os.path.join(macros['save_model_loc'], 'pointer_decoder_model.torch'))\n",
    "# print(\"Pointer Decoder model\\n\", pointer_decoder_model)\n",
    "\n",
    "# # Create dummy data for testing the predict fn\n",
    "# q = np.random.randint(0, len(vectors), (3, 30))\n",
    "# p = np.random.randint(0, len(vectors), (3, 200))\n",
    "\n",
    "# y_cap_start, y_cap_end, _ = predict(torch.tensor(p, dtype=torch.long, device=device), \n",
    "#                                    torch.tensor(q, dtype=torch.long, device=device),\n",
    "#                                    ques_model=ques_model,\n",
    "#                                    para_model=para_model,\n",
    "#                                    mlstm_model=mlstm_model,\n",
    "#                                    pointer_decoder_model=pointer_decoder_model,\n",
    "#                                     macros=macros,\n",
    "#                                     debug=macros['debug'])\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
