{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA over unstructured data\n",
    "\n",
    "Using Match LSTM, Pointer Networks, as mentioned in paper https://arxiv.org/pdf/1608.07905.pdf\n",
    "\n",
    "We start with the pre-processing provided by https://github.com/MurtyShikhar/Question-Answering to clean up the data and make neat para, ques files.\n",
    "\n",
    "\n",
    "### @TODOs:\n",
    "\n",
    "1. [done] _Figure out how to put in real, pre-trained embeddings in embeddings layer._\n",
    "2. [done] _Explicitly provide batch size when instantiating model_\n",
    "3. [done] is ./val.ids.* validation set or test set?: **validation**\n",
    "4. [done:em] emInstead of test loss, calculate test acc metrics\n",
    "    1. todo: new metrics like P, R, F1\n",
    "5. [done] Update unit test codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "from io import open\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import traceback\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "import pylab\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from networks import Encoder, MatchLSTMEncoder, PointerDecoder\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug Legend\n",
    "\n",
    "- 5: Print everything that goes in every tensor.\n",
    "- 4: ??\n",
    "- 3: Check every model individually\n",
    "- 2: Print things in training loops\n",
    "- 1: ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macros \n",
    "DATA_LOC = './data/domain/'\n",
    "MODEL_LOC = './models/mlstms/domain/'\n",
    "DEBUG = 1\n",
    "\n",
    "# nn Macros\n",
    "QUES_LEN, PARA_LEN =  30, 200\n",
    "VOCAB_SIZE = 120000\n",
    "# VOCAB_SIZE = glove_file.shape[1]               # @TODO: get actual size\n",
    "HIDDEN_DIM = 150\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 50                  # Might have total 100 batches.\n",
    "EPOCHS = 300\n",
    "TEST_EVERY_ = 1\n",
    "LR = 0.001\n",
    "CROP = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder \n",
    "Use a simple lstm class to have encoder for question and paragraph. \n",
    "The output of these will be used in the match lstm\n",
    "\n",
    "$H^p = LSTM(P)$ \n",
    "\n",
    "\n",
    "$H^q = LSTM(Q)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputlen, macros, glove_file, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Catch dim\n",
    "        self.inputlen = inputlen\n",
    "        self.hiddendim = macros['hidden_dim']\n",
    "        self.embeddingdim =  macros['embedding_dim']\n",
    "        self.vocablen = macros['vocab_size']\n",
    "#         self.device = macros['device']\n",
    "        self.batch_size = macros['batch_size']\n",
    "        self.debug = macros['debug']\n",
    "        \n",
    "        # Embedding Layer\n",
    "#         self.embedding = nn.Embedding(self.vocablen, self.embeddingdim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(glove_file))\n",
    "        self.embedding.weight.requires_grad = True\n",
    "       \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(self.embeddingdim, self.hiddendim, bidirectional=True, batch_first=False)\n",
    "        \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \n",
    "        # Returns a new hidden layer var for LSTM\n",
    "        return (torch.zeros((2, batch_size, self.hiddendim), device=device), \n",
    "                torch.zeros((2, batch_size, self.hiddendim), device=device))\n",
    "    \n",
    "    def forward(self, x, h, device):\n",
    "        \n",
    "        # Input: x (batch, len ) (current input)\n",
    "        # Hidden: h (1, batch, hiddendim) (last hidden state)\n",
    "        \n",
    "        # Batchsize: b int (inferred)\n",
    "        b = x.shape[0]\n",
    "        \n",
    "        if self.debug > 4: print(\"x:\\t\", x.shape)\n",
    "        if self.debug > 4: print(\"h:\\t\", h[0].shape, h[1].shape)\n",
    "        \n",
    "        x_emb = self.embedding(x)\n",
    "        if self.debug > 4: \n",
    "            print(\"x_emb:\\t\", x_emb.shape)\n",
    "#             print(\"x_emb_wrong:\\t\", x_emb.transpose(1,0).shape)           \n",
    "            \n",
    "        ycap, h = self.lstm(x_emb.transpose(1,0), h)\n",
    "        if self.debug > 4: \n",
    "            print(\"ycap:\\t\", ycap.shape)\n",
    "        \n",
    "        return ycap, h\n",
    "    \n",
    "    \n",
    "# # with torch.no_grad():\n",
    "# #     print (\"Trying out question encoder LSTM\")\n",
    "# #     model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "# #     dummy_x = torch.tensor([22,45,12], dtype=torch.long)\n",
    "# #     hidden = model.init_hidden()\n",
    "# #     ycap, h = model(dummy_x, hidden)\n",
    "    \n",
    "# #     print(ycap.shape)\n",
    "# #     print(h[0].shape, h[1].shape)\n",
    "\n",
    "\n",
    "if DEBUG > 4:\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        macros = {\n",
    "        \"ques_len\": QUES_LEN,\n",
    "        \"hidden_dim\": HIDDEN_DIM, \n",
    "        \"vocab_size\": VOCAB_SIZE, \n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"para_len\": PARA_LEN,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"lr\": LR,\n",
    "        \"debug\":5,\n",
    "        \"device\":device\n",
    "    }\n",
    "\n",
    "        dummy_para = torch.randint(0,VOCAB_SIZE-1,(PARA_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,PARA_LEN).long()\n",
    "    #     print (dummy_para.shape)\n",
    "        dummy_question = torch.randint(0,VOCAB_SIZE-1,(QUES_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,QUES_LEN).long()\n",
    "    #     print (dummy_question.shape)\n",
    "        glove_file = torch.randn((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "    #     print(\"LSTM with batches\")\n",
    "        ques_model = Encoder(QUES_LEN, macros, glove_file, device).cuda(device)\n",
    "        para_model = Encoder(QUES_LEN, macros, glove_file, device).cuda(device)\n",
    "        ques_hidden = ques_model.init_hidden(BATCH_SIZE, device)\n",
    "        para_hidden = para_model.init_hidden(BATCH_SIZE, device)\n",
    "        ques_embedded,hidden_ques = ques_model(dummy_question,ques_hidden, device)\n",
    "        para_embedded,hidden_para = para_model(dummy_para,para_hidden, device)\n",
    "        \n",
    "#         print (ques_embedded.shape) # question_length,batch,embedding_dim\n",
    "#         print (para_embedded.shape) # para_length,batch,embedding_dim\n",
    "#         print (hidden_para[0].shape,hidden_para[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match LSTM\n",
    "\n",
    "Use a match LSTM to compute a **summarized sequential vector** for the paragraph w.r.t the question.\n",
    "\n",
    "Consider the summarized vector ($H^r$) as the output of a new decoder, where the inputs are $H^p, H^q$ computed above. \n",
    "\n",
    "1. Attend the para word $i$ with the entire question ($H^q$)\n",
    "  \n",
    "    1. $\\vec{G}_i = tanh(W^qH^q + repeat(W^ph^p_i + W^r\\vec{h^r_{i-1} + b^p}))$\n",
    "    \n",
    "    2. *Computing it*: Here, $\\vec{G}_i$ is equivalent to `energy`, computed differently.\n",
    "    \n",
    "    3. Use a linear layer to compute the content within the $repeat$ fn.\n",
    "    \n",
    "    4. Add with another linear (without bias) with $H_q$\n",
    "    \n",
    "    5. $tanh$ the bloody thing\n",
    "  \n",
    "  \n",
    "2. Softmax over it to get $\\alpha$ weights.\n",
    "\n",
    "    1. $\\vec{\\alpha_i} = softmax(w^t\\vec{G}_i + repeat(b))$\n",
    "    \n",
    "3. Use the attention weight vector $\\vec{\\alpha_i}$ to obtain a weighted version of the question and concat it with the current token of the passage to form a vector $\\vec{z_i}$\n",
    "\n",
    "4. Use $\\vec{z_i}$ to compute the desired $h^r_i$:\n",
    "\n",
    "    1. $ h^r_i = LSTM(\\vec{z_i}, h^r_{i-1}) $\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchLSTMEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, macros, device):\n",
    "        \n",
    "        super(MatchLSTMEncoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = macros['hidden_dim']\n",
    "        self.ques_len = macros['ques_len']\n",
    "        self.batch_size = macros['batch_size']\n",
    "        self.debug = macros['debug']    \n",
    "        \n",
    "        # Catch lens and params\n",
    "        self.lin_g_repeat_a_dense = nn.Linear(2*self.hidden_dim, self.hidden_dim)\n",
    "        self.lin_g_repeat_b_dense = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        self.lin_g_nobias = nn.Linear(2*self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        \n",
    "        self.alpha_i_w = nn.Parameter(torch.rand((self.hidden_dim, 1)))\n",
    "        self.alpha_i_b = nn.Parameter(torch.rand((1)))\n",
    "        \n",
    "        self.lstm_summary = nn.LSTM((self.ques_len+1)*2*self.hidden_dim, self.hidden_dim, batch_first=False)\n",
    "                                      \n",
    "    \n",
    "    def forward(self, H_p, h_ri, H_q, hidden, device):\n",
    "        \"\"\"\n",
    "            Ideally, we would have manually unrolled the lstm \n",
    "            but due to memory constraints, we do it in the module.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find the batchsize\n",
    "        batch_size = H_p.shape[1]\n",
    "        \n",
    "        H_r = torch.empty((0, batch_size, self.hidden_dim), device=device, dtype=torch.float)\n",
    "        H_r = torch.cat((H_r, h_ri), dim=0)\n",
    "        \n",
    "        if self.debug > 4:\n",
    "            print( \"H_p:\\t\\t\\t\", H_p.shape)\n",
    "            print( \"H_q:\\t\\t\\t\", H_q.shape)\n",
    "            print( \"h_ri:\\t\\t\\t\", h_ri.shape)\n",
    "            print( \"H_r:\\t\\t\\t\", H_r.shape)\n",
    "            print( \"hid:\\t\\t\\t\", hidden.shape)\n",
    "        \n",
    "        for i in range(H_p.shape[0]):\n",
    "            \n",
    "            # We call the (W^P.H^P + W^rh^r_i-1 + b^P) as lin_repeat_input.\n",
    "            \n",
    "            # We first write out its two components as\n",
    "            lin_repeat_input_a = self.lin_g_repeat_a_dense(H_p[i].view(1, batch_size, -1))\n",
    "            if self.debug > 4: print(\"lin_repeat_input_a:\\t\", lin_repeat_input_a.shape)\n",
    "            \n",
    "            lin_repeat_input_b = self.lin_g_repeat_b_dense(H_r[i].view(1, batch_size, -1))\n",
    "            if self.debug > 4: print(\"lin_repeat_input_b:\\t\", lin_repeat_input_b.shape)\n",
    "            \n",
    "            # Add the two terms up\n",
    "            lin_repeat_input_a.add_(lin_repeat_input_b)\n",
    "#             if self.debug > 4: print(\"lin_g_input_b unrepeated:\", lin_g_input_b.shape)\n",
    "\n",
    "            lin_g_input_b = lin_repeat_input_a.repeat(H_q.shape[0], 1, 1)\n",
    "            if self.debug > 4: print(\"lin_g_input_b:\\t\\t\", lin_g_input_b.shape)\n",
    "\n",
    "            # lin_g_input_a = self.lin_g_nobias.matmul(H_q.view(-1, self.ques_len, self.hidden_dim)) #self.lin_g_nobias(H_q)\n",
    "            lin_g_input_a =  self.lin_g_nobias(H_q)\n",
    "            if self.debug > 4: print(\"lin_g_input_a:\\t\\t\", lin_g_input_a.shape)\n",
    "\n",
    "            G_i = F.tanh(lin_g_input_a + lin_g_input_b)\n",
    "            if self.debug > 4: print(\"G_i:\\t\\t\\t\", G_i.shape)\n",
    "            # Note; G_i should be a 1D vector over ques_len\n",
    "            \n",
    "            # Attention weights\n",
    "            alpha_i_input_a = G_i.transpose(1,0).matmul(self.alpha_i_w).view(batch_size, 1, -1)\n",
    "            if self.debug > 4: print(\"alpha_i_input_a:\\t\", alpha_i_input_a.shape)\n",
    "\n",
    "            alpha_i_input = alpha_i_input_a.add_(self.alpha_i_b.view(-1,1,1).repeat(1,1,self.ques_len))\n",
    "            if self.debug > 4: print(\"alpha_i_input:\\t\\t\", alpha_i_input.shape)\n",
    "\n",
    "            # Softmax over alpha inputs\n",
    "            alpha_i = F.softmax(alpha_i_input, dim=-1)\n",
    "            if self.debug > 4: print(\"alpha_i:\\t\\t\", alpha_i.shape)  \n",
    "                \n",
    "            # Weighted summary of question with alpha    \n",
    "            z_i_input_b = (\n",
    "                            H_q.transpose(1, 0) *\n",
    "                           (alpha_i.view(batch_size, self.ques_len, -1).repeat(1, 1, 2*self.hidden_dim))\n",
    "                          ).transpose(1, 0)\n",
    "            if self.debug > 4: print(\"z_i_input_b:\\t\\t\", z_i_input_b.shape)\n",
    "\n",
    "            z_i = torch.cat((H_p[i].view(1, batch_size, -1), z_i_input_b), dim=0)\n",
    "            if self.debug > 4: print(\"z_i:\\t\\t\\t\", z_i.shape)\n",
    "\n",
    "            # Take input from LSTM, concat in H_r and nullify the temp var.                \n",
    "            h_ri, (_, hidden) = self.lstm_summary(z_i.transpose(1,0).contiguous().view(1, batch_size, -1), \n",
    "                                             (H_r[i].view(1,batch_size, -1), hidden))\n",
    "            if self.debug > 4:\n",
    "                print(\"newh_ri:\\t\\t\", h_ri.shape)\n",
    "                print(\"newhidden:\\t\\t\", hidden.shape)\n",
    "            H_r = torch.cat((H_r, h_ri), dim=0)\n",
    "            \n",
    "            if self.debug > 4:\n",
    "                print(\"\\tH_r:\\t\\t\\t\", H_r.shape)\n",
    "\n",
    "        return H_r[1:]\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return torch.zeros((1, batch_size, self.hidden_dim), device=device)\n",
    "#                 torch.zeros((1, batch_size, self.hidden_dim), device=device))\n",
    "\n",
    "\n",
    "\n",
    "if DEBUG > 4:\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        macros = {\n",
    "            \"ques_len\": QUES_LEN,\n",
    "            \"hidden_dim\": HIDDEN_DIM, \n",
    "            \"vocab_size\": VOCAB_SIZE, \n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"para_len\": PARA_LEN,\n",
    "            \"embedding_dim\": EMBEDDING_DIM,\n",
    "            \"lr\": LR,\n",
    "            \"debug\":5,\n",
    "            \"device\":device\n",
    "        }\n",
    "            \n",
    "        matchLSTMEncoder = MatchLSTMEncoder(macros,device).cuda(device)\n",
    "        hidden = matchLSTMEncoder.init_hidden(BATCH_SIZE,device)\n",
    "        para_embedded = torch.rand((PARA_LEN, BATCH_SIZE, 2*HIDDEN_DIM), device=device)\n",
    "        ques_embedded = torch.rand((QUES_LEN, BATCH_SIZE, 2*HIDDEN_DIM), device=device)\n",
    "        h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "    #     if DEBUG:\n",
    "    #         print (\"init h_ri shape is: \", h_ri.shape)\n",
    "    #         print (\"the para length is \", len(para_embedded))\n",
    "        H_r = matchLSTMEncoder(para_embedded.view(-1,BATCH_SIZE,2*HIDDEN_DIM),\n",
    "                               h_ri, \n",
    "                               ques_embedded, \n",
    "                               hidden,\n",
    "                               device)\n",
    "        print(\"H_r: \", H_r.shape)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network\n",
    "\n",
    "Using a ptrnet over $H_r$ to unfold and get most probable spans.\n",
    "We use the **boundry model** to do that (predict start and end of seq).\n",
    "\n",
    "A simple energy -> softmax -> decoder. Where softmaxed energy is supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class PointerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, macros, device):\n",
    "        super(PointerDecoder, self).__init__()\n",
    "        \n",
    "        # Keep args\n",
    "        self.hidden_dim = macros['hidden_dim']\n",
    "        self.batch_size = macros['batch_size']\n",
    "        self.para_len = macros['para_len']\n",
    "        self.debug = macros['debug']\n",
    "        \n",
    "        self.lin_f_repeat = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.lin_f_nobias = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        \n",
    "        self.beta_k_w = nn.Parameter(torch.randn(self.hidden_dim, 1))\n",
    "        self.beta_k_b = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.hidden_dim*self.para_len, self.hidden_dim, batch_first=False)\n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return torch.zeros((1, batch_size, self.hidden_dim), device=device)\n",
    "#                 torch.zeros((1, batch_size, self.hidden_dim), device=device))\n",
    "    \n",
    "    def forward(self, h_ak, H_r, hidden, device):\n",
    "        \n",
    "        # h_ak (current decoder's last op) (1,batch,hiddendim)\n",
    "        # H_r (weighted summary of para) (P, batch, hiddendim)\n",
    "        batch_size = H_r.shape[1]\n",
    "        \n",
    "        if self.debug > 4:\n",
    "            print(\"h_ak:\\t\\t\\t\", h_ak.shape)\n",
    "            print(\"H_r:\\t\\t\\t\", H_r.shape)\n",
    "            print(\"hidden:\\t\\t\\t\", hidden.shape)\n",
    "            \n",
    "        # Prepare inputs for the tanh used to compute energy\n",
    "        f_input_b = self.lin_f_repeat(h_ak)\n",
    "        if self.debug > 4: print(\"f_input_b unrepeated:  \", f_input_b.shape)\n",
    "        \n",
    "        #H_r shape is ([PARA_LEN, BATCHSIZE, EmbeddingDIM])\n",
    "        f_input_b = f_input_b.repeat(H_r.shape[0], 1, 1)\n",
    "        if self.debug > 4: print(\"f_input_b repeated:\\t\", f_input_b.shape)\n",
    "            \n",
    "        f_input_a = self.lin_f_nobias(H_r)\n",
    "        if self.debug > 4: print(\"f_input_a:\\t\\t\", f_input_a.shape)\n",
    "            \n",
    "        # Send it off to tanh now\n",
    "        F_k = F.tanh(f_input_a+f_input_b)\n",
    "        if self.debug > 4: print(\"F_k:\\t\\t\\t\", F_k.shape) #PARA_LEN,BATCHSIZE,EmbeddingDim\n",
    "        \n",
    "        # Attention weights\n",
    "        beta_k_input_a = F_k.transpose(1,0).matmul(self.beta_k_w).view(batch_size, 1, -1)\n",
    "        if self.debug > 4: print(\"beta_k_input_a:\\t\\t\", beta_k_input_a.shape)\n",
    "            \n",
    "        beta_k_input = beta_k_input_a.add_(self.beta_k_b.repeat(1,1,self.para_len))\n",
    "        if self.debug > 4: print(\"beta_k_input:\\t\\t\", beta_k_input.shape)\n",
    "            \n",
    "        beta_k = F.softmax(beta_k_input, dim=-1)\n",
    "        if self.debug > 4: print(\"beta_k:\\t\\t\\t\", beta_k.shape)\n",
    "        \n",
    "        lstm_input_a = H_r.transpose(1,0) * (beta_k.view(batch_size, self.para_len, -1).repeat(1,1,self.hidden_dim))\n",
    "        if self.debug > 4: print(\"lstm_input_a:\\t\\t\", lstm_input_a.shape)\n",
    "        \n",
    "        h_ak, (_, hidden) = self.lstm(lstm_input_a.transpose(1,0).contiguous().view(1, batch_size, -1), (h_ak, hidden))\n",
    "        \n",
    "        return h_ak, hidden, F.log_softmax(beta_k_input, dim=-1)\n",
    "            \n",
    "if DEBUG > 4:\n",
    "    with torch.no_grad():\n",
    "        macros = {\n",
    "            \"ques_len\": QUES_LEN,\n",
    "            \"hidden_dim\": HIDDEN_DIM, \n",
    "            \"vocab_size\": VOCAB_SIZE, \n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"para_len\": PARA_LEN,\n",
    "            \"embedding_dim\": EMBEDDING_DIM,\n",
    "            \"lr\": LR,\n",
    "            \"debug\":5,\n",
    "            \"device\":device\n",
    "        }\n",
    "        \n",
    "        pointerDecoder = PointerDecoder(macros, device).cuda(device)\n",
    "        h_ak = torch.randn(1,BATCH_SIZE,HIDDEN_DIM, device=device)\n",
    "        H_r = torch.randn(PARA_LEN, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "        hidden = pointerDecoder.init_hidden(BATCH_SIZE, device)\n",
    "        h_ak, hidden, beta_k = pointerDecoder(h_ak, H_r, hidden, device)\n",
    "        print (beta_k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull the real data from disk.\n",
    "\n",
    "Files stored in `./data/squad/train.ids.*`\n",
    "Pull both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_loc, macros, crop=None):\n",
    "    \"\"\"\n",
    "        Given the dataloc and the data available in a specific format, it would pick the data up, and make trainable matrices,\n",
    "        Harvest train_P, train_Q, train_Y, test_P, test_Q, test_Y matrices in this format\n",
    "        \n",
    "        If crop given, will trim the data at a certain length\n",
    "        \n",
    "        **return_type**: np matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpacking macros\n",
    "    PARA_LEN = macros['para_len']\n",
    "    QUES_LEN = macros['ques_len']\n",
    "    \n",
    "    train_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.ids.question')))])\n",
    "    train_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.ids.context')))])\n",
    "    train_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.span')))])\n",
    "\n",
    "    test_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.ids.question')))])\n",
    "    test_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.ids.context')))])\n",
    "    test_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.span')))])\n",
    "\n",
    "    if macros['debug'] > 3:\n",
    "        print(\"Train Q: \", train_q.shape)\n",
    "        print(\"Train P: \", train_p.shape)\n",
    "        print(\"Train Y: \", train_y.shape)\n",
    "        print(\"Test Q: \", test_q.shape)\n",
    "        print(\"Test P: \", test_p.shape)\n",
    "        print(\"Test Y: \", test_y.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "        Parse the semi-raw data:\n",
    "            - shuffle\n",
    "            - pad, prepare\n",
    "            - dump useless vars\n",
    "    \"\"\"\n",
    "    # Shuffle data\n",
    "    \n",
    "    if crop:\n",
    "        index_train, index_test = np.random.choice(np.arange(len(train_p)), crop), \\\n",
    "                                  np.random.choice(np.arange(len(test_p)), crop)\n",
    "    else:\n",
    "        index_train, index_test = np.arange(len(train_p)), np.arange(len(test_p))\n",
    "        np.random.shuffle(index_train)\n",
    "        np.random.shuffle(index_test)\n",
    "\n",
    "    train_p, train_q, train_y = train_p[index_train], train_q[index_train], train_y[index_train]\n",
    "    test_p, test_q, test_y = test_p[index_test], test_q[index_test], test_y[index_test]\n",
    "\n",
    "#     sanity_check(train_p, train_y)\n",
    "\n",
    "    if macros['debug'] >= 5:\n",
    "        print(\"Max q len: \", max(len(q) for q in train_q))\n",
    "        \n",
    "    \n",
    "    # Pad and prepare\n",
    "    train_P = np.zeros((len(train_p), PARA_LEN))\n",
    "    train_Q = np.zeros((len(train_q), QUES_LEN))\n",
    "    train_Y_start = np.zeros((len(train_p), PARA_LEN))\n",
    "    train_Y_end = np.zeros((len(train_p), PARA_LEN))\n",
    "\n",
    "    test_P = np.zeros((len(test_p), PARA_LEN))\n",
    "    test_Q = np.zeros((len(test_q), QUES_LEN))\n",
    "    test_Y_start = np.zeros((len(test_p), PARA_LEN))\n",
    "    test_Y_end = np.zeros((len(test_p), PARA_LEN))\n",
    "    \n",
    "#     print(train_P.shape)\n",
    "\n",
    "    crop_train = []    # Remove these rows from training\n",
    "    for i in range(len(train_p)):\n",
    "        p = train_p[i]\n",
    "        q = train_q[i]\n",
    "        y = train_y[i]\n",
    "        \n",
    "        # First see if you can keep this example or not (due to size)\n",
    "        if y[0] >= PARA_LEN or y[1] >= PARA_LEN:\n",
    "            crop_train.append(i)\n",
    "            continue\n",
    "\n",
    "\n",
    "        train_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "        train_Q[i, :min(QUES_LEN, len(q))] = q[:min(QUES_LEN, len(q))]\n",
    "        train_Y_start[i, y[0]] = 1\n",
    "        train_Y_end[i, y[1]] = 1\n",
    "\n",
    "    crop_test = []\n",
    "    for i in range(len(test_p)):\n",
    "        p = test_p[i]\n",
    "        q = test_q[i]\n",
    "        y = test_y[i]\n",
    "\n",
    "        # First see if you can keep this example or not (due to size)\n",
    "        if y[0] >= PARA_LEN or y[1] >= PARA_LEN:\n",
    "            crop_test.append(i)\n",
    "            continue\n",
    "\n",
    "        test_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "        test_Q[i, :min(QUES_LEN, len(q))] = q[:min(QUES_LEN, len(q))]\n",
    "        test_Y_start[i, y[0]] = 1\n",
    "        test_Y_end[i, y[1]] = 1\n",
    "        \n",
    "        \n",
    "    # Remove the instances which are in crop_train\n",
    "    train_P = np.delete(train_P, crop_train, axis=0)\n",
    "    train_Q = np.delete(train_Q, crop_train, axis=0)\n",
    "    train_Y_start = np.delete(train_Y_start, crop_train, axis=0)\n",
    "    train_Y_end = np.delete(train_Y_end, crop_train, axis=0)\n",
    "    \n",
    "    test_P = np.delete(test_P, crop_test, axis=0)\n",
    "    test_Q = np.delete(test_Q, crop_test, axis=0)\n",
    "    test_Y_start = np.delete(test_Y_start, crop_test, axis=0)\n",
    "    test_Y_end = np.delete(test_Y_end, crop_test, axis=0)\n",
    "\n",
    "    if macros['debug'] >= 1:\n",
    "        print(\"Train Q: \", train_Q.shape)\n",
    "        print(\"Train P: \", train_P.shape)\n",
    "        print(\"Train Y: \", train_Y_start.shape)\n",
    "        print(\"Test Q: \", test_Q.shape)\n",
    "        print(\"Test P: \", test_P.shape)\n",
    "        print(\"Test Y: \", test_Y_start.shape)\n",
    "        print(\"Crop_train: \", len(crop_train))\n",
    "        print(\"Crop_test: \", len(crop_test))\n",
    "    # Let's free up some memory now\n",
    "    train_p, train_q, train_y, test_p, test_q, test_y = None, None, None, None, None, None\n",
    "    \n",
    "    # Load embedding matrics\n",
    "    vectors = np.load(os.path.join(data_loc, 'glove.new.trimmed.300.npy'))\n",
    "    \n",
    "    return train_P, train_Q, train_Y_start, train_Y_end, test_P, test_Q, test_Y_start, test_Y_end, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macros = {\n",
    "#     \"ques_len\": QUES_LEN,\n",
    "#     \"hidden_dim\": HIDDEN_DIM, \n",
    "#     \"vocab_size\": VOCAB_SIZE, \n",
    "#     \"batch_size\": BATCH_SIZE,\n",
    "#     \"para_len\": PARA_LEN,\n",
    "#     \"embedding_dim\": EMBEDDING_DIM,\n",
    "#     \"debug\": 5\n",
    "# } \n",
    "\n",
    "# a = prepare_data(DATA_LOC, macros=macros, crop=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, and running the model\n",
    "- Write a train fn\n",
    "- Write a training loop invoking it\n",
    "- Fill in real data\n",
    "\n",
    "----------\n",
    "\n",
    "Feats:\n",
    "- Function to test every n epochs.\n",
    "- Report train accuracy every epoch\n",
    "- Store the train, test accuracy for every instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def save_model(loc, models, epochs=0, optimizer=None):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            loc: str of the folder where the models are to be saved\n",
    "            models: dict of 'model_name': model_object\n",
    "            epochs, optimizers are int, torch.optims (discarded right now).\n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(models) is dict and len(models.keys()) == 4\n",
    "    \n",
    "    # Assumes four models. Doesn't save device/epochs/optimizer right now.\n",
    "    \n",
    "    for name in models:\n",
    "        torch.save(models[name], os.path.join(loc, name+'.torch'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(para_batch,\n",
    "          ques_batch,\n",
    "          answer_start_batch,\n",
    "          answer_end_batch,\n",
    "          ques_model,\n",
    "          para_model,\n",
    "          mlstm_model,\n",
    "          pointer_decoder_model,\n",
    "          optimizer, \n",
    "          loss_fn,\n",
    "          macros,\n",
    "          debug=2):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    :param para_batch: paragraphs (batch, max_seq_len_para) \n",
    "    :param ques_batch: questions corresponding to para (batch, max_seq_len_ques)\n",
    "    :param answer_start_batch: one-hot vector denoting pos of span start (batch, max_seq_len_para)\n",
    "    :param answer_end_batch: one-hot vector denoting pos of span end (batch, max_seq_len_para)\n",
    "    \n",
    "    # Models\n",
    "    :param ques_model: model to encode ques\n",
    "    :param para_model: model to encode para\n",
    "    :param mlstm_model: model to match para, ques to get para summary\n",
    "    :param pointer_decoder_model: model to get a pointer over start and end span pointer\n",
    "    \n",
    "    # Loss and Optimizer.\n",
    "    :param loss_fn: \n",
    "    :param optimizer: \n",
    "    \n",
    "    :return: \n",
    "    \n",
    "    \n",
    "    NOTE: When using MSE, \n",
    "        - target labels are one-hot\n",
    "        - target label is float tensor\n",
    "        - shape (batch, 1, len)\n",
    "        \n",
    "        When using CrossEntropy\n",
    "        - target is not onehot\n",
    "        - long\n",
    "        - shape (batch, )\n",
    "    \"\"\"\n",
    "    try:    \n",
    "    #     DEBUG = debug\n",
    "    #     BATCH_SIZE = macros['batch_size']\n",
    "    #     HIDDEN_DIM = macros['hidden_dim']\n",
    "\n",
    "        if debug >=2: \n",
    "            print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "            print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "            print(\"\\tanswer_start_batch:\\t\", answer_start_batch.shape)\n",
    "            print(\"\\tanswer_end_batch:\\t\\t\", answer_end_batch.shape)\n",
    "\n",
    "        # Wiping all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initializing all hidden states.\n",
    "        hidden_quesenc = ques_model.init_hidden(macros['batch_size'], device)\n",
    "        hidden_paraenc = para_model.init_hidden(macros['batch_size'], device)\n",
    "        hidden_mlstm = mlstm_model.init_hidden(macros['batch_size'], device)\n",
    "        hidden_ptrnet = pointer_decoder_model.init_hidden(macros['batch_size'], device)\n",
    "        h_ri = torch.zeros((1, macros['batch_size'], macros['hidden_dim']), dtype=torch.float, device=device)\n",
    "        h_ak = torch.zeros((1, macros['batch_size'], macros['hidden_dim']), dtype=torch.float, device=device)\n",
    "        if debug >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "\n",
    "        #passing the data through LSTM pre-processing layer\n",
    "        H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc, device=device)\n",
    "        H_p, para_model_hidden = para_model(para_batch, hidden_paraenc, device=device)\n",
    "        if debug >= 2: \n",
    "            print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "            print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "            print(\"\\tH_ri:\\t\\t\", h_ri.shape)\n",
    "    #         raw_input(\"Check memory and ye shall continue\")\n",
    "            print(\"------------Encoded hidden states------------\")\n",
    "\n",
    "        H_r = mlstm_model(H_p.view(-1, macros['batch_size'], 2*macros['hidden_dim']), h_ri, H_q, hidden_mlstm, device=device)\n",
    "        if debug >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "\n",
    "        #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "        h_ak, hidden_ptrnet, beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device=device)\n",
    "        h_ak, hidden_ptrnet, beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device=device)\n",
    "        if debug >= 2: print(\"------------Passed through pointernet------------\")\n",
    "\n",
    "\n",
    "        # For crossentropy\n",
    "        _, answer_start_batch = answer_start_batch.max(dim=2)\n",
    "        _, answer_end_batch = answer_end_batch.max(dim=2)\n",
    "        answer_start_batch = answer_start_batch.view(-1).long()\n",
    "        answer_end_batch = answer_end_batch.view(-1).long()\n",
    "#         print(beta_k_start.view(-1, macros['para_len']).shape, answer_start_batch.view(-1).shape)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = loss_fn(beta_k_start.view(-1, macros['para_len']), answer_start_batch)\n",
    "        loss += loss_fn(beta_k_end.view(-1, macros['para_len']), answer_end_batch)\n",
    "#         loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "#         loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "        if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "\n",
    "        loss.backward()\n",
    "        if debug >= 2: print(\"------------Calculated Gradients------------\")\n",
    "\n",
    "        #optimization step\n",
    "        optimizer.step()\n",
    "        if debug >= 2: print(\"------------Updated weights.------------\")\n",
    "            \n",
    "        return beta_k_start, beta_k_end, loss\n",
    "    \n",
    "    except: \n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function (no grad, no eval)\n",
    "def predict(para_batch,\n",
    "            ques_batch,\n",
    "            ques_model,\n",
    "            para_model,\n",
    "            mlstm_model,\n",
    "            pointer_decoder_model,\n",
    "            macros,\n",
    "            loss_fn=None,\n",
    "            debug=DEBUG):\n",
    "    \"\"\"\n",
    "        Function which returns the model's output based on a given set of P&Q's. \n",
    "        Does not convert to strings, gives the direct model output.\n",
    "        \n",
    "        Expects:\n",
    "            four models\n",
    "            data\n",
    "            misc macros\n",
    "    \"\"\"\n",
    "    \n",
    "#     BATCH_SIZE = macros['batch_size']\n",
    "    BATCH_SIZE = ques_batch.shape[0]\n",
    "    HIDDEN_DIM = macros['hidden_dim']\n",
    "    DEBUG = debug\n",
    "    \n",
    "    if debug >=2: \n",
    "        print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "        print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "        \n",
    "    with torch.no_grad():    \n",
    "\n",
    "        # Initializing all hidden states.\n",
    "        hidden_quesenc = ques_model.init_hidden(BATCH_SIZE, device)\n",
    "        hidden_paraenc = para_model.init_hidden(BATCH_SIZE, device)\n",
    "        hidden_mlstm = mlstm_model.init_hidden(BATCH_SIZE, device)\n",
    "        hidden_ptrnet = pointer_decoder_model.init_hidden(BATCH_SIZE, device)\n",
    "        h_ri = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        h_ak = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        if DEBUG >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "            \n",
    "        #passing the data through LSTM pre-processing layer\n",
    "        H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc, device)\n",
    "        H_p, para_model_hidden = para_model(para_batch, hidden_paraenc, device)\n",
    "        if DEBUG >= 2: \n",
    "            print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "            print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "            print(\"\\tH_ri:\\t\\t\", h_ri.shape)\n",
    "#             raw_input(\"Check memory and ye shall continue\")\n",
    "            print(\"------------Encoded hidden states------------\")\n",
    "\n",
    "        H_r = mlstm_model(H_p.view(-1, BATCH_SIZE, 2*HIDDEN_DIM), h_ri, H_q, hidden_mlstm, device)\n",
    "        if DEBUG >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "\n",
    "        #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "        h_ak, hidden_ptrnet, beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device)\n",
    "        _, _, beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device)\n",
    "        if DEBUG >= 2: print(\"------------Passed through pointernet------------\")\n",
    "                            \n",
    "        # For crossentropy\n",
    "#         _, answer_start_batch = answer_start_batch.max(dim=2)[1]\n",
    "#         _, answer_end_batch = answer_end_batch.max(dim=2)[1]\n",
    "#         print(\"labels: \", answer_start_batch.shape)[1]\n",
    "            \n",
    "#         #How will we manage batches for loss.\n",
    "#         loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "#         loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "#         if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "            \n",
    "        return (beta_k_start, beta_k_end, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'em': 0.5, u'p': 1.0, u'r': 0.75, u'f1': 0.8333333333333333}\n"
     ]
    }
   ],
   "source": [
    "# Eval function (no grad no eval no nothing)\n",
    "def eval(y_cap, y, metrics={'em':None, 'p':None, 'r':None, 'f1':None}):\n",
    "    \"\"\" \n",
    "        Returns the exact-match (em) metric by default.\n",
    "        Can specifiy more in a list (TODO)\n",
    "        \n",
    "        Inputs:\n",
    "        - y_cap: list of two tensors (start, end) of dim [BATCH_SIZE, PARA_LEN] each\n",
    "        - y: list of two tensors (start, end) of dim [BATCH_SIZE, 1] each\n",
    "    \"\"\"\n",
    "    \n",
    "#     y_cap= torch.argmax(y_cap[0], dim=1).float(), torch.argmax(y_cap[1], dim=1).float()\n",
    "#     y = torch.argmax(y[0], dim=1).float(), torch.argmax(y[1], dim=1).float()\n",
    "    \n",
    "    \n",
    "    # If we want f1 and haven't specified that we want p and q, fuck it and add it there\n",
    "    if 'f1' in metrics.keys():\n",
    "        metrics['p'] = None \n",
    "        metrics['r'] = None\n",
    "    \n",
    "    # Convert to numpy arrays of size (batch, 2)\n",
    "    y_cap= np.vstack((torch.argmax(y_cap[0], dim=1).float().data, torch.argmax(y_cap[1], dim=1).float().data)).transpose()\n",
    "    y = np.vstack((torch.argmax(y[0], dim=1).float().data, torch.argmax(y[1], dim=1).float().data)).transpose()\n",
    "      \n",
    "    # First, if start > end, fix that (we're cool that way.)\n",
    "    for i in range(y_cap.shape[0]):\n",
    "        if y_cap[i][0] > y_cap[i][1]: \n",
    "            y_cap[i] = y_cap[i][[1,0]]\n",
    "            \n",
    "    if \"em\" in metrics.keys():\n",
    "        metrics['em'] = np.mean(np.logical_and(np.equal(y[:,0], y_cap[:,0]),np.equal(y[:,1], y_cap[:,1])))\n",
    "            \n",
    "    if 'f1' in metrics.keys():\n",
    "        \n",
    "        f1, pr, rk = [], [], []\n",
    "        for i in range(y.shape[0]):\n",
    "#             _y, _y_cap = [], []\n",
    "            \n",
    "            if y[i][0] == y[i][1]:\n",
    "                _y = [int(y[i][0])]\n",
    "            else:\n",
    "                _y = range(y[i][0], y[i][1])\n",
    "                \n",
    "            if y_cap[i][0] == y_cap[i][1]:\n",
    "                _y_cap = [int(y_cap[i][0])]\n",
    "            else:\n",
    "                _y_cap = range(y_cap[i][0], y_cap[i][1])\n",
    "                \n",
    "#             print(_y)\n",
    "#             print(_y_cap)\n",
    "                \n",
    "            intersection = len(set(_y).intersection(_y_cap))\n",
    "            \n",
    "            positives = float(len(_y_cap))\n",
    "            truth = float(len(_y))\n",
    "            \n",
    "            p = intersection/positives\n",
    "            r = intersection/truth\n",
    "            \n",
    "#             try:\n",
    "#                 p = intersection/positives\n",
    "#                 r = intersection/truth\n",
    "#             except:\n",
    "#                 traceback.print_exc()\n",
    "#                 print(\"y: \", y[i])\n",
    "#                 print(\"ycap: \", y_cap[i])\n",
    "#                 print(\"pos: \", positives)\n",
    "            \n",
    "            f = (2*p*r)/(p+r) if p > 0 and r > 0 else 0.0\n",
    "            \n",
    "            f1.append(f)\n",
    "            pr.append(p)\n",
    "            rk.append(r)\n",
    "            \n",
    "        f1 = np.mean(f1)\n",
    "        pr = np.mean(pr)\n",
    "        rk = np.mean(rk)\n",
    "        \n",
    "        metrics['f1'] = f1\n",
    "        metrics['p'] = pr\n",
    "        metrics['r'] = rk\n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "                \n",
    "#         _y_cap = np.asarray([np.arange(x[0],x[1]) for x in y_cap])\n",
    "#         _y = np.asarray([np.arange(x[0],x[1]) for x in y])\n",
    "        \n",
    "# #         intersection = [float(len(np.intersect1d(y[i],y_cap[i]))) for i in range(len(y))]\n",
    "#         intersection = np.min((y[:,1], y_cap[:,1]), axis=0)-np.max((y[:,0], y_cap[:,0]), axis=0).astype(np.float)\n",
    "#         intersection[intersection<0] = 0.0\n",
    "#         patchwork = np.zeros(intersection.shape)\n",
    "#         for i in range(len(y)):\n",
    "#             if y[i][0] == y[i][1] and y[i][0]\n",
    "#         intersection =  intersection.astype(np.float)\n",
    "        \n",
    "#         positives = y_cap[:,1] - y_cap[:,0]\n",
    "#         truth = y[:,1] - y[:,0]\n",
    "        \n",
    "#         p = np.mean(intersection/positives.astype(np.float))\n",
    "#         r = np.mean(intersection/truth.astype(np.float))\n",
    "        \n",
    "#         metrics['p'] = p\n",
    "#         metrics['r'] = r\n",
    "        \n",
    "#         # f1\n",
    "#         # If p or r is zero, f1 is zero\n",
    "#         if p > 0 and r > 0:\n",
    "#             metrics['f1'] = 2*p*r/(p+r)\n",
    "#         else:\n",
    "#             metrics['f1'] = 0.0\n",
    "    \n",
    "#     else:\n",
    "#         if 'p' in metrics.keys():\n",
    "            \n",
    "#             intersection = np.min((y[:,1], y_cap[:,1]), axis=0)-np.max((y[:,0], y_cap[:,0]), axis=0)\n",
    "#             intersection[intersection<0] = 0\n",
    "#             intersection =  intersection.astype(np.float)\n",
    "\n",
    "#             positives = y_cap[:,1] - y_cap[:,0]\n",
    "\n",
    "#             p = np.mean(intersection/positives)\n",
    "\n",
    "#             metrics['p'] = p\n",
    "            \n",
    "#         if 'r'  in metrics.keys():\n",
    "\n",
    "#             intersection = np.min((y[:,1], y_cap[:,1]), axis=0)-np.max((y[:,0], y_cap[:,0]), axis=0)\n",
    "#             intersection[intersection<0] = 0\n",
    "#             intersection =  intersection.astype(np.float)\n",
    "\n",
    "#             truth = y[:,1] - y[:,0]\n",
    "\n",
    "#             r = np.mean(intersection/truth)\n",
    "\n",
    "#             metrics['r'] = r\n",
    "        \n",
    "#     if metrics['em'] > 0 and metrics['f1'] <= 0:\n",
    "#         print(\"Here comes\")\n",
    "#         i = np.argmax(np.logical_and(np.equal(y[:,0], y_cap[:,0]),np.equal(y[:,1], y_cap[:,1])))\n",
    "#         print(i,'\\n',y[i],y_cap[i])\n",
    "        \n",
    "    if DEBUG >= 3: \n",
    "        print(\"Test performance: \", metrics)\n",
    "        print(\"------------Evaluated------------\")\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "if True:\n",
    "    # Testing this function\n",
    "    metrics = {'em':None}\n",
    "#     y = torch.tensor([[3]]).float(), torch.tensor([[4]]).float()\n",
    "    y = torch.tensor([[0,0,3,0], [0,2,0,0]]), torch.tensor([[0,0,0,3], [0,0,0,3]])\n",
    "    y_cap = torch.tensor([[0,0,3,0],[0,0,3,0]]), torch.tensor([[0,0,0,3],[0,0,0,3]])\n",
    "#     y = torch.randint(0, PARA_LEN, (BATCH_SIZE,)).float(), torch.randint(0, PARA_LEN, (BATCH_SIZE,)).float()\n",
    "#     y_cap = torch.rand((BATCH_SIZE, PARA_LEN)), torch.rand((BATCH_SIZE, PARA_LEN))\n",
    "    print(eval(y_cap, y))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(_models, _data, _macros, _epochs, _save=0, _test_eval=0, _train_eval=0, _debug=2):\n",
    "    \"\"\"\n",
    "        > Instantiate models\n",
    "        > Instantiate loss, optimizer\n",
    "        > Instantiate ways to store loss\n",
    "\n",
    "        > Per epoch\n",
    "            > sample batch and give to train fn\n",
    "            > get loss\n",
    "            > if epoch %k ==0: get test accuracy\n",
    "\n",
    "        > have fn to calculate test accuracy\n",
    "        \n",
    "        > _save: int\n",
    "            > 0: dont\n",
    "            > 1+: save every _save epoch (overwrite)\n",
    "            > -1 -> save best (turned to 1 if test evals dont happen.)\n",
    "        \n",
    "        > Save the model at every epoch if we don't test on test. \n",
    "            > else save on the best performning mode\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack data\n",
    "    DEBUG = _debug\n",
    "    train_P = _data['train']['P']\n",
    "    train_Q = _data['train']['Q']\n",
    "    train_Y_start = _data['train']['Ys']\n",
    "    train_Y_end = _data['train']['Ye']\n",
    "    test_P = _data['test']['P']\n",
    "    test_Q = _data['test']['Q']\n",
    "    test_Y_start = _data['test']['Ys']\n",
    "    test_Y_end = _data['test']['Ye']\n",
    "\n",
    "    ques_model, para_model, mlstm_model, pointer_decoder_model = _models\n",
    "    _data = None\n",
    "\n",
    "    # Instantiate Loss\n",
    "#         loss_fn = nn.MSELoss()\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, ques_model.parameters())) + \n",
    "                             list(filter(lambda p: p.requires_grad, para_model.parameters())) + \n",
    "                             list(mlstm_model.parameters()) + \n",
    "                             list(pointer_decoder_model.parameters()), lr=macros['lr'])\n",
    "#         optimizer = optim.Adam(list(ques_model.parameters()) + \\\n",
    "#                                list(para_model.parameters()) + \\\n",
    "#                                list(mlstm_model.parameters()) + \\\n",
    "#                               list(pointer_decoder_model.parameters()), lr=macros['lr'])\n",
    "\n",
    "    # Losses\n",
    "    train_losses = []\n",
    "    train_em = []\n",
    "    train_f = []\n",
    "    test_losses = []\n",
    "    test_em = []\n",
    "    test_f = []\n",
    "    best_test = 0.0\n",
    "    found_best_test = False\n",
    "    \n",
    "    try: \n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(_epochs):\n",
    "            print(\"Epoch: \", epoch, \"/\", _epochs)\n",
    "\n",
    "            epoch_loss = []\n",
    "            epoch_train_em = []\n",
    "            epoch_train_f = []\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            for iter in range(int(len(train_P)/BATCH_SIZE)):\n",
    "    #         for iter in range(2):\n",
    "\n",
    "                batch_time = time.time()\n",
    "\n",
    "                # Sample batch and train on it\n",
    "                sample_index = np.random.randint(0, len(train_P), _macros['batch_size'])\n",
    "            \n",
    "#                 grad_old = sum([x.grad.sum().item() for x in params])\n",
    "\n",
    "                y_cap_start, y_cap_end, loss = train(\n",
    "                    para_batch = torch.tensor(train_P[sample_index], dtype=torch.long, device=device),\n",
    "                    ques_batch = torch.tensor(train_Q[sample_index], dtype=torch.long, device=device),\n",
    "                    answer_start_batch = torch.tensor(train_Y_start[sample_index], dtype=torch.float, device=device).view( _macros['batch_size'], 1, _macros['para_len']),\n",
    "                    answer_end_batch = torch.tensor(train_Y_end[sample_index], dtype=torch.float, device=device).view(_macros['batch_size'], 1, _macros['para_len']),\n",
    "                    ques_model = ques_model,\n",
    "                    para_model = para_model,\n",
    "                    mlstm_model = mlstm_model,\n",
    "                    pointer_decoder_model = pointer_decoder_model,\n",
    "                    optimizer = optimizer, \n",
    "                    loss_fn= loss_fn,\n",
    "                    macros=_macros,\n",
    "                    debug=_macros['debug']\n",
    "                )\n",
    "\n",
    "                if _train_eval: \n",
    "\n",
    "                    # Calculate train accuracy for this minibatch\n",
    "                    metrics = eval(\n",
    "                        y=(torch.tensor(train_Y_start[sample_index], dtype=torch.long, device=device).view( -1, _macros['para_len']),\n",
    "                            torch.tensor(train_Y_end[sample_index], dtype=torch.long, device=device).view(-1, _macros['para_len'])),\n",
    "                        y_cap=[y_cap_start.squeeze(), y_cap_end.squeeze()])\n",
    "\n",
    "                    epoch_train_em.append(metrics['em'])\n",
    "                    epoch_train_f.append(metrics['f1'])\n",
    "    \n",
    "                epoch_loss.append(loss.item())\n",
    "    \n",
    "#                 grad_new = sum([x.grad.sum().item() for x in params])\n",
    "\n",
    "                print(\"Batch:\\t%d\" % iter,\"/%d\\t\\b: \" % (len(train_P)/_macros['batch_size']),\n",
    "                      str(\"%s\" % (time.time() - batch_time))[:8], \n",
    "                      str(\"\\t\\b%s\" % (time.time() - epoch_time))[:10], \n",
    "                      \"\\tl:%f\" % loss.item(),\n",
    "                      \"\\tem:%f\" % epoch_train_em[-1] if _train_eval else \"\",\n",
    "                     \"\\t\\bf1:%f\" % epoch_train_f[-1] if _train_eval else \"\")\n",
    "#                      \"\\t\\b\\b%s\" % grad_new - grad_old)\n",
    "#                      end=None if iter+1 == int(len(train_P)/BATCH_SIZE) else \"\\r\")\n",
    "\n",
    "            train_losses.append(epoch_loss)\n",
    "        \n",
    "            if _train_eval: \n",
    "                train_em.append(epoch_train_em)\n",
    "                train_f.append(epoch_train_f)\n",
    "#             if np.mean(epoch_train_em) > best_test_em:\n",
    "#                 found_best_test_em = True\n",
    "#                 best_test_em = np.mean(epoch_train_em)\n",
    "                \n",
    "            if _test_eval and epoch % _test_eval == 0:\n",
    "\n",
    "                y_cap_start, y_cap_end, test_loss = predict(\n",
    "                    para_batch = torch.tensor(test_P, dtype=torch.long, device=device),\n",
    "                    ques_batch = torch.tensor(test_Q, dtype=torch.long, device=device),\n",
    "                    ques_model = ques_model,\n",
    "                    para_model = para_model,\n",
    "                    mlstm_model = mlstm_model,\n",
    "                    pointer_decoder_model = pointer_decoder_model,\n",
    "                    macros = _macros,\n",
    "                    loss_fn= loss_fn,\n",
    "                    debug = _macros['debug']\n",
    "                )\n",
    "                metrics = eval(\n",
    "                    y=(torch.tensor(test_Y_start, dtype=torch.long, device=device).view( -1, _macros['para_len']),\n",
    "                        torch.tensor(test_Y_end, dtype=torch.long, device=device).view(-1, _macros['para_len'])),\n",
    "                    y_cap=[y_cap_start.squeeze(), y_cap_end.squeeze()])\n",
    "\n",
    "                test_losses.append(test_loss)\n",
    "                test_em.append(metrics['em'])\n",
    "                test_f.append(metrics['f1'])\n",
    "                \n",
    "                # Check if we outperformed the best one.\n",
    "                if metrics['f1'] > best_test:\n",
    "                    \n",
    "                    # Set flag\n",
    "                    found_best_test = True\n",
    "                    \n",
    "                    # Update value\n",
    "                    best_test = metrics['f1']   \n",
    "                \n",
    "            # Saving logic\n",
    "            if _save == 0:\n",
    "                pass\n",
    "            elif ( _save>0 and epoch % _save == 0) or \\\n",
    "            ( _save == -1 and found_best_test ):\n",
    "                models = { 'ques_model': ques_model,\n",
    "                           'para_model': para_model,\n",
    "                           'mlstm_model':  mlstm_model,\n",
    "                           'pointer_decoder_model': pointer_decoder_model\n",
    "                         }\n",
    "                \n",
    "                save_model(macros['save_model_loc'], models,\n",
    "                          epochs=epoch,\n",
    "                           optimizer=optimizer)\n",
    "                \n",
    "                print(\"Saving new model on epoch %d\" % epoch)\n",
    "            \n",
    "            # Reset flags\n",
    "            found_best_test = False\n",
    "            \n",
    "            # At the end of every epoch, do print the average epoch loss, and other stat\n",
    "            print(\"\\nEpoch performance: \",\n",
    "                  \"%ssec\" % str(time.time() - epoch_time)[:6],\n",
    "                  \"Trl:%f\" % np.mean(epoch_loss, axis=0),\n",
    "                  \"\\n\\tTrem:%f\" % np.mean(epoch_train_em) if _train_eval and epoch % _train_eval == 0 else \"\",\n",
    "                  \"\\tTrf1:%f\" % np.mean(epoch_train_f) if _train_eval and epoch % _train_eval == 0 else \"\",\n",
    "                  \"\\tTeem:%f\" % test_em[-1] if _test_eval and epoch % _test_eval == 0 else \"\",\n",
    "                  \"\\tTef1:%f\\n\" % test_f[-1] if _test_eval and epoch % _test_eval == 0 else \"\\n\")\n",
    "\n",
    "#         return train_losses, train_em, test_losses, test_em\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        # someone called a ctrl+c on it. Let' return the things computed so far atlest.\n",
    "        print(\"Found keyboard interrupt. Stopping training loop\")\n",
    "        \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:       \n",
    "        return train_losses, train_em, train_f, test_losses, test_em, test_f, best_test\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "    Saving said models.\n",
    "    TODO\n",
    "\"\"\"\n",
    "# ques_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(loss, loss2=None, _label=\"Some label\", _label2=\"Some other label\", _name=\"Generic Name\", _only_epoch=True):\n",
    "    \"\"\"\n",
    "        Fn to visualize loss.\n",
    "        Expects either\n",
    "            - [int, int] for epoch level stuff\n",
    "            - [ [int, int], [int, int] ] for batch level data. \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = [15, 8] \n",
    "    \n",
    "    # Detect input format\n",
    "    if type(loss[0]) is not list: #in [int, float, long]:\n",
    "        \n",
    "#         print(\"here\")\n",
    "        plt.plot(loss, '-b', label=_label)\n",
    "        if loss2: plt.plot(loss2, '-r', label=_label2)\n",
    "        plt.ylabel(_name)\n",
    "        pylab.legend(loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "    elif type(loss[0]) == list:\n",
    "        \n",
    "        if _only_epoch:\n",
    "            loss = [ np.mean(x) for x in loss ]\n",
    "            if loss2 is not None: \n",
    "                loss2 = [ np.mean(x) for x in loss2 ]\n",
    "            \n",
    "        else:\n",
    "            loss = [ y for x in loss for y in x ]\n",
    "            if loss2 is not None: loss2 = [ y for x in loss2 for y in x ]\n",
    "            \n",
    "        plt.plot(loss, '-b', label=_label)\n",
    "        if loss2 is not None: plt.plot(loss2, '-r', label=_label2)\n",
    "        plt.ylabel(_name)\n",
    "        pylab.legend(loc='upper left')\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator\n",
    "\n",
    "One cell which instantiates and runs everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Q:  (466, 30)\n",
      "Train P:  (466, 200)\n",
      "Train Y:  (466, 200)\n",
      "Test Q:  (119, 30)\n",
      "Test P:  (119, 200)\n",
      "Test Y:  (119, 200)\n",
      "Crop_train:  7\n",
      "Crop_test:  0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Cell which pulls everything together.\n",
    "\n",
    "    > init models\n",
    "    > get data prepared\n",
    "    > pass models and data to training loop\n",
    "    > gets trained models and loss\n",
    "    > saves models\n",
    "    > visualizes loss?\n",
    "\n",
    "No other function but this one ever sees global macros!\n",
    "\"\"\"\n",
    "macros = {\n",
    "    \"ques_len\": QUES_LEN,\n",
    "    \"hidden_dim\": HIDDEN_DIM, \n",
    "    \"vocab_size\": VOCAB_SIZE, \n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"para_len\": PARA_LEN,\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"lr\": LR,\n",
    "    \"debug\":DEBUG,\n",
    "    \"save_model_loc\": MODEL_LOC\n",
    "#     \"device\": device\n",
    "} \n",
    "\n",
    "data = {'train':{}, 'test':{}}\n",
    "data['train']['P'], data['train']['Q'], data['train']['Ys'], data['train']['Ye'], \\\n",
    "data['test']['P'], data['test']['Q'], data['test']['Ys'], data['test']['Ye'], vectors = \\\n",
    "    prepare_data(DATA_LOC, macros, crop=CROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate models\n",
    "ques_model = Encoder(QUES_LEN, macros, vectors, device).cuda(device)\n",
    "para_model = Encoder(PARA_LEN, macros, vectors, device).cuda(device)\n",
    "mlstm_model = MatchLSTMEncoder(macros, device).cuda(device)\n",
    "pointer_decoder_model = PointerDecoder(macros, device).cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 / 300\n",
      "Batch:\t0 /9:  1.443475 1.443495 \tl:10.583466 \tem:0.000000 f1:0.109029\n",
      "Batch:\t1 /9:  1.186676 2.630656 \tl:9.414362 \tem:0.000000 f1:0.055898\n",
      "Batch:\t2 /9:  1.070636 3.701713 \tl:8.668116 \tem:0.000000 f1:0.052433\n",
      "Batch:\t3 /9:  1.103342 4.805891 \tl:8.522520 \tem:0.000000 f1:0.058390\n",
      "Batch:\t4 /9:  1.211871 6.018450 \tl:9.525246 \tem:0.000000 f1:0.071845\n",
      "Batch:\t5 /9:  1.256335 7.275634 \tl:8.589943 \tem:0.000000 f1:0.020586\n",
      "Batch:\t6 /9:  1.207035 8.483947 \tl:7.914226 \tem:0.000000 f1:0.066475\n",
      "Batch:\t7 /9:  1.181278 9.666632 \tl:8.140624 \tem:0.000000 f1:0.049781\n",
      "Batch:\t8 /9:  1.116395 10.78414 \tl:8.158880 \tem:0.000000 f1:0.044039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/priyansh/virtualenvironment/pwc-reg-m/local/lib/python2.7/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type PointerDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/data/priyansh/virtualenvironment/pwc-reg-m/local/lib/python2.7/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/data/priyansh/virtualenvironment/pwc-reg-m/local/lib/python2.7/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type MatchLSTMEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new model on epoch 0\n",
      "\n",
      "Epoch performance:  11.657sec Trl:8.835265 \n",
      "\tTrem:0.000000 \tTrf1:0.058719 \tTeem:0.000000 \tTef1:0.066727\n",
      "\n",
      "Epoch:  1 / 300\n",
      "Batch:\t0 /9:  1.141504 1.141520 \tl:8.112770 \tem:0.000000 f1:0.077118\n",
      "Batch:\t1 /9:  1.058375 2.200437 \tl:7.979981 \tem:0.000000 f1:0.056627\n",
      "Batch:\t2 /9:  1.173218 3.374130 \tl:7.328404 \tem:0.000000 f1:0.143073\n",
      "Batch:\t3 /9:  1.108540 4.483371 \tl:7.801500 \tem:0.000000 f1:0.044944\n",
      "Batch:\t4 /9:  1.490628 5.974680 \tl:7.394086 \tem:0.000000 f1:0.088575\n",
      "Batch:\t5 /9:  1.226348 7.201564 \tl:6.660672 \tem:0.000000 f1:0.105497\n",
      "Batch:\t6 /9:  1.237454 8.439659 \tl:7.456442 \tem:0.000000 f1:0.079975\n",
      "Batch:\t7 /9:  1.183071 9.623573 \tl:7.322899 \tem:0.000000 f1:0.049188\n",
      "Batch:\t8 /9:  1.121415 10.74632 \tl:7.007781 \tem:0.000000 f1:0.070035\n",
      "Saving new model on epoch 1\n",
      "\n",
      "Epoch performance:  11.475sec Trl:7.451615 \n",
      "\tTrem:0.000000 \tTrf1:0.079448 \tTeem:0.000000 \tTef1:0.066936\n",
      "\n",
      "Epoch:  2 / 300\n",
      "Batch:\t0 /9:  1.207694 1.207710 \tl:7.287562 \tem:0.000000 f1:0.056449\n",
      "Batch:\t1 /9:  1.125352 2.333657 \tl:6.865221 \tem:0.000000 f1:0.123096\n",
      "Batch:\t2 /9:  1.114248 3.448669 \tl:6.819185 \tem:0.040000 f1:0.150028\n",
      "Batch:\t3 /9:  1.235746 4.685884 \tl:6.219960 \tem:0.000000 f1:0.107097\n",
      "Batch:\t4 /9:  1.550938 6.237608 \tl:6.829328 \tem:0.000000 f1:0.069599\n",
      "Batch:\t5 /9:  1.429743 7.668992 \tl:7.301320 \tem:0.000000 f1:0.080767\n",
      "Batch:\t6 /9:  1.379653 9.049301 \tl:6.619207 \tem:0.000000 f1:0.109810\n",
      "Batch:\t7 /9:  1.436408 10.48601 \tl:6.396840 \tem:0.000000 f1:0.096112\n",
      "Batch:\t8 /9:  1.264012 11.75125 \tl:6.205810 \tem:0.000000 f1:0.180127\n",
      "Saving new model on epoch 2\n",
      "\n",
      "Epoch performance:  12.510sec Trl:6.727159 \n",
      "\tTrem:0.004444 \tTrf1:0.108121 \tTeem:0.016807 \tTef1:0.087061\n",
      "\n",
      "Epoch:  3 / 300\n",
      "Batch:\t0 /9:  1.306678 1.306694 \tl:5.863198 \tem:0.020000 f1:0.173952\n",
      "Batch:\t1 /9:  1.163735 2.471318 \tl:6.518927 \tem:0.000000 f1:0.075932\n",
      "Batch:\t2 /9:  1.297374 3.769459 \tl:5.939472 \tem:0.000000 f1:0.110969\n",
      "Batch:\t3 /9:  1.254477 5.025315 \tl:6.124633 \tem:0.000000 f1:0.113720\n",
      "Batch:\t4 /9:  1.309767 6.335534 \tl:5.998361 \tem:0.000000 f1:0.145940\n",
      "Batch:\t5 /9:  1.207673 7.544482 \tl:6.222171 \tem:0.000000 f1:0.077169\n",
      "Batch:\t6 /9:  1.494616 9.039826 \tl:5.791663 \tem:0.000000 f1:0.085235\n",
      "Batch:\t7 /9:  1.443174 10.48387 \tl:5.914903 \tem:0.000000 f1:0.106042\n",
      "Batch:\t8 /9:  1.381528 11.86665 \tl:5.762264 \tem:0.000000 f1:0.081212\n",
      "\n",
      "Epoch performance:  12.061sec Trl:6.015066 \n",
      "\tTrem:0.002222 \tTrf1:0.107797 \tTeem:0.000000 \tTef1:0.077664\n",
      "\n",
      "Epoch:  4 / 300\n",
      "Batch:\t0 /9:  1.396446 1.396460 \tl:5.551534 \tem:0.000000 f1:0.079853\n",
      "Batch:\t1 /9:  1.430228 2.827998 \tl:5.489725 \tem:0.000000 f1:0.104914\n",
      "Batch:\t2 /9:  1.476550 4.305322 \tl:5.357818 \tem:0.000000 f1:0.093166\n",
      "Batch:\t3 /9:  1.449448 5.755364 \tl:4.837436 \tem:0.000000 f1:0.106738\n",
      "Batch:\t4 /9:  1.407673 7.164563 \tl:4.677864 \tem:0.020000 f1:0.157665\n",
      "Batch:\t5 /9:  1.446837 8.612051 \tl:5.669066 \tem:0.000000 f1:0.093679\n",
      "Batch:\t6 /9:  1.408233 10.02109 \tl:4.059865 \tem:0.020000 f1:0.182402\n",
      "Batch:\t7 /9:  1.280078 11.30231 \tl:4.407181 \tem:0.000000 f1:0.130254\n",
      "Batch:\t8 /9:  1.102115 12.40461 \tl:4.912185 \tem:0.020000 f1:0.068757\n",
      "\n",
      "Epoch performance:  12.618sec Trl:4.995852 \n",
      "\tTrem:0.006667 \tTrf1:0.113048 \tTeem:0.016807 \tTef1:0.039072\n",
      "\n",
      "Epoch:  5 / 300\n",
      "Batch:\t0 /9:  1.264570 1.264597 \tl:4.125364 \tem:0.000000 f1:0.057101\n",
      "Batch:\t1 /9:  1.123509 2.388761 \tl:4.240085 \tem:0.000000 f1:0.097568\n",
      "Batch:\t2 /9:  1.159934 3.549719 \tl:4.137084 \tem:0.000000 f1:0.033020\n",
      "Batch:\t3 /9:  1.099340 4.649927 \tl:4.144815 \tem:0.000000 f1:0.086802\n",
      "Batch:\t4 /9:  1.149446 5.799879 \tl:3.786431 \tem:0.000000 f1:0.185559\n",
      "Batch:\t5 /9:  1.099361 6.900263 \tl:4.156988 \tem:0.000000 f1:0.098261\n",
      "Batch:\t6 /9:  1.188805 8.089986 \tl:3.619961 \tem:0.000000 f1:0.168797\n",
      "Batch:\t7 /9:  1.316359 9.407094 \tl:3.419767 \tem:0.000000 f1:0.122498\n",
      "Batch:\t8 /9:  1.556990 10.96480 \tl:4.043047 \tem:0.020000 f1:0.106469\n",
      "\n",
      "Epoch performance:  11.170sec Trl:3.963727 \n",
      "\tTrem:0.002222 \tTrf1:0.106231 \tTeem:0.016807 \tTef1:0.062883\n",
      "\n",
      "Epoch:  6 / 300\n",
      "Batch:\t0 /9:  1.557337 1.557353 \tl:3.971370 \tem:0.020000 f1:0.100348\n",
      "Batch:\t1 /9:  1.474697 3.033354 \tl:3.532715 \tem:0.000000 f1:0.119666\n",
      "Batch:\t2 /9:  1.434906 4.468754 \tl:3.336041 \tem:0.000000 f1:0.088808\n",
      "Batch:\t3 /9:  1.539864 6.034582 \tl:3.750000 \tem:0.000000 f1:0.093134\n",
      "Batch:\t4 /9:  1.432606 7.468637 \tl:3.809489 \tem:0.020000 f1:0.134909\n",
      "Batch:\t5 /9:  1.189778 8.658942 \tl:3.221941 \tem:0.040000 f1:0.182005\n",
      "Batch:\t6 /9:  1.074012 9.733159 \tl:3.517370 \tem:0.000000 f1:0.107913\n",
      "Batch:\t7 /9:  1.217278 10.95123 \tl:3.389116 \tem:0.000000 f1:0.106960\n",
      "Batch:\t8 /9:  1.144931 12.09692 \tl:3.199608 \tem:0.000000 f1:0.185328\n",
      "\n",
      "Epoch performance:  12.297sec Trl:3.525295 \n",
      "\tTrem:0.008889 \tTrf1:0.124341 \tTeem:0.016807 \tTef1:0.070663\n",
      "\n",
      "Epoch:  7 / 300\n",
      "Batch:\t0 /9:  1.295456 1.295477 \tl:3.289530 \tem:0.080000 f1:0.206813\n",
      "Batch:\t1 /9:  1.599823 2.895965 \tl:3.308245 \tem:0.000000 f1:0.094237\n",
      "Batch:\t2 /9:  1.456384 4.353603 \tl:2.788931 \tem:0.020000 f1:0.182113\n",
      "Batch:\t3 /9:  1.226288 5.580563 \tl:2.917871 \tem:0.060000 f1:0.205944\n",
      "Batch:\t4 /9:  1.133359 6.714921 \tl:3.544142 \tem:0.020000 f1:0.101576\n",
      "Batch:\t5 /9:  1.057901 7.773098 \tl:2.669762 \tem:0.080000 f1:0.198290\n",
      "Batch:\t6 /9:  1.265390 9.039471 \tl:2.969719 \tem:0.060000 f1:0.169250\n",
      "Batch:\t7 /9:  1.568989 10.60968 \tl:2.914409 \tem:0.100000 f1:0.275907\n",
      "Batch:\t8 /9:  1.238634 11.84928 \tl:2.754514 \tem:0.020000 f1:0.147066\n",
      "Saving new model on epoch 7\n",
      "\n",
      "Epoch performance:  13.084sec Trl:3.017458 \n",
      "\tTrem:0.048889 \tTrf1:0.175688 \tTeem:0.033613 \tTef1:0.120715\n",
      "\n",
      "Epoch:  8 / 300\n",
      "Batch:\t0 /9:  1.281572 1.281592 \tl:3.303928 \tem:0.080000 f1:0.171527\n",
      "Batch:\t1 /9:  1.259906 2.542083 \tl:2.482571 \tem:0.100000 f1:0.208351\n",
      "Batch:\t2 /9:  1.069629 3.612488 \tl:2.812513 \tem:0.100000 f1:0.160734\n",
      "Batch:\t3 /9:  1.225507 4.838995 \tl:2.499145 \tem:0.180000 f1:0.262362\n",
      "Batch:\t4 /9:  1.565206 6.404930 \tl:2.845545 \tem:0.080000 f1:0.229312\n",
      "Batch:\t5 /9:  1.471748 7.878181 \tl:3.467854 \tem:0.160000 f1:0.321253\n",
      "Batch:\t6 /9:  1.405533 9.283906 \tl:2.644260 \tem:0.220000 f1:0.322797\n",
      "Batch:\t7 /9:  1.454808 10.73932 \tl:2.492959 \tem:0.140000 f1:0.230108\n",
      "Batch:\t8 /9:  1.217096 11.95757 \tl:2.177128 \tem:0.260000 f1:0.356971\n",
      "Saving new model on epoch 8\n",
      "\n",
      "Epoch performance:  12.665sec Trl:2.747323 \n",
      "\tTrem:0.146667 \tTrf1:0.251491 \tTeem:0.058824 \tTef1:0.191309\n",
      "\n",
      "Epoch:  9 / 300\n",
      "Batch:\t0 /9:  1.271961 1.271979 \tl:2.294315 \tem:0.200000 f1:0.353642\n",
      "Batch:\t1 /9:  1.124774 2.397724 \tl:2.050751 \tem:0.120000 f1:0.382388\n",
      "Batch:\t2 /9:  1.211773 3.610702 \tl:2.511683 \tem:0.360000 f1:0.496541\n",
      "Batch:\t3 /9:  1.155074 4.765985 \tl:1.972519 \tem:0.320000 f1:0.371370\n",
      "Batch:\t4 /9:  1.135324 5.902671 \tl:1.800097 \tem:0.360000 f1:0.489440\n",
      "Batch:\t5 /9:  1.182587 7.086015 \tl:2.132018 \tem:0.340000 f1:0.426365\n",
      "Batch:\t6 /9:  1.245589 8.332983 \tl:1.768562 \tem:0.500000 f1:0.569015\n",
      "Batch:\t7 /9:  1.354736 9.688862 \tl:2.019816 \tem:0.380000 f1:0.477541\n",
      "Batch:\t8 /9:  1.323990 11.01390 \tl:1.843562 \tem:0.540000 f1:0.654389\n",
      "Saving new model on epoch 9\n",
      "\n",
      "Epoch performance:  11.731sec Trl:2.043702 \n",
      "\tTrem:0.346667 \tTrf1:0.468966 \tTeem:0.075630 \tTef1:0.236439\n",
      "\n",
      "Epoch:  10 / 300\n",
      "Batch:\t0 /9:  1.256087 1.256105 \tl:1.667047 \tem:0.540000 f1:0.573621\n",
      "Batch:\t1 /9:  1.127738 2.384984 \tl:1.416665 \tem:0.560000 f1:0.623910\n",
      "Batch:\t2 /9:  1.153850 3.539623 \tl:1.670660 \tem:0.600000 f1:0.710252\n",
      "Batch:\t3 /9:  1.150828 4.691786 \tl:1.722521 \tem:0.540000 f1:0.634315\n",
      "Batch:\t4 /9:  1.161504 5.854600 \tl:1.389904 \tem:0.600000 f1:0.733076\n",
      "Batch:\t5 /9:  1.253136 7.108928 \tl:1.335605 \tem:0.640000 f1:0.690954\n",
      "Batch:\t6 /9:  1.212139 8.321760 \tl:1.022390 \tem:0.680000 f1:0.726989\n",
      "Batch:\t7 /9:  1.297857 9.620565 \tl:1.285106 \tem:0.740000 f1:0.776795\n",
      "Batch:\t8 /9:  1.137331 10.75869 \tl:0.998260 \tem:0.700000 f1:0.733164\n",
      "Saving new model on epoch 10\n",
      "\n",
      "Epoch performance:  12.882sec Trl:1.389795 \n",
      "\tTrem:0.622222 \tTrf1:0.689231 \tTeem:0.084034 \tTef1:0.268612\n",
      "\n",
      "Epoch:  11 / 300\n",
      "Batch:\t0 /9:  1.269738 1.269757 \tl:1.201399 \tem:0.600000 f1:0.661964\n",
      "Batch:\t1 /9:  1.174472 2.444909 \tl:1.180743 \tem:0.640000 f1:0.725378\n",
      "Batch:\t2 /9:  1.268784 3.714996 \tl:1.676206 \tem:0.720000 f1:0.792428\n",
      "Batch:\t3 /9:  1.136668 4.853170 \tl:0.634761 \tem:0.820000 f1:0.864257\n",
      "Batch:\t4 /9:  1.238093 6.092945 \tl:1.085299 \tem:0.700000 f1:0.776064\n",
      "Batch:\t5 /9:  1.195088 7.288814 \tl:0.827525 \tem:0.800000 f1:0.844379\n",
      "Batch:\t6 /9:  1.179268 8.468786 \tl:0.895803 \tem:0.740000 f1:0.787039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t7 /9:  1.160017 9.629680 \tl:0.976774 \tem:0.760000 f1:0.785949\n",
      "Batch:\t8 /9:  1.237483 10.86792 \tl:0.890497 \tem:0.760000 f1:0.811693\n",
      "Saving new model on epoch 11\n",
      "\n",
      "Epoch performance:  11.646sec Trl:1.041001 \n",
      "\tTrem:0.726667 \tTrf1:0.783239 \tTeem:0.092437 \tTef1:0.292074\n",
      "\n",
      "Epoch:  12 / 300\n",
      "Batch:\t0 /9:  1.439039 1.439054 \tl:1.075382 \tem:0.740000 f1:0.752988\n",
      "Batch:\t1 /9:  1.220315 2.659783 \tl:0.784673 \tem:0.860000 f1:0.935244\n",
      "Batch:\t2 /9:  1.297039 3.957473 \tl:0.766506 \tem:0.840000 f1:0.872510\n",
      "Batch:\t3 /9:  1.184334 5.142501 \tl:0.914566 \tem:0.860000 f1:0.916819\n",
      "Batch:\t4 /9:  1.209707 6.352882 \tl:0.516560 \tem:0.900000 f1:0.934430\n",
      "Batch:\t5 /9:  1.119240 7.498244 \tl:0.844877 \tem:0.800000 f1:0.841735\n",
      "Batch:\t6 /9:  1.196823 8.696198 \tl:0.864636 \tem:0.820000 f1:0.870373\n",
      "Batch:\t7 /9:  1.270387 9.967953 \tl:0.431928 \tem:0.920000 f1:0.924444\n",
      "Batch:\t8 /9:  1.170415 11.13890 \tl:0.722592 \tem:0.820000 f1:0.847605\n",
      "\n",
      "Epoch performance:  11.342sec Trl:0.769080 \n",
      "\tTrem:0.840000 \tTrf1:0.877350 \tTeem:0.092437 \tTef1:0.292012\n",
      "\n",
      "Epoch:  13 / 300\n",
      "Batch:\t0 /9:  1.223412 1.223428 \tl:0.449044 \tem:0.860000 f1:0.889810\n",
      "Batch:\t1 /9:  1.310291 2.535222 \tl:1.009654 \tem:0.840000 f1:0.909133\n",
      "Batch:\t2 /9:  1.287199 3.823342 \tl:0.410683 \tem:0.880000 f1:0.897910\n",
      "Batch:\t3 /9:  1.177412 5.001382 \tl:0.414663 \tem:0.920000 f1:0.942467\n",
      "Batch:\t4 /9:  1.333172 6.335519 \tl:0.966952 \tem:0.740000 f1:0.796942\n",
      "Batch:\t5 /9:  1.181938 7.517684 \tl:0.532360 \tem:0.860000 f1:0.884895\n",
      "Batch:\t6 /9:  1.163206 8.681838 \tl:0.243721 \tem:0.980000 f1:0.985000\n",
      "Batch:\t7 /9:  1.223651 9.906991 \tl:0.649475 \tem:0.900000 f1:0.936951\n",
      "Batch:\t8 /9:  1.214455 11.12174 \tl:0.325840 \tem:0.900000 f1:0.907963\n",
      "Saving new model on epoch 13\n",
      "\n",
      "Epoch performance:  11.850sec Trl:0.555821 \n",
      "\tTrem:0.875556 \tTrf1:0.905675 \tTeem:0.084034 \tTef1:0.302804\n",
      "\n",
      "Epoch:  14 / 300\n",
      "Batch:\t0 /9:  1.301954 1.301970 \tl:0.575668 \tem:0.860000 f1:0.912949\n",
      "Batch:\t1 /9:  1.424905 2.727765 \tl:0.545672 \tem:0.880000 f1:0.907344\n",
      "Batch:\t2 /9:  1.291533 4.020298 \tl:0.623279 \tem:0.860000 f1:0.891740\n",
      "Batch:\t3 /9:  1.294862 5.316636 \tl:0.320337 \tem:0.900000 f1:0.921235\n",
      "Batch:\t4 /9:  1.149698 6.467791 \tl:0.280039 \tem:0.920000 f1:0.928810\n",
      "Batch:\t5 /9:  1.262950 7.731773 \tl:0.219834 \tem:0.980000 f1:0.980909\n",
      "Batch:\t6 /9:  1.189471 8.922384 \tl:0.243031 \tem:0.920000 f1:0.956388\n",
      "Batch:\t7 /9:  1.227122 10.15104 \tl:0.144831 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.235934 11.38781 \tl:0.425709 \tem:0.920000 f1:0.938433\n",
      "Saving new model on epoch 14\n",
      "\n",
      "Epoch performance:  12.385sec Trl:0.375378 \n",
      "\tTrem:0.915556 \tTrf1:0.937534 \tTeem:0.109244 \tTef1:0.318445\n",
      "\n",
      "Epoch:  15 / 300\n",
      "Batch:\t0 /9:  1.340662 1.340681 \tl:0.359002 \tem:0.940000 f1:0.968889\n",
      "Batch:\t1 /9:  1.304682 2.646802 \tl:0.134616 \tem:0.980000 f1:0.993333\n",
      "Batch:\t2 /9:  1.318109 3.966087 \tl:0.133388 \tem:0.960000 f1:0.962857\n",
      "Batch:\t3 /9:  1.312227 5.279271 \tl:0.092484 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.230340 6.510665 \tl:0.154731 \tem:0.980000 f1:0.981290\n",
      "Batch:\t5 /9:  1.208381 7.720175 \tl:0.383046 \tem:0.880000 f1:0.963030\n",
      "Batch:\t6 /9:  1.190137 8.910670 \tl:0.099993 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.194176 10.10599 \tl:0.452833 \tem:0.940000 f1:0.953333\n",
      "Batch:\t8 /9:  1.078414 11.18550 \tl:0.112524 \tem:0.960000 f1:0.976970\n",
      "\n",
      "Epoch performance:  11.439sec Trl:0.213624 \n",
      "\tTrem:0.960000 \tTrf1:0.977745 \tTeem:0.092437 \tTef1:0.265581\n",
      "\n",
      "Epoch:  16 / 300\n",
      "Batch:\t0 /9:  1.544276 1.544291 \tl:0.127515 \tem:0.960000 f1:0.962212\n",
      "Batch:\t1 /9:  1.524765 3.070178 \tl:0.040389 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.264339 4.335692 \tl:0.334884 \tem:0.960000 f1:0.961143\n",
      "Batch:\t3 /9:  1.154254 5.490440 \tl:0.271078 \tem:0.960000 f1:0.961143\n",
      "Batch:\t4 /9:  1.300403 6.791365 \tl:0.042468 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.254045 8.046382 \tl:0.201591 \tem:0.940000 f1:0.945333\n",
      "Batch:\t6 /9:  1.166999 9.214091 \tl:0.172329 \tem:0.940000 f1:0.952080\n",
      "Batch:\t7 /9:  1.199748 10.41488 \tl:0.111396 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.141083 11.55699 \tl:0.205022 \tem:0.940000 f1:0.959311\n",
      "\n",
      "Epoch performance:  11.734sec Trl:0.167408 \n",
      "\tTrem:0.966667 \tTrf1:0.971247 \tTeem:0.092437 \tTef1:0.275932\n",
      "\n",
      "Epoch:  17 / 300\n",
      "Batch:\t0 /9:  1.089185 1.089200 \tl:0.149267 \tem:0.940000 f1:0.960017\n",
      "Batch:\t1 /9:  1.279164 2.369101 \tl:0.214567 \tem:0.960000 f1:0.965333\n",
      "Batch:\t2 /9:  1.120001 3.490179 \tl:0.030063 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.157193 4.648415 \tl:0.090697 \tem:0.960000 f1:0.979291\n",
      "Batch:\t4 /9:  1.233236 5.882669 \tl:0.195846 \tem:0.980000 f1:0.985333\n",
      "Batch:\t5 /9:  1.226867 7.110009 \tl:0.041928 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.147461 8.258200 \tl:0.038668 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.543293 9.802309 \tl:0.175916 \tem:0.940000 f1:0.940000\n",
      "Batch:\t8 /9:  1.576872 11.38040 \tl:0.071067 \tem:0.980000 f1:0.985333\n",
      "\n",
      "Epoch performance:  11.586sec Trl:0.112002 \n",
      "\tTrem:0.973333 \tTrf1:0.979479 \tTeem:0.075630 \tTef1:0.277585\n",
      "\n",
      "Epoch:  18 / 300\n",
      "Batch:\t0 /9:  1.213961 1.213974 \tl:0.095756 \tem:0.980000 f1:0.980909\n",
      "Batch:\t1 /9:  1.255767 2.470596 \tl:0.030587 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.254372 3.725856 \tl:0.088138 \tem:0.980000 f1:0.980000\n",
      "Batch:\t3 /9:  1.404989 5.131513 \tl:0.045241 \tem:0.980000 f1:0.995758\n",
      "Batch:\t4 /9:  1.317313 6.449606 \tl:0.037454 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.272153 7.723473 \tl:0.024352 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.322246 9.047136 \tl:0.030765 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.227717 10.27572 \tl:0.032970 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.232309 11.50922 \tl:0.021087 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.721sec Trl:0.045150 \n",
      "\tTrem:0.993333 \tTrf1:0.995185 \tTeem:0.092437 \tTef1:0.272373\n",
      "\n",
      "Epoch:  19 / 300\n",
      "Batch:\t0 /9:  1.295010 1.295026 \tl:0.110273 \tem:0.980000 f1:0.986792\n",
      "Batch:\t1 /9:  1.259850 2.555504 \tl:0.039399 \tem:0.980000 f1:0.992632\n",
      "Batch:\t2 /9:  1.276806 3.833886 \tl:0.034137 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.225075 5.059674 \tl:0.020230 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.255000 6.315279 \tl:0.010026 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.234340 7.550106 \tl:0.018219 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.159481 8.710415 \tl:0.023019 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.193621 9.905025 \tl:0.010125 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.229156 11.13475 \tl:0.130520 \tem:0.960000 f1:0.965333\n",
      "\n",
      "Epoch performance:  11.342sec Trl:0.043994 \n",
      "\tTrem:0.991111 \tTrf1:0.993862 \tTeem:0.117647 \tTef1:0.308311\n",
      "\n",
      "Epoch:  20 / 300\n",
      "Batch:\t0 /9:  1.240992 1.241003 \tl:0.065983 \tem:0.980000 f1:0.985333\n",
      "Batch:\t1 /9:  1.187152 2.428678 \tl:0.012878 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.199177 3.629323 \tl:0.015795 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.209291 4.839227 \tl:0.012733 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.237863 6.078537 \tl:0.013391 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.153959 7.233036 \tl:0.009479 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.163398 8.398181 \tl:0.026318 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.231045 9.629963 \tl:0.102849 \tem:0.980000 f1:0.992000\n",
      "Batch:\t8 /9:  1.175785 10.80626 \tl:0.016569 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.995sec Trl:0.030666 \n",
      "\tTrem:0.995556 \tTrf1:0.997481 \tTeem:0.109244 \tTef1:0.307688\n",
      "\n",
      "Epoch:  21 / 300\n",
      "Batch:\t0 /9:  1.187003 1.187016 \tl:0.011888 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.310292 2.497871 \tl:0.007836 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.156774 3.655311 \tl:0.039286 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.136003 4.792318 \tl:0.111958 \tem:0.980000 f1:0.993333\n",
      "Batch:\t4 /9:  1.131412 5.924314 \tl:0.008845 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.161926 7.086769 \tl:0.011676 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.178173 8.265621 \tl:0.008750 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.279481 9.545735 \tl:0.174549 \tem:0.980000 f1:0.991980\n",
      "Batch:\t8 /9:  1.333408 10.87983 \tl:0.008263 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.052sec Trl:0.042561 \n",
      "\tTrem:0.995556 \tTrf1:0.998368 \tTeem:0.092437 \tTef1:0.266673\n",
      "\n",
      "Epoch:  22 / 300\n",
      "Batch:\t0 /9:  1.139122 1.139137 \tl:0.054200 \tem:0.980000 f1:0.980000\n",
      "Batch:\t1 /9:  1.608217 2.748244 \tl:0.009329 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.299126 4.048283 \tl:0.009082 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.289261 5.338205 \tl:0.015588 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.318453 6.657729 \tl:0.015021 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.200804 7.859608 \tl:0.009725 \tem:1.000000 f1:1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t6 /9:  1.193134 9.053940 \tl:0.010506 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.162252 10.21734 \tl:0.020839 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.354710 11.57303 \tl:0.009810 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.789sec Trl:0.017122 \n",
      "\tTrem:0.997778 \tTrf1:0.997778 \tTeem:0.100840 \tTef1:0.292428\n",
      "\n",
      "Epoch:  23 / 300\n",
      "Batch:\t0 /9:  1.269027 1.269039 \tl:0.009776 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.325109 2.594794 \tl:0.019504 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.476447 4.072571 \tl:0.010317 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.153733 5.227545 \tl:0.006825 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.203249 6.431032 \tl:0.012649 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.316029 7.748286 \tl:0.007770 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.237868 8.987528 \tl:0.006760 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.234491 10.22267 \tl:0.079827 \tem:0.980000 f1:0.980225\n",
      "Batch:\t8 /9:  1.233981 11.45728 \tl:0.006057 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.626sec Trl:0.017721 \n",
      "\tTrem:0.997778 \tTrf1:0.997803 \tTeem:0.100840 \tTef1:0.268722\n",
      "\n",
      "Epoch:  24 / 300\n",
      "Batch:\t0 /9:  1.255133 1.255152 \tl:0.010242 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.199421 2.455594 \tl:0.104273 \tem:0.960000 f1:0.976889\n",
      "Batch:\t2 /9:  1.214257 3.671597 \tl:0.005738 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.159196 4.832088 \tl:0.005410 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.111109 5.943773 \tl:0.018015 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.140984 7.086107 \tl:0.009103 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.147665 8.234291 \tl:0.009183 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.260327 9.495324 \tl:0.011305 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.164861 10.66148 \tl:0.015757 \tem:1.000000 f1:1.000000\n",
      "Saving new model on epoch 24\n",
      "\n",
      "Epoch performance:  11.442sec Trl:0.021003 \n",
      "\tTrem:0.995556 \tTrf1:0.997432 \tTeem:0.134454 \tTef1:0.321149\n",
      "\n",
      "Epoch:  25 / 300\n",
      "Batch:\t0 /9:  1.341072 1.341093 \tl:0.037039 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.141278 2.483675 \tl:0.066048 \tem:0.960000 f1:0.960000\n",
      "Batch:\t2 /9:  1.283990 3.768447 \tl:0.008634 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.168076 4.937530 \tl:0.009295 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.234251 6.172780 \tl:0.011404 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.227424 7.400718 \tl:0.013212 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.156337 8.557747 \tl:0.010321 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.254055 9.812716 \tl:0.093534 \tem:0.980000 f1:0.980000\n",
      "Batch:\t8 /9:  1.259519 11.07306 \tl:0.016913 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.278sec Trl:0.029600 \n",
      "\tTrem:0.993333 \tTrf1:0.993333 \tTeem:0.109244 \tTef1:0.317520\n",
      "\n",
      "Epoch:  26 / 300\n",
      "Batch:\t0 /9:  1.225959 1.225975 \tl:0.012349 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.248778 2.475877 \tl:0.004968 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.204137 3.680701 \tl:0.087126 \tem:0.980000 f1:0.980000\n",
      "Batch:\t3 /9:  1.128551 4.810360 \tl:0.065470 \tem:0.960000 f1:0.970000\n",
      "Batch:\t4 /9:  1.132665 5.943650 \tl:0.435654 \tem:0.940000 f1:0.941702\n",
      "Batch:\t5 /9:  1.512176 7.456603 \tl:0.013899 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.586951 9.045011 \tl:0.022008 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.529442 10.57557 \tl:0.525800 \tem:0.920000 f1:0.920455\n",
      "Batch:\t8 /9:  1.298338 11.87482 \tl:0.736439 \tem:0.860000 f1:0.897299\n",
      "\n",
      "Epoch performance:  12.076sec Trl:0.211524 \n",
      "\tTrem:0.962222 \tTrf1:0.967717 \tTeem:0.084034 \tTef1:0.276503\n",
      "\n",
      "Epoch:  27 / 300\n",
      "Batch:\t0 /9:  1.296704 1.296737 \tl:1.229771 \tem:0.840000 f1:0.843020\n",
      "Batch:\t1 /9:  1.180918 2.479653 \tl:0.388801 \tem:0.940000 f1:0.953333\n",
      "Batch:\t2 /9:  1.197932 3.677980 \tl:0.451413 \tem:0.900000 f1:0.939741\n",
      "Batch:\t3 /9:  1.168434 4.847715 \tl:0.621100 \tem:0.780000 f1:0.812279\n",
      "Batch:\t4 /9:  1.188821 6.037939 \tl:0.382346 \tem:0.900000 f1:0.918182\n",
      "Batch:\t5 /9:  1.162634 7.201743 \tl:0.447154 \tem:0.860000 f1:0.899504\n",
      "Batch:\t6 /9:  1.216588 8.419025 \tl:0.172787 \tem:0.960000 f1:0.964000\n",
      "Batch:\t7 /9:  1.273345 9.693490 \tl:0.187026 \tem:0.960000 f1:0.960000\n",
      "Batch:\t8 /9:  1.249467 10.94402 \tl:0.242688 \tem:0.940000 f1:0.947480\n",
      "\n",
      "Epoch performance:  11.151sec Trl:0.458121 \n",
      "\tTrem:0.897778 \tTrf1:0.915282 \tTeem:0.058824 \tTef1:0.247813\n",
      "\n",
      "Epoch:  28 / 300\n",
      "Batch:\t0 /9:  1.278566 1.278580 \tl:0.483541 \tem:0.900000 f1:0.958222\n",
      "Batch:\t1 /9:  1.150335 2.429826 \tl:0.194580 \tem:0.980000 f1:0.980000\n",
      "Batch:\t2 /9:  1.414854 3.845925 \tl:0.217307 \tem:0.940000 f1:0.969993\n",
      "Batch:\t3 /9:  1.289432 5.136360 \tl:0.197237 \tem:0.960000 f1:0.977374\n",
      "Batch:\t4 /9:  1.171849 6.309992 \tl:0.657013 \tem:0.940000 f1:0.948000\n",
      "Batch:\t5 /9:  1.189197 7.500422 \tl:0.180138 \tem:0.940000 f1:0.965740\n",
      "Batch:\t6 /9:  1.187294 8.688564 \tl:0.274581 \tem:0.960000 f1:0.976449\n",
      "Batch:\t7 /9:  1.241842 9.931238 \tl:0.496165 \tem:0.860000 f1:0.903195\n",
      "Batch:\t8 /9:  1.259624 11.19222 \tl:0.487585 \tem:0.920000 f1:0.938130\n",
      "\n",
      "Epoch performance:  11.395sec Trl:0.354238 \n",
      "\tTrem:0.933333 \tTrf1:0.957456 \tTeem:0.075630 \tTef1:0.268307\n",
      "\n",
      "Epoch:  29 / 300\n",
      "Batch:\t0 /9:  1.290393 1.290414 \tl:0.132502 \tem:0.980000 f1:0.982500\n",
      "Batch:\t1 /9:  1.281037 2.572683 \tl:0.216089 \tem:0.980000 f1:0.980000\n",
      "Batch:\t2 /9:  1.191875 3.765304 \tl:0.519170 \tem:0.860000 f1:0.899080\n",
      "Batch:\t3 /9:  1.600179 5.366732 \tl:0.081068 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.238328 6.606064 \tl:0.272484 \tem:0.940000 f1:0.956000\n",
      "Batch:\t5 /9:  1.229400 7.836987 \tl:0.391167 \tem:0.920000 f1:0.927784\n",
      "Batch:\t6 /9:  1.279206 9.116667 \tl:0.115323 \tem:0.960000 f1:0.964706\n",
      "Batch:\t7 /9:  1.286427 10.40360 \tl:0.127221 \tem:0.980000 f1:0.980000\n",
      "Batch:\t8 /9:  1.281168 11.68593 \tl:0.145128 \tem:0.960000 f1:0.968000\n",
      "\n",
      "Epoch performance:  11.893sec Trl:0.222239 \n",
      "\tTrem:0.953333 \tTrf1:0.962008 \tTeem:0.092437 \tTef1:0.278220\n",
      "\n",
      "Epoch:  30 / 300\n",
      "Batch:\t0 /9:  1.295913 1.295931 \tl:0.239237 \tem:0.920000 f1:0.931062\n",
      "Batch:\t1 /9:  1.312282 2.609827 \tl:0.087611 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.271173 3.881860 \tl:0.508959 \tem:0.920000 f1:0.948571\n",
      "Batch:\t3 /9:  1.285492 5.167634 \tl:0.467708 \tem:0.920000 f1:0.924000\n",
      "Batch:\t4 /9:  1.170552 6.339035 \tl:0.061355 \tem:0.980000 f1:0.980000\n",
      "Batch:\t5 /9:  1.245733 7.585923 \tl:0.037447 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.170955 8.757513 \tl:0.115174 \tem:0.980000 f1:0.980000\n",
      "Batch:\t7 /9:  1.103903 9.862514 \tl:0.382442 \tem:0.940000 f1:0.955362\n",
      "Batch:\t8 /9:  1.141116 11.00456 \tl:0.165168 \tem:0.960000 f1:0.975636\n",
      "\n",
      "Epoch performance:  11.164sec Trl:0.229455 \n",
      "\tTrem:0.957778 \tTrf1:0.966070 \tTeem:0.075630 \tTef1:0.293970\n",
      "\n",
      "Epoch:  31 / 300\n",
      "Batch:\t0 /9:  1.097600 1.097619 \tl:0.256373 \tem:0.960000 f1:0.960000\n",
      "Batch:\t1 /9:  1.181758 2.281113 \tl:0.158830 \tem:0.940000 f1:0.965730\n",
      "Batch:\t2 /9:  1.153836 3.435904 \tl:0.065396 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.309945 4.746864 \tl:0.068844 \tem:0.980000 f1:0.984286\n",
      "Batch:\t4 /9:  1.174098 5.921884 \tl:0.310289 \tem:0.960000 f1:0.967123\n",
      "Batch:\t5 /9:  1.167289 7.089864 \tl:0.097869 \tem:0.980000 f1:0.987395\n",
      "Batch:\t6 /9:  1.392719 8.483550 \tl:0.119137 \tem:0.960000 f1:0.968258\n",
      "Batch:\t7 /9:  1.566102 10.05036 \tl:0.112964 \tem:0.960000 f1:0.960000\n",
      "Batch:\t8 /9:  1.437681 11.48884 \tl:0.461903 \tem:0.960000 f1:0.960000\n",
      "\n",
      "Epoch performance:  11.692sec Trl:0.183512 \n",
      "\tTrem:0.966667 \tTrf1:0.972532 \tTeem:0.075630 \tTef1:0.291535\n",
      "\n",
      "Epoch:  32 / 300\n",
      "Batch:\t0 /9:  1.534808 1.534821 \tl:0.128101 \tem:0.980000 f1:0.984068\n",
      "Batch:\t1 /9:  1.462427 2.997862 \tl:0.042034 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.600127 4.599411 \tl:0.229340 \tem:0.940000 f1:0.953029\n",
      "Batch:\t3 /9:  1.450102 6.050431 \tl:0.115160 \tem:0.980000 f1:0.984000\n",
      "Batch:\t4 /9:  1.285036 7.336080 \tl:0.101318 \tem:0.980000 f1:0.995385\n",
      "Batch:\t5 /9:  1.191904 8.528686 \tl:0.198439 \tem:0.960000 f1:0.970351\n",
      "Batch:\t6 /9:  1.153591 9.683643 \tl:0.075115 \tem:0.960000 f1:0.960000\n",
      "Batch:\t7 /9:  1.223141 10.90710 \tl:0.103723 \tem:0.960000 f1:0.961951\n",
      "Batch:\t8 /9:  1.141685 12.05032 \tl:0.064589 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  12.256sec Trl:0.117536 \n",
      "\tTrem:0.973333 \tTrf1:0.978754 \tTeem:0.075630 \tTef1:0.307303\n",
      "\n",
      "Epoch:  33 / 300\n",
      "Batch:\t0 /9:  1.523855 1.523869 \tl:0.127260 \tem:0.980000 f1:0.980000\n",
      "Batch:\t1 /9:  1.317023 2.841420 \tl:0.106190 \tem:0.980000 f1:0.980000\n",
      "Batch:\t2 /9:  1.275817 4.118081 \tl:0.083263 \tem:0.960000 f1:0.960000\n",
      "Batch:\t3 /9:  1.247612 5.366592 \tl:0.061024 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.172576 6.539427 \tl:0.057452 \tem:0.980000 f1:0.984444\n",
      "Batch:\t5 /9:  1.159739 7.701267 \tl:0.026711 \tem:1.000000 f1:1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t6 /9:  1.223807 8.926492 \tl:0.017450 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.250000 10.17778 \tl:0.020975 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.595023 11.77390 \tl:0.110804 \tem:0.960000 f1:0.964404\n",
      "Saving new model on epoch 33\n",
      "\n",
      "Epoch performance:  13.843sec Trl:0.067903 \n",
      "\tTrem:0.984444 \tTrf1:0.985428 \tTeem:0.126050 \tTef1:0.335288\n",
      "\n",
      "Epoch:  34 / 300\n",
      "Batch:\t0 /9:  1.373208 1.373249 \tl:0.011386 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.194761 2.568728 \tl:0.034169 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.202844 3.772588 \tl:0.030854 \tem:0.980000 f1:0.981600\n",
      "Batch:\t3 /9:  1.092497 4.866091 \tl:0.096535 \tem:0.940000 f1:0.964494\n",
      "Batch:\t4 /9:  1.342598 6.209288 \tl:0.013233 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.293123 7.503676 \tl:0.150178 \tem:0.940000 f1:0.962798\n",
      "Batch:\t6 /9:  1.165389 8.669394 \tl:0.012555 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.199090 9.869915 \tl:0.104622 \tem:0.960000 f1:0.971014\n",
      "Batch:\t8 /9:  1.157848 11.02824 \tl:0.047266 \tem:0.980000 f1:0.996000\n",
      "\n",
      "Epoch performance:  11.205sec Trl:0.055644 \n",
      "\tTrem:0.977778 \tTrf1:0.986212 \tTeem:0.117647 \tTef1:0.314158\n",
      "\n",
      "Epoch:  35 / 300\n",
      "Batch:\t0 /9:  1.261678 1.261691 \tl:0.033183 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.298009 2.560160 \tl:0.080411 \tem:0.980000 f1:0.993333\n",
      "Batch:\t2 /9:  1.171731 3.732406 \tl:0.006662 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.166514 4.899436 \tl:0.039227 \tem:0.980000 f1:0.980000\n",
      "Batch:\t4 /9:  1.277112 6.177786 \tl:0.018775 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.265152 7.444127 \tl:0.017866 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.245526 8.690898 \tl:0.014453 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.151326 9.843106 \tl:0.017497 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.219938 11.06426 \tl:0.009960 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.266sec Trl:0.026448 \n",
      "\tTrem:0.995556 \tTrf1:0.997037 \tTeem:0.075630 \tTef1:0.270549\n",
      "\n",
      "Epoch:  36 / 300\n",
      "Batch:\t0 /9:  1.224367 1.224380 \tl:0.184320 \tem:0.960000 f1:0.960000\n",
      "Batch:\t1 /9:  1.502578 2.727479 \tl:0.010502 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.280217 4.008590 \tl:0.019660 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.180907 5.190612 \tl:0.014353 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.243829 6.435142 \tl:0.009526 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.335566 7.771644 \tl:0.017889 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.240492 9.012745 \tl:0.005995 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.175483 10.18974 \tl:0.017900 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.232374 11.42292 \tl:0.280386 \tem:0.960000 f1:0.964706\n",
      "\n",
      "Epoch performance:  11.626sec Trl:0.062281 \n",
      "\tTrem:0.991111 \tTrf1:0.991634 \tTeem:0.092437 \tTef1:0.309029\n",
      "\n",
      "Epoch:  37 / 300\n",
      "Batch:\t0 /9:  1.237340 1.237360 \tl:0.007286 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.197237 2.435095 \tl:0.007514 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.198775 3.635185 \tl:0.009934 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.237606 4.873759 \tl:0.018079 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.174129 6.048543 \tl:0.021428 \tem:0.980000 f1:0.980000\n",
      "Batch:\t5 /9:  1.317600 7.367254 \tl:0.008727 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.207341 8.575200 \tl:0.007815 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.272213 9.866126 \tl:0.006758 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.165914 11.03292 \tl:0.011136 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.237sec Trl:0.010964 \n",
      "\tTrem:0.997778 \tTrf1:0.997778 \tTeem:0.100840 \tTef1:0.322914\n",
      "\n",
      "Epoch:  38 / 300\n",
      "Batch:\t0 /9:  1.356912 1.356926 \tl:0.005180 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.266587 2.624843 \tl:0.007341 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.182727 3.808614 \tl:0.006356 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.182159 4.991786 \tl:0.005672 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.258210 6.250990 \tl:0.004219 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.153394 7.405323 \tl:0.012529 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.175361 8.581661 \tl:0.005070 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.177139 9.760061 \tl:0.004213 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.167607 10.92870 \tl:0.005638 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.119sec Trl:0.006247 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.318509\n",
      "\n",
      "Epoch:  39 / 300\n",
      "Batch:\t0 /9:  1.270421 1.270437 \tl:0.005954 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.266206 2.537714 \tl:0.004197 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.280771 3.819097 \tl:0.006917 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.134278 4.953959 \tl:0.004079 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.149853 6.104619 \tl:0.003964 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.140341 7.245253 \tl:0.004193 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.260532 8.506418 \tl:0.003456 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.148074 9.654974 \tl:0.004559 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.304368 10.96018 \tl:0.002992 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.149sec Trl:0.004479 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.092437 \tTef1:0.312720\n",
      "\n",
      "Epoch:  40 / 300\n",
      "Batch:\t0 /9:  1.281591 1.281605 \tl:0.004735 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.263468 2.545917 \tl:0.003040 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.239502 3.786022 \tl:0.003759 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.205061 4.991675 \tl:0.003797 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.172034 6.165266 \tl:0.002588 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.243850 7.410168 \tl:0.003139 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.173860 8.585304 \tl:0.003887 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.154729 9.740736 \tl:0.002840 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.185122 10.92682 \tl:0.002326 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.124sec Trl:0.003345 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313654\n",
      "\n",
      "Epoch:  41 / 300\n",
      "Batch:\t0 /9:  1.138257 1.138270 \tl:0.002611 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.171542 2.311004 \tl:0.002812 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.194659 3.506426 \tl:0.002536 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.224176 4.732563 \tl:0.002077 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.304951 6.038865 \tl:0.001948 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.321326 7.361344 \tl:0.002012 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.326592 8.688284 \tl:0.003047 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.199965 9.888883 \tl:0.002622 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.230877 11.12052 \tl:0.002183 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.324sec Trl:0.002427 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313654\n",
      "\n",
      "Epoch:  42 / 300\n",
      "Batch:\t0 /9:  1.268795 1.268810 \tl:0.001896 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.367379 2.636969 \tl:0.001541 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.289799 3.927926 \tl:0.002247 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.213876 5.142646 \tl:0.001977 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.226320 6.369435 \tl:0.001681 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.154842 7.525353 \tl:0.001387 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.184832 8.711384 \tl:0.002233 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.165740 9.877789 \tl:0.002097 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.153439 11.03184 \tl:0.002116 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.234sec Trl:0.001908 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313731\n",
      "\n",
      "Epoch:  43 / 300\n",
      "Batch:\t0 /9:  1.268172 1.268187 \tl:0.001935 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.205610 2.474552 \tl:0.002526 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.172224 3.647365 \tl:0.001508 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.151899 4.800507 \tl:0.001626 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.246988 6.048176 \tl:0.001590 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.144756 7.193912 \tl:0.001566 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.179600 8.374738 \tl:0.001329 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.157135 9.532584 \tl:0.001246 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.222655 10.75624 \tl:0.001440 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.959sec Trl:0.001641 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313731\n",
      "\n",
      "Epoch:  44 / 300\n",
      "Batch:\t0 /9:  1.202688 1.202702 \tl:0.001234 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.248049 2.452244 \tl:0.001114 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.263605 3.716971 \tl:0.002155 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.253001 4.970668 \tl:0.001042 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.523412 6.495471 \tl:0.001070 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.443861 7.940712 \tl:0.001458 \tem:1.000000 f1:1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t6 /9:  1.232541 9.174401 \tl:0.001416 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.172571 10.34818 \tl:0.001668 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.326699 11.67628 \tl:0.001284 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.929sec Trl:0.001382 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313731\n",
      "\n",
      "Epoch:  45 / 300\n",
      "Batch:\t0 /9:  1.391079 1.391093 \tl:0.001572 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.229897 2.621268 \tl:0.001149 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.128230 3.749981 \tl:0.001713 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.299942 5.050909 \tl:0.001394 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.163738 6.214955 \tl:0.001704 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.275470 7.490927 \tl:0.001428 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.254942 8.747380 \tl:0.002264 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.131628 9.879651 \tl:0.001833 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.207818 11.08820 \tl:0.001839 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.276sec Trl:0.001655 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313731\n",
      "\n",
      "Epoch:  46 / 300\n",
      "Batch:\t0 /9:  1.311071 1.311085 \tl:0.001312 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.151933 2.463718 \tl:0.001442 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.163962 3.628602 \tl:0.001496 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.145703 4.775556 \tl:0.001138 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.111171 5.887568 \tl:0.002105 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.147789 7.035910 \tl:0.001987 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.251133 8.287758 \tl:0.001329 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.157899 9.446143 \tl:0.001220 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.261370 10.70929 \tl:0.001494 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.916sec Trl:0.001503 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  47 / 300\n",
      "Batch:\t0 /9:  1.268977 1.268993 \tl:0.001129 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.248509 2.518867 \tl:0.000966 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.207938 3.727432 \tl:0.001166 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.267612 4.995815 \tl:0.000936 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.132009 6.129341 \tl:0.001099 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.142361 7.272252 \tl:0.001003 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.197988 8.471313 \tl:0.001531 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.068243 9.540360 \tl:0.000784 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.143687 10.68485 \tl:0.001051 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.893sec Trl:0.001074 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  48 / 300\n",
      "Batch:\t0 /9:  1.300511 1.300527 \tl:0.000984 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.242986 2.546509 \tl:0.000984 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.153814 3.701357 \tl:0.000745 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.267704 4.970381 \tl:0.001106 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.235466 6.206614 \tl:0.000967 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.067399 7.274350 \tl:0.001294 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.051967 8.326842 \tl:0.001607 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.205935 9.533419 \tl:0.001259 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.091181 10.62549 \tl:0.001106 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.789sec Trl:0.001117 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  49 / 300\n",
      "Batch:\t0 /9:  1.252164 1.252177 \tl:0.001178 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.182871 2.435560 \tl:0.000836 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.209759 3.646272 \tl:0.001167 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.169585 4.816725 \tl:0.000569 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.147132 5.964353 \tl:0.000812 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.241609 7.207093 \tl:0.000997 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.253170 8.460527 \tl:0.001482 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.146448 9.607284 \tl:0.000838 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.264053 10.87241 \tl:0.001125 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.062sec Trl:0.001001 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  50 / 300\n",
      "Batch:\t0 /9:  1.271506 1.271520 \tl:0.001021 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.210827 2.482637 \tl:0.000842 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.137238 3.620742 \tl:0.000952 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.247546 4.869271 \tl:0.000944 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.255614 6.125465 \tl:0.000837 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.151003 7.277580 \tl:0.000704 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.141105 8.420067 \tl:0.000707 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.122724 9.543433 \tl:0.000844 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.133998 10.67804 \tl:0.000862 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.864sec Trl:0.000857 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  51 / 300\n",
      "Batch:\t0 /9:  1.229043 1.229057 \tl:0.000699 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.250931 2.480426 \tl:0.000889 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.100629 3.581677 \tl:0.000732 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.268400 4.851266 \tl:0.000859 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.266850 6.119015 \tl:0.000775 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.121602 7.241127 \tl:0.001014 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.286193 8.527533 \tl:0.000814 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.216211 9.744274 \tl:0.000924 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.297183 11.04303 \tl:0.000887 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.256sec Trl:0.000844 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  52 / 300\n",
      "Batch:\t0 /9:  1.228851 1.228865 \tl:0.000717 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.212821 2.444004 \tl:0.000973 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.166993 3.612541 \tl:0.000738 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.147897 4.761054 \tl:0.000604 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.185055 5.947763 \tl:0.000534 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.285837 7.234270 \tl:0.000874 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.134285 8.369011 \tl:0.001339 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.203108 9.572916 \tl:0.000652 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.287138 10.86072 \tl:0.000959 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.121sec Trl:0.000821 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  53 / 300\n",
      "Batch:\t0 /9:  1.114333 1.114354 \tl:0.000540 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.279078 2.394555 \tl:0.000884 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.233870 3.628997 \tl:0.000813 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.060703 4.690591 \tl:0.000911 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.111129 5.802918 \tl:0.000683 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.126327 6.930603 \tl:0.000768 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.141445 8.072762 \tl:0.000646 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.205744 9.279541 \tl:0.000841 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.127804 10.40887 \tl:0.001033 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.598sec Trl:0.000791 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  54 / 300\n",
      "Batch:\t0 /9:  1.085099 1.085114 \tl:0.000706 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.285711 2.371447 \tl:0.000862 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.194880 3.567409 \tl:0.000833 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.146611 4.715204 \tl:0.000855 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.260845 5.976279 \tl:0.000670 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.229671 7.206700 \tl:0.000501 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.125257 8.332215 \tl:0.001080 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.235881 9.568754 \tl:0.000613 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.211081 10.78082 \tl:0.000881 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.984sec Trl:0.000778 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  55 / 300\n",
      "Batch:\t0 /9:  1.361134 1.361148 \tl:0.000794 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.625147 2.987744 \tl:0.000839 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.563718 4.560222 \tl:0.000733 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.454599 6.016839 \tl:0.001037 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.441546 7.459696 \tl:0.000751 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.464752 8.925232 \tl:0.001131 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.264526 10.19127 \tl:0.001027 \tem:1.000000 f1:1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t7 /9:  1.217777 11.41048 \tl:0.000542 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.256488 12.66751 \tl:0.000920 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  12.898sec Trl:0.000864 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  56 / 300\n",
      "Batch:\t0 /9:  1.218688 1.218703 \tl:0.000465 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.150305 2.369965 \tl:0.000823 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.174247 3.544740 \tl:0.000530 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.152764 4.698407 \tl:0.000947 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.340039 6.039484 \tl:0.000642 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.208629 7.248943 \tl:0.000613 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.149364 8.398643 \tl:0.000725 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.101047 9.500873 \tl:0.000643 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.189635 10.69184 \tl:0.000737 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.899sec Trl:0.000680 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  57 / 300\n",
      "Batch:\t0 /9:  1.184772 1.184786 \tl:0.000632 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.215147 2.401229 \tl:0.000900 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.069715 3.471953 \tl:0.000566 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.075181 4.547327 \tl:0.000703 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.081161 5.629582 \tl:0.000703 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.144381 6.774550 \tl:0.000697 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.146616 7.922557 \tl:0.000667 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.321892 9.245344 \tl:0.000505 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.216990 10.46386 \tl:0.000698 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.689sec Trl:0.000675 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  58 / 300\n",
      "Batch:\t0 /9:  1.225528 1.225543 \tl:0.000769 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.134624 2.361704 \tl:0.000457 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.289451 3.652098 \tl:0.000817 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.112545 4.765557 \tl:0.000714 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.156941 5.923226 \tl:0.000443 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.222259 7.147078 \tl:0.000569 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.150826 8.298581 \tl:0.000706 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.157871 9.457167 \tl:0.000495 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.209841 10.66763 \tl:0.000614 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.833sec Trl:0.000620 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  59 / 300\n",
      "Batch:\t0 /9:  1.270661 1.270684 \tl:0.000607 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.282778 2.554592 \tl:0.000477 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.064193 3.619035 \tl:0.000593 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.132470 4.752110 \tl:0.000559 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.231969 5.985177 \tl:0.000533 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.159721 7.145627 \tl:0.000611 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.304130 8.450992 \tl:0.000547 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.347905 9.800359 \tl:0.000589 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.268218 11.06952 \tl:0.000590 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.272sec Trl:0.000567 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313930\n",
      "\n",
      "Epoch:  60 / 300\n",
      "Batch:\t0 /9:  1.233857 1.233878 \tl:0.000512 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.227534 2.462967 \tl:0.000652 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.225578 3.689378 \tl:0.000684 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.187281 4.877238 \tl:0.000546 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.075282 5.953520 \tl:0.000602 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.239002 7.193629 \tl:0.001065 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.126427 8.320319 \tl:0.000486 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.147694 9.469295 \tl:0.000484 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.266697 10.73778 \tl:0.000692 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.945sec Trl:0.000636 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.309449\n",
      "\n",
      "Epoch:  61 / 300\n",
      "Batch:\t0 /9:  1.244812 1.244826 \tl:0.000517 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.184901 2.430800 \tl:0.000406 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.272333 3.703638 \tl:0.000432 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.442782 5.147119 \tl:0.000461 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.254076 6.402565 \tl:0.000408 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.167097 7.570940 \tl:0.000448 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.153987 8.726114 \tl:0.000947 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.272521 9.999374 \tl:0.000596 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.195194 11.19516 \tl:0.000552 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.397sec Trl:0.000530 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.304967\n",
      "\n",
      "Epoch:  62 / 300\n",
      "Batch:\t0 /9:  1.334752 1.334764 \tl:0.000682 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.607510 2.943173 \tl:0.000472 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.456115 4.400351 \tl:0.000493 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.129539 5.531172 \tl:0.000443 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.078637 6.644155 \tl:0.000468 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.133807 7.778257 \tl:0.000504 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.291588 9.070523 \tl:0.000542 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.130663 10.20224 \tl:0.000557 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.355391 11.55864 \tl:0.000473 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.770sec Trl:0.000515 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.304967\n",
      "\n",
      "Epoch:  63 / 300\n",
      "Batch:\t0 /9:  1.124612 1.124627 \tl:0.000492 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.167181 2.292473 \tl:0.005272 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.194364 3.488036 \tl:0.000490 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.170825 4.660299 \tl:0.000580 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.230001 5.891879 \tl:0.000560 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.194032 7.086745 \tl:0.000464 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.239886 8.326838 \tl:0.000457 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.280081 9.607462 \tl:0.001100 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.287092 10.89512 \tl:0.000482 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.112sec Trl:0.001100 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.302784\n",
      "\n",
      "Epoch:  64 / 300\n",
      "Batch:\t0 /9:  1.243580 1.243592 \tl:0.001499 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.125117 2.370074 \tl:0.000446 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.217210 3.588159 \tl:0.000359 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.064443 4.653125 \tl:0.000519 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.159721 5.814078 \tl:0.000330 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.258577 7.073857 \tl:0.000655 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.160116 8.234719 \tl:0.000507 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.152621 9.387572 \tl:0.000541 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.160331 10.54871 \tl:0.000903 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.778sec Trl:0.000640 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.298143\n",
      "\n",
      "Epoch:  65 / 300\n",
      "Batch:\t0 /9:  1.187756 1.187771 \tl:0.000452 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.295129 2.484334 \tl:0.000360 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.213560 3.699358 \tl:0.000566 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.178653 4.878918 \tl:0.000745 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.169507 6.049379 \tl:0.000655 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.149839 7.200289 \tl:0.000368 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.143615 8.359896 \tl:0.000500 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.119536 9.480012 \tl:0.000689 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.173998 10.65478 \tl:0.000610 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.869sec Trl:0.000549 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.298143\n",
      "\n",
      "Epoch:  66 / 300\n",
      "Batch:\t0 /9:  1.122537 1.122550 \tl:0.000467 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.150156 2.274000 \tl:0.000491 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.163377 3.438431 \tl:0.000422 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.284209 4.723567 \tl:0.000334 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.155367 5.880164 \tl:0.000348 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.145391 7.026404 \tl:0.000496 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.160933 8.188755 \tl:0.000422 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.153986 9.343719 \tl:0.000372 \tem:1.000000 f1:1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t8 /9:  1.192255 10.53724 \tl:0.000487 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.745sec Trl:0.000426 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.298143\n",
      "\n",
      "Epoch:  67 / 300\n",
      "Batch:\t0 /9:  1.397225 1.397242 \tl:0.000798 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.268090 2.666666 \tl:0.000330 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.277015 3.944593 \tl:0.000348 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.120360 5.065260 \tl:0.000524 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.389895 6.455484 \tl:0.000395 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.567031 8.023774 \tl:0.000479 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.261188 9.285624 \tl:0.000509 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.261384 10.54795 \tl:0.000382 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.165067 11.71343 \tl:0.000423 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.934sec Trl:0.000465 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.298143\n",
      "\n",
      "Epoch:  68 / 300\n",
      "Batch:\t0 /9:  1.164775 1.164789 \tl:0.000369 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.157858 2.323595 \tl:0.000425 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.254765 3.579113 \tl:0.000676 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.241972 4.821666 \tl:0.000325 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.123584 5.946207 \tl:0.000437 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.295482 7.242306 \tl:0.000481 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.234229 8.476861 \tl:0.000656 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.192328 9.670242 \tl:0.000425 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.105676 10.77611 \tl:0.000335 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.003sec Trl:0.000459 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.298143\n",
      "\n",
      "Epoch:  69 / 300\n",
      "Batch:\t0 /9:  1.271480 1.271495 \tl:0.000472 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.143600 2.415349 \tl:0.000714 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.160055 3.576003 \tl:0.000510 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.231256 4.808434 \tl:0.000365 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.163717 5.972646 \tl:0.000377 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.116163 7.089026 \tl:0.000637 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.081154 8.170651 \tl:0.000324 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.085825 9.256983 \tl:0.000523 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.159054 10.41682 \tl:0.000478 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.637sec Trl:0.000489 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.117647 \tTef1:0.311588\n",
      "\n",
      "Epoch:  70 / 300\n",
      "Batch:\t0 /9:  1.123910 1.123924 \tl:0.000336 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.074233 2.199162 \tl:0.000460 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.219566 3.420336 \tl:0.000576 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.307889 4.729465 \tl:0.000266 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.512300 6.242408 \tl:0.000333 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.429502 7.672593 \tl:0.000376 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.454952 9.128228 \tl:0.000553 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.475947 10.60562 \tl:0.000483 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.264855 11.87176 \tl:0.000420 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  12.078sec Trl:0.000423 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.117647 \tTef1:0.311588\n",
      "\n",
      "Epoch:  71 / 300\n",
      "Batch:\t0 /9:  1.235402 1.235418 \tl:0.000402 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.165457 2.402181 \tl:0.000426 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.164398 3.567438 \tl:0.000395 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.237674 4.806190 \tl:0.000567 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.152328 5.959762 \tl:0.000431 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.153500 7.114037 \tl:0.000381 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.276653 8.391617 \tl:0.000434 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.598822 9.991625 \tl:0.000382 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.470591 11.46335 \tl:0.000434 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.699sec Trl:0.000428 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.117647 \tTef1:0.311588\n",
      "\n",
      "Epoch:  72 / 300\n",
      "Batch:\t0 /9:  1.557688 1.557715 \tl:0.000452 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.504062 3.063037 \tl:0.000334 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.485642 4.550163 \tl:0.000433 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.231040 5.782326 \tl:0.000295 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.071183 6.854311 \tl:0.000444 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.078923 7.934190 \tl:0.000347 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.115520 9.051003 \tl:0.000408 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.247852 10.30004 \tl:0.000405 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.176674 11.47772 \tl:0.000651 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.685sec Trl:0.000419 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.117647 \tTef1:0.311588\n",
      "\n",
      "Epoch:  73 / 300\n",
      "Batch:\t0 /9:  1.273607 1.273620 \tl:0.000413 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.240587 2.515480 \tl:0.000331 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.226439 3.742601 \tl:0.000607 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.291341 5.035214 \tl:0.000414 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.141587 6.177500 \tl:0.000520 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.057002 7.235291 \tl:0.000383 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.090600 8.326808 \tl:0.000345 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.330155 9.657811 \tl:0.000283 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.208168 10.86652 \tl:0.000437 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.028sec Trl:0.000415 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.117647 \tTef1:0.311588\n",
      "\n",
      "Epoch:  74 / 300\n",
      "Batch:\t0 /9:  1.084645 1.084664 \tl:0.000356 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.181807 2.267528 \tl:0.000384 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.102769 3.370903 \tl:0.000343 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.249154 4.620724 \tl:0.000297 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.256781 5.878629 \tl:0.000441 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.206959 7.086442 \tl:0.000376 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.133388 8.220987 \tl:0.000394 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.219982 9.442087 \tl:0.000313 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.203714 10.64604 \tl:0.284079 \tem:0.980000 f1:0.990000\n",
      "\n",
      "Epoch performance:  10.854sec Trl:0.031887 \n",
      "\tTrem:0.997778 \tTrf1:0.998889 \tTeem:0.117647 \tTef1:0.325193\n",
      "\n",
      "Epoch:  75 / 300\n",
      "Batch:\t0 /9:  1.222723 1.222736 \tl:0.000445 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.381737 2.605942 \tl:0.019554 \tem:0.980000 f1:0.980000\n",
      "Batch:\t2 /9:  1.305710 3.912415 \tl:0.002726 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.228604 5.142126 \tl:0.000403 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.165836 6.309426 \tl:0.124387 \tem:0.960000 f1:0.960000\n",
      "Batch:\t5 /9:  1.117639 7.427719 \tl:0.000598 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.241628 8.669684 \tl:0.000446 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.419445 10.09050 \tl:0.000585 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.600859 11.69243 \tl:0.000727 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.898sec Trl:0.016652 \n",
      "\tTrem:0.993333 \tTrf1:0.993333 \tTeem:0.084034 \tTef1:0.268277\n",
      "\n",
      "Epoch:  76 / 300\n",
      "Batch:\t0 /9:  1.566176 1.566189 \tl:0.000676 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.201272 2.768047 \tl:0.000880 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.255901 4.025266 \tl:0.001482 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.144032 5.170345 \tl:0.003763 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.303539 6.475024 \tl:0.027739 \tem:0.980000 f1:0.993333\n",
      "Batch:\t5 /9:  1.293565 7.769260 \tl:0.174235 \tem:0.980000 f1:0.990000\n",
      "Batch:\t6 /9:  1.295840 9.065613 \tl:0.192304 \tem:0.940000 f1:0.984872\n",
      "Batch:\t7 /9:  1.150929 10.21790 \tl:0.001815 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.238177 11.45770 \tl:0.001205 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.644sec Trl:0.044900 \n",
      "\tTrem:0.988889 \tTrf1:0.996467 \tTeem:0.100840 \tTef1:0.310143\n",
      "\n",
      "Epoch:  77 / 300\n",
      "Batch:\t0 /9:  1.164878 1.164895 \tl:0.005659 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.222573 2.388319 \tl:0.001228 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.128649 3.517776 \tl:0.001838 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.149269 4.668069 \tl:0.002548 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.139585 5.808825 \tl:0.015775 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.231010 7.040526 \tl:0.003059 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.393429 8.434659 \tl:0.003509 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.293555 9.729336 \tl:0.004310 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.087107 10.81787 \tl:0.006690 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.981sec Trl:0.004957 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.075630 \tTef1:0.313985\n",
      "\n",
      "Epoch:  78 / 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t0 /9:  1.082170 1.082183 \tl:0.006652 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.088672 2.171375 \tl:0.013799 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.170941 3.343305 \tl:0.011861 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.137242 4.480849 \tl:0.004278 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.178158 5.659566 \tl:0.005259 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.288730 6.949544 \tl:0.062719 \tem:0.980000 f1:0.993196\n",
      "Batch:\t6 /9:  1.170452 8.120836 \tl:0.003355 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.192972 9.314162 \tl:0.005687 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.156332 10.47197 \tl:0.003446 \tem:1.000000 f1:1.000000\n",
      "Saving new model on epoch 78\n",
      "\n",
      "Epoch performance:  11.186sec Trl:0.013006 \n",
      "\tTrem:0.997778 \tTrf1:0.999244 \tTeem:0.100840 \tTef1:0.336174\n",
      "\n",
      "Epoch:  79 / 300\n",
      "Batch:\t0 /9:  1.202981 1.203002 \tl:0.002164 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.156255 2.360201 \tl:0.002425 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.158138 3.519016 \tl:0.003110 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.163056 4.682534 \tl:0.003040 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.241786 5.925173 \tl:0.003067 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.126227 7.052132 \tl:0.002608 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.279232 8.331918 \tl:0.002491 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.322360 9.654533 \tl:0.002724 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.379788 11.03494 \tl:0.002441 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.234sec Trl:0.002674 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.323116\n",
      "\n",
      "Epoch:  80 / 300\n",
      "Batch:\t0 /9:  1.281316 1.281329 \tl:0.002528 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.284686 2.567127 \tl:0.001896 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.162273 3.729959 \tl:0.001544 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.227704 4.959228 \tl:0.002298 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.241675 6.201843 \tl:0.001918 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.219627 7.422716 \tl:0.001573 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.180865 8.604208 \tl:0.002024 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.210506 9.815448 \tl:0.001349 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.593070 11.40886 \tl:0.001807 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.597sec Trl:0.001882 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.321700\n",
      "\n",
      "Epoch:  81 / 300\n",
      "Batch:\t0 /9:  1.144036 1.144051 \tl:0.001489 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.164937 2.310212 \tl:0.001586 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.162451 3.473804 \tl:0.001769 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.159944 4.634294 \tl:0.001411 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.262424 5.898098 \tl:0.001856 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.285537 7.184921 \tl:0.001185 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.137327 8.322812 \tl:0.001052 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.154282 9.478034 \tl:0.001459 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.199077 10.67777 \tl:0.001273 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.883sec Trl:0.001453 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.321700\n",
      "\n",
      "Epoch:  82 / 300\n",
      "Batch:\t0 /9:  1.199147 1.199164 \tl:0.001403 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.522491 2.722662 \tl:0.000735 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.435679 4.159337 \tl:0.001007 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.401801 5.561605 \tl:0.000862 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.507998 7.079819 \tl:0.001257 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.475245 8.556334 \tl:0.001128 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.207672 9.765433 \tl:0.001144 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.232245 10.99924 \tl:0.001190 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.153144 12.15338 \tl:0.001020 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  12.367sec Trl:0.001083 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.322051\n",
      "\n",
      "Epoch:  83 / 300\n",
      "Batch:\t0 /9:  1.272602 1.272624 \tl:0.000884 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.176681 2.450603 \tl:0.001067 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.231456 3.682366 \tl:0.001027 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.117340 4.800795 \tl:0.000893 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.176528 5.978401 \tl:0.001015 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.212944 7.192085 \tl:0.000913 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.119348 8.312052 \tl:0.000888 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.108880 9.422002 \tl:0.000645 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.253769 10.67609 \tl:0.000899 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.835sec Trl:0.000914 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.322051\n",
      "\n",
      "Epoch:  84 / 300\n",
      "Batch:\t0 /9:  1.078682 1.078694 \tl:0.001007 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.309617 2.388871 \tl:0.000654 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.140317 3.530230 \tl:0.000775 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.217327 4.748230 \tl:0.000771 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.263329 6.012532 \tl:0.000650 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.234241 7.247472 \tl:0.000731 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.234510 8.482678 \tl:0.000773 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.204642 9.687911 \tl:0.000872 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.167882 10.85713 \tl:0.000775 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.055sec Trl:0.000779 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313874\n",
      "\n",
      "Epoch:  85 / 300\n",
      "Batch:\t0 /9:  1.171137 1.171154 \tl:0.000881 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.223772 2.396005 \tl:0.001096 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.131498 3.528686 \tl:0.000657 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.146927 4.676787 \tl:0.000926 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.158188 5.835732 \tl:0.001039 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.244007 7.080647 \tl:0.000665 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.308341 8.389899 \tl:0.000734 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.208427 9.599108 \tl:0.000622 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.197940 10.79817 \tl:0.000693 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.006sec Trl:0.000813 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313874\n",
      "\n",
      "Epoch:  86 / 300\n",
      "Batch:\t0 /9:  1.215097 1.215114 \tl:0.000856 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.167455 2.407394 \tl:0.000666 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.232372 3.640325 \tl:0.000716 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.229984 4.871061 \tl:0.000422 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.122905 5.994167 \tl:0.000726 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.192800 7.187500 \tl:0.000656 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.299530 8.487931 \tl:0.000588 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.557215 10.04647 \tl:0.000618 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.262111 11.30881 \tl:0.000618 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.560sec Trl:0.000652 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313874\n",
      "\n",
      "Epoch:  87 / 300\n",
      "Batch:\t0 /9:  1.313450 1.313465 \tl:0.000421 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.164336 2.478327 \tl:0.000641 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.235831 3.714696 \tl:0.000685 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.077017 4.792140 \tl:0.000543 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.178548 5.971665 \tl:0.000678 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.141875 7.114526 \tl:0.000814 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.559771 8.674895 \tl:0.000479 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.454983 10.13009 \tl:0.000461 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.421707 11.55234 \tl:0.000842 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.756sec Trl:0.000618 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313874\n",
      "\n",
      "Epoch:  88 / 300\n",
      "Batch:\t0 /9:  1.550009 1.550024 \tl:0.000668 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.522998 3.074018 \tl:0.000558 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.257298 4.331921 \tl:0.000500 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.362981 5.695693 \tl:0.000439 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.405071 7.102723 \tl:0.000523 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.192764 8.296378 \tl:0.000506 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.241942 9.539825 \tl:0.000649 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.150306 10.69101 \tl:0.000490 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.189982 11.88163 \tl:0.000482 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  12.088sec Trl:0.000535 \n",
      "\tTrem:1.000000 \tTrf1:1.000000 \tTeem:0.100840 \tTef1:0.313874\n",
      "\n",
      "Epoch:  89 / 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t0 /9:  1.229765 1.229776 \tl:0.000629 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.157592 2.387865 \tl:0.000640 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.152554 3.541004 \tl:0.000389 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.186048 4.728039 \tl:0.000544 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.257901 5.986781 \tl:0.000590 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.156593 7.144340 \tl:0.257457 \tem:0.980000 f1:0.993333\n",
      "Batch:\t6 /9:  1.216481 8.362222 \tl:0.000685 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.169862 9.533521 \tl:0.005363 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.129328 10.66405 \tl:0.003523 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.872sec Trl:0.029980 \n",
      "\tTrem:0.997778 \tTrf1:0.999259 \tTeem:0.084034 \tTef1:0.287339\n",
      "\n",
      "Epoch:  90 / 300\n",
      "Batch:\t0 /9:  1.270783 1.270799 \tl:0.061193 \tem:0.980000 f1:0.990256\n",
      "Batch:\t1 /9:  1.135285 2.407553 \tl:0.059744 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.214159 3.623041 \tl:0.056240 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.111800 4.735956 \tl:0.032131 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.274693 6.011937 \tl:0.129428 \tem:0.980000 f1:0.995294\n",
      "Batch:\t5 /9:  1.243607 7.256659 \tl:0.041278 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.254946 8.512286 \tl:0.073063 \tem:0.980000 f1:0.989231\n",
      "Batch:\t7 /9:  1.223440 9.736242 \tl:0.058333 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.155977 10.89291 \tl:0.161239 \tem:0.980000 f1:0.980000\n",
      "\n",
      "Epoch performance:  11.081sec Trl:0.074739 \n",
      "\tTrem:0.991111 \tTrf1:0.994976 \tTeem:0.067227 \tTef1:0.275488\n",
      "\n",
      "Epoch:  91 / 300\n",
      "Batch:\t0 /9:  1.088811 1.088831 \tl:0.032233 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.194765 2.284276 \tl:0.064233 \tem:0.960000 f1:0.978462\n",
      "Batch:\t2 /9:  1.236937 3.522166 \tl:0.031922 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.253080 4.775964 \tl:0.033451 \tem:0.980000 f1:0.988000\n",
      "Batch:\t4 /9:  1.261137 6.037366 \tl:0.013268 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.124108 7.161834 \tl:0.067164 \tem:0.980000 f1:0.993333\n",
      "Batch:\t6 /9:  1.146533 8.309280 \tl:0.142527 \tem:0.980000 f1:0.980000\n",
      "Batch:\t7 /9:  1.178595 9.488896 \tl:0.005147 \tem:1.000000 f1:1.000000\n",
      "Batch:\t8 /9:  1.067218 10.55656 \tl:0.003849 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  10.764sec Trl:0.043755 \n",
      "\tTrem:0.988889 \tTrf1:0.993311 \tTeem:0.092437 \tTef1:0.311031\n",
      "\n",
      "Epoch:  92 / 300\n",
      "Batch:\t0 /9:  1.249577 1.249644 \tl:0.500625 \tem:0.960000 f1:0.960000\n",
      "Batch:\t1 /9:  1.248100 2.498664 \tl:0.038140 \tem:0.980000 f1:0.980000\n",
      "Batch:\t2 /9:  1.153414 3.652611 \tl:0.275077 \tem:0.920000 f1:0.923226\n",
      "Batch:\t3 /9:  1.219321 4.872610 \tl:0.012692 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.145542 6.018973 \tl:0.182689 \tem:0.960000 f1:0.963556\n",
      "Batch:\t5 /9:  1.126419 7.146173 \tl:0.027277 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.139984 8.287255 \tl:0.009399 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.261422 9.549739 \tl:0.039705 \tem:0.980000 f1:0.980000\n",
      "Batch:\t8 /9:  1.136272 10.68704 \tl:0.274525 \tem:0.960000 f1:0.961509\n",
      "\n",
      "Epoch performance:  10.880sec Trl:0.151125 \n",
      "\tTrem:0.973333 \tTrf1:0.974255 \tTeem:0.067227 \tTef1:0.282620\n",
      "\n",
      "Epoch:  93 / 300\n",
      "Batch:\t0 /9:  1.296723 1.296736 \tl:0.220849 \tem:0.920000 f1:0.934468\n",
      "Batch:\t1 /9:  1.357724 2.655638 \tl:0.015089 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.255805 3.911694 \tl:0.019808 \tem:1.000000 f1:1.000000\n",
      "Batch:\t3 /9:  1.210841 5.123664 \tl:0.195974 \tem:0.980000 f1:0.980000\n",
      "Batch:\t4 /9:  1.148967 6.273780 \tl:0.021876 \tem:1.000000 f1:1.000000\n",
      "Batch:\t5 /9:  1.133208 7.407469 \tl:0.297808 \tem:0.940000 f1:0.960058\n",
      "Batch:\t6 /9:  1.159914 8.568059 \tl:0.085973 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.222711 9.791351 \tl:0.928165 \tem:0.920000 f1:0.943727\n",
      "Batch:\t8 /9:  1.175482 10.96803 \tl:0.387344 \tem:0.980000 f1:0.996585\n",
      "\n",
      "Epoch performance:  11.161sec Trl:0.241432 \n",
      "\tTrem:0.971111 \tTrf1:0.979426 \tTeem:0.084034 \tTef1:0.251321\n",
      "\n",
      "Epoch:  94 / 300\n",
      "Batch:\t0 /9:  1.185121 1.185140 \tl:0.904211 \tem:0.900000 f1:0.917502\n",
      "Batch:\t1 /9:  1.225422 2.411814 \tl:0.080116 \tem:0.980000 f1:0.994054\n",
      "Batch:\t2 /9:  1.070352 3.482598 \tl:0.294086 \tem:0.940000 f1:0.956727\n",
      "Batch:\t3 /9:  1.311429 4.795021 \tl:1.276122 \tem:0.820000 f1:0.859446\n",
      "Batch:\t4 /9:  1.152132 5.948553 \tl:0.634786 \tem:0.800000 f1:0.828216\n",
      "Batch:\t5 /9:  1.125636 7.074941 \tl:0.274351 \tem:0.940000 f1:0.961064\n",
      "Batch:\t6 /9:  1.156692 8.232421 \tl:1.144904 \tem:0.900000 f1:0.913333\n",
      "Batch:\t7 /9:  1.308228 9.541344 \tl:0.937493 \tem:0.820000 f1:0.863407\n",
      "Batch:\t8 /9:  1.191176 10.73305 \tl:1.211224 \tem:0.740000 f1:0.817845\n",
      "\n",
      "Epoch performance:  10.924sec Trl:0.750810 \n",
      "\tTrem:0.871111 \tTrf1:0.901288 \tTeem:0.084034 \tTef1:0.265653\n",
      "\n",
      "Epoch:  95 / 300\n",
      "Batch:\t0 /9:  1.126667 1.126680 \tl:0.142338 \tem:0.980000 f1:0.985714\n",
      "Batch:\t1 /9:  1.148853 2.276033 \tl:0.363338 \tem:0.920000 f1:0.929231\n",
      "Batch:\t2 /9:  1.144186 3.420818 \tl:0.339260 \tem:0.940000 f1:0.948814\n",
      "Batch:\t3 /9:  1.259620 4.681643 \tl:0.709042 \tem:0.900000 f1:0.934403\n",
      "Batch:\t4 /9:  1.081184 5.763342 \tl:0.929965 \tem:0.800000 f1:0.843871\n",
      "Batch:\t5 /9:  1.568569 7.332853 \tl:0.726830 \tem:0.900000 f1:0.964405\n",
      "Batch:\t6 /9:  1.218606 8.552424 \tl:1.104725 \tem:0.840000 f1:0.897332\n",
      "Batch:\t7 /9:  1.152271 9.704905 \tl:0.302232 \tem:0.940000 f1:0.948941\n",
      "Batch:\t8 /9:  1.057049 10.76240 \tl:0.449024 \tem:0.900000 f1:0.959580\n",
      "\n",
      "Epoch performance:  10.965sec Trl:0.562973 \n",
      "\tTrem:0.902222 \tTrf1:0.934699 \tTeem:0.084034 \tTef1:0.283421\n",
      "\n",
      "Epoch:  96 / 300\n",
      "Batch:\t0 /9:  1.250075 1.250089 \tl:0.192897 \tem:0.960000 f1:0.969082\n",
      "Batch:\t1 /9:  1.282466 2.533119 \tl:0.378594 \tem:0.920000 f1:0.951681\n",
      "Batch:\t2 /9:  1.168255 3.703029 \tl:0.603129 \tem:0.920000 f1:0.926091\n",
      "Batch:\t3 /9:  1.310760 5.015267 \tl:0.802172 \tem:0.860000 f1:0.904440\n",
      "Batch:\t4 /9:  1.182410 6.199160 \tl:0.295195 \tem:0.940000 f1:0.941455\n",
      "Batch:\t5 /9:  1.076723 7.277105 \tl:0.085341 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.606618 8.884589 \tl:0.225588 \tem:0.940000 f1:0.959503\n",
      "Batch:\t7 /9:  1.273268 10.15859 \tl:0.367938 \tem:0.920000 f1:0.936322\n",
      "Batch:\t8 /9:  1.227128 11.38669 \tl:0.086260 \tem:0.980000 f1:0.990000\n",
      "\n",
      "Epoch performance:  11.562sec Trl:0.337457 \n",
      "\tTrem:0.937778 \tTrf1:0.953175 \tTeem:0.100840 \tTef1:0.266200\n",
      "\n",
      "Epoch:  97 / 300\n",
      "Batch:\t0 /9:  1.244628 1.244641 \tl:0.077426 \tem:1.000000 f1:1.000000\n",
      "Batch:\t1 /9:  1.152239 2.397402 \tl:0.294654 \tem:0.960000 f1:0.976032\n",
      "Batch:\t2 /9:  1.380258 3.778393 \tl:0.222161 \tem:0.920000 f1:0.943260\n",
      "Batch:\t3 /9:  1.334989 5.113672 \tl:0.237144 \tem:0.960000 f1:0.965263\n",
      "Batch:\t4 /9:  1.230612 6.345001 \tl:0.433120 \tem:0.960000 f1:0.971649\n",
      "Batch:\t5 /9:  1.213961 7.560276 \tl:0.250129 \tem:0.860000 f1:0.891540\n",
      "Batch:\t6 /9:  1.118268 8.679517 \tl:0.169585 \tem:0.960000 f1:0.976000\n",
      "Batch:\t7 /9:  1.136704 9.816741 \tl:0.205836 \tem:0.980000 f1:0.980000\n",
      "Batch:\t8 /9:  1.113472 10.93046 \tl:0.793933 \tem:0.920000 f1:0.938098\n",
      "\n",
      "Epoch performance:  11.136sec Trl:0.298221 \n",
      "\tTrem:0.946667 \tTrf1:0.960205 \tTeem:0.092437 \tTef1:0.272631\n",
      "\n",
      "Epoch:  98 / 300\n",
      "Batch:\t0 /9:  1.219679 1.219694 \tl:0.081678 \tem:0.980000 f1:0.980000\n",
      "Batch:\t1 /9:  1.224380 2.445115 \tl:0.207952 \tem:0.960000 f1:0.970303\n",
      "Batch:\t2 /9:  1.180196 3.626925 \tl:0.097481 \tem:0.980000 f1:0.996727\n",
      "Batch:\t3 /9:  1.206048 4.834089 \tl:0.044013 \tem:1.000000 f1:1.000000\n",
      "Batch:\t4 /9:  1.264565 6.099499 \tl:0.142329 \tem:0.960000 f1:0.969333\n",
      "Batch:\t5 /9:  1.139294 7.240065 \tl:0.143581 \tem:0.980000 f1:0.980000\n",
      "Batch:\t6 /9:  1.143776 8.385271 \tl:0.081893 \tem:0.960000 f1:0.960000\n",
      "Batch:\t7 /9:  1.179193 9.565876 \tl:0.102362 \tem:0.960000 f1:0.971429\n",
      "Batch:\t8 /9:  1.264765 10.83228 \tl:0.035568 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.022sec Trl:0.104095 \n",
      "\tTrem:0.975556 \tTrf1:0.980866 \tTeem:0.092437 \tTef1:0.269476\n",
      "\n",
      "Epoch:  99 / 300\n",
      "Batch:\t0 /9:  1.181820 1.181835 \tl:0.129069 \tem:0.960000 f1:0.973333\n",
      "Batch:\t1 /9:  1.255975 2.438065 \tl:0.052166 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.319329 3.758547 \tl:0.362343 \tem:0.980000 f1:0.980000\n",
      "Batch:\t3 /9:  1.557381 5.316646 \tl:0.060774 \tem:0.960000 f1:0.960000\n",
      "Batch:\t4 /9:  1.232526 6.551054 \tl:0.057545 \tem:0.980000 f1:0.995636\n",
      "Batch:\t5 /9:  1.263883 7.816154 \tl:0.023147 \tem:1.000000 f1:1.000000\n",
      "Batch:\t6 /9:  1.197514 9.014229 \tl:0.021018 \tem:1.000000 f1:1.000000\n",
      "Batch:\t7 /9:  1.228143 10.24390 \tl:0.035298 \tem:0.980000 f1:0.989412\n",
      "Batch:\t8 /9:  1.078775 11.32314 \tl:0.014683 \tem:1.000000 f1:1.000000\n",
      "\n",
      "Epoch performance:  11.508sec Trl:0.084005 \n",
      "\tTrem:0.984444 \tTrf1:0.988709 \tTeem:0.092437 \tTef1:0.254369\n",
      "\n",
      "Epoch:  100 / 300\n",
      "Batch:\t0 /9:  1.247637 1.247656 \tl:0.025359 \tem:0.980000 f1:0.981702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t1 /9:  1.166624 2.415791 \tl:0.012038 \tem:1.000000 f1:1.000000\n",
      "Batch:\t2 /9:  1.199188 3.615579 \tl:0.442797 \tem:0.980000 f1:0.983333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-f205e19f0955>\", line 100, in train\n",
      "    loss.backward()\n",
      "  File \"/data/priyansh/virtualenvironment/pwc-reg-m/local/lib/python2.7/site-packages/torch/tensor.py\", line 93, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/data/priyansh/virtualenvironment/pwc-reg-m/local/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 89, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-2dbe6f7f8aec>\", line 92, in training_loop\n",
      "    debug=_macros['debug']\n",
      "TypeError: 'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "op = training_loop(_models=[ques_model, para_model, mlstm_model, pointer_decoder_model],\n",
    "                   _data=data,\n",
    "                   _debug=macros['debug'],\n",
    "                   _save=-1,\n",
    "                   _test_eval=1,\n",
    "                   _train_eval=1,\n",
    "                   _epochs=EPOCHS,\n",
    "                   _macros=macros)    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# See if gradients are being passed\n",
    "p = list(filter(lambda p: p.requires_grad, ques_model.parameters())) + \\\n",
    "    list(filter(lambda p: p.requires_grad, para_model.parameters())) + \\\n",
    "    list(para_model.parameters()) + \\\n",
    "    list(pointer_decoder_model.parameters())\n",
    "                       \n",
    "print([ x.grad.sum().item() for x in p])                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "\n",
    "So far, we plot the training losss. \n",
    "Shall we superimpose test loss on it too? We don't calculate test loss per batch though (fortunately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAHVCAYAAAC0UcmeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmYXHWdL/73NwkQlhAgBERZOoKgaRCQcIWBBDCOIqPIuIDM6A9RhrnquN3RUbwLesWfOuo4OtdBEXEcxQURlxF1UEQWZZFdFhUuAgmyJOwBAgk5949vmoSYpZJ09anqfr2e5zynqrq6zodOcare57uVpmkCAABA/xjXdgEAAACsHUEOAACgzwhyAAAAfUaQAwAA6DOCHAAAQJ8R5AAAAPqMIAcAANBnBDkAAIA+I8gBAAD0mQltF7C8rbfeuhkYGGi7DAAAgFZcccUV85ummbqm5/VUkBsYGMjll1/edhkAAACtKKXc1snzdK0EAADoM4IcAABAnxHkAAAA+kxPjZFbmUWLFmXu3LlZuHBh26WMahMnTsz222+fDTbYoO1SAACANej5IDd37txMmjQpAwMDKaW0Xc6o1DRN7r333sydOzfTpk1ruxwAAGANer5r5cKFCzNlyhQhrotKKZkyZYpWTwAA6BM9H+SSCHEjwN8YAAD6R18EOQAAAJYR5DrwkY98JIODg3n+85+fvfbaK5deeumIHXtgYCDz589f7XM222yztXrND37wg/nkJz+5PmUBAAAt6vnJTtp28cUX54c//GGuvPLKbLTRRpk/f36eeOKJtssCAADGsL4Kcu96V3L11cP7mnvtlfzzP6/653feeWe23nrrbLTRRkmSrbfe+qmfnXvuuXnPe96TxYsXZ999983JJ5+cjTbaKAMDAzn66KPz4x//OBMmTMgpp5ySE044ITfffHPe+9735r/+1/+aJPnEJz6RM844I48//nj+8i//Mh/60IdWW+sRRxyROXPmZOHChXnnO9+Z448//qmfvfvd784555yTZzzjGfnmN7+ZqVOn5v/+3/+bt73tbZk3b1422WSTfPGLX8xzn/vc9fhrAQAAvUDXyjV4yUtekjlz5mTXXXfNW9/61px//vlJ6myab3zjG/Otb30rv/nNb7J48eKcfPLJT/3ejjvumKuvvjozZ87MG9/4xpx55pm55JJLcuKJJyZJzjnnnNx000257LLLcvXVV+eKK67IBRdcsNpaTjvttFxxxRW5/PLL89nPfjb33ntvkuSRRx7JjBkzcv311+eggw56KhAef/zx+Zd/+ZdcccUV+eQnP5m3vvWt3fgTAQAAI6yvWuRW13LWLZtttlmuuOKKXHjhhTnvvPNy1FFH5WMf+1j23nvvTJs2LbvuumuS5JhjjsnnPve5vOtd70qSHH744UmSPfbYIwsWLMikSZMyadKkbLTRRnnggQdyzjnn5Jxzzsnee++dJFmwYEFuuummzJo1a5W1fPazn813v/vdJMmcOXNy0003ZcqUKRk3blyOOuqoJMnrX//6vOpVr8qCBQvyq1/9Kq997Wuf+v3HH398+P9AAADAiOurINeW8ePH5+CDD87BBx+cPfbYI1/5yleeCmCrMtQVc9y4cU/dHrq/ePHiNE2TE044IX/7t3/bUQ2/+MUv8rOf/SwXX3xxNtlkkxx88MGrXPetlJIlS5Zkiy22yNXD3RcVAABona6Va/C73/0uN91001P3r7766uy0007Zbbfdcuutt+bmm29Oknz1q1/NQQcd1PHrvvSlL81pp52WBQsWJEnuuOOO3HPPPat8/oMPPpgtt9wym2yySX7729/mkksueepnS5YsyZlnnpkk+frXv54DDzwwm2++eaZNm5Zvf/vbSZKmaXLNNdd0/h8OAAD0LC1ya7BgwYK8/e1vzwMPPJAJEyZkl112ySmnnJKJEyfmy1/+cl772tc+NdnJ0CQmnXjJS16SG2+8Mfvvv3+S2oXza1/7WrbZZpuVPv/QQw/N5z//+Tzvec/Lbrvtlv322++pn2266aa57LLLctJJJ2WbbbbJt771rSTJ6aefnre85S056aSTsmjRorzuda/LnnvuuR5/DQAAoBeUpmnaruEpM2bMaC6//PKnPXbjjTfmec97XksVJYsXJ6Uk48e3VsKIaftvDQAAY10p5YqmaWas6Xm6Vq7GY4/V5Q4eeKDtSgAAAJYR5FZjww3r3vrfAABAL+mLINdW98/x45MJE5KxMGt/L3WxBQAAVq/ng9zEiRNz7733thY0Ntpo9LfINU2Te++9NxMnTmy7FAAAoAM9P2vl9ttvn7lz52bevHmtHH/evBrknnyylcOPmIkTJ2b77bdvuwwAAKADPR/kNthgg0ybNq2143/5y8lnP5s8+mgyrufbLwEAgLFANFmDgYE6Ru7uu9uuBAAAoBLk1mBgoO5vvbXNKgAAAJYR5NZgp53q/rbb2q0DAABgiCC3BkNBToscAADQKwS5Ndhss2TrrQU5AACgdwhyHRgYEOQAAIDeIch1YKedBDkAAKB3CHIdGBiok500TduVAAAACHIdGRhIFi5M7rmn7UoAAAAEuY5YSw4AAOglglwHBDkAAKCXCHIdsCg4AADQSwS5DkyalGy1lRY5AACgNwhyHbKWHAAA0CsEuQ4JcgAAQK8Q5Do0tCi4teQAAIC2CXIdGhhIHnssmT+/7UoAAICxTpDrkCUIAACAXiHIdUiQAwAAeoUg16GhteQEOQAAoG2CXIcmT0622EKQAwAA2ifIrYWBgeS229quAgAAGOsEubVgLTkAAKAXCHJrYSjIWUsOAABokyC3FnbaKXnkkeTee9uuBAAAGMsEubUwtASBcXIAAECbBLm1YC05AACgFwhya0GQAwAAeoEgtxa22CLZfHNBDgAAaFdXg1wp5d2llOtLKdeVUr5RSpnYzeONBEsQAAAAbetakCulPCvJO5LMaJpm9yTjk7yuW8cbKRYFBwAA2tbtrpUTkmxcSpmQZJMkf+zy8brOWnIAAEDbuhbkmqa5I8knk9ye5M4kDzZNc86KzyulHF9KubyUcvm8efO6Vc6wGRhIHn44uf/+tisBAADGqm52rdwyySuTTEvyzCSbllJev+LzmqY5pWmaGU3TzJg6dWq3yhk2O+1U98bJAQAAbelm18oXJ/lD0zTzmqZZlOSsJH/WxeONCIuCAwAAbetmkLs9yX6llE1KKSXJ7CQ3dvF4I8JacgAAQNu6OUbu0iRnJrkyyW+WHuuUbh1vpGy5ZTJpkiAHAAC0Z0I3X7xpmhOTnNjNY4y0Uuo4OUEOAABoS7eXHxiVLAoOAAC0SZBbBxYFBwAA2iTIrYOBgeTBB5MHHmi7EgAAYCwS5NaBmSsBAIA2CXLrwKLgAABAmwS5dWBRcAAAoE2C3DqYMiXZdFMtcgAAQDsEuXVQiiUIAACA9ghy68ii4AAAQFsEuXWkRQ4AAGiLILeOBgbqOnIPPth2JQAAwFgjyK0jM1cCAABtEeTWkUXBAQCAtghy68ii4AAAQFsEuXU0dWqy8ca6VgIAACNPkFtH1pIDAADaIsitB0EOAABogyC3HiwKDgAAtEGQWw/TpiX33Vc3AACAkSLIrYd99637Sy5ptw4AAGBsEeTWwwtfmEyYkFx4YduVAAAAY4kgtx422STZZx9BDgAAGFmC3HqaOTP59a+ThQvbrgQAABgrBLn1NHNm8sQTNcwBAACMBEFuPR1wQN3rXgkAAIwUQW49TZmSTJ8uyAEAACNHkBsGM2cmv/pV8uSTbVcCAACMBYLcMDjwwOShh5Lf/KbtSgAAgLFAkBsGM2fWve6VAADASBDkhsFOOyU77JBcdFHblQAAAGOBIDdMDjywtsg1TduVAAAAo50gN0xmzkzuvDO55Za2KwEAAEY7QW6YDI2T070SAADoNkFumEyfnmy5pQlPAACA7hPkhsm4cckBBwhyAABA9wlyw2jmzOT3v0/uuaftSgAAgNFMkBtGxskBAAAjQZAbRvvsk0ycqHslAADQXYLcMNpww+SFL9QiBwAAdJcgN8xmzkyuuipZsKDtSgAAgNFKkBtmBx6YPPlkcvHFbVcCAACMVoLcMNt//7oUgXFyAABAtwhyw2zzzZO99jJODgAA6B5BrgsOPDC55JLkiSfargQAABiNBLkumDkzeeyx5Mor264EAAAYjQS5LjjwwLrXvRIAAOgGQa4LnvGMZJddTHgCAAB0hyDXJTNn1ha5JUvargQAABhtBLkumTkzue++5Le/bbsSAABgtBHkumRonJzulQAAwHAT5Lpkl12SbbcV5AAAgOEnyHVJKbV7pSAHAAAMN0Gui2bNSm6/PbnttrYrAQAARhNBrotmzap7rXIAAMBwEuS6aPfdk8mTkwsuaLsSAABgNBHkumj8+Dp7pSAHAAAMJ0Guy2bNSn73u+Tuu9uuBAAAGC0EuS4bGid30UXt1gEAAIweglyXveAFycYb614JAAAMH0GuyzbcMNl/f0EOAAAYPoLcCJg1K7nmmuSBB9quBAAAGA0EuREwa1bSNMmvftV2JQAAwGggyI2AF74w2WAD3SsBAIDhIciNgE02SWbMEOQAAIDhIciNkFmzkl//Onn00bYrAQAA+p0gN0JmzUoWL04uvbTtSgAAgH4nyI2QP/uzpBTdKwEAgPUnyI2QLbZI9txTkAMAANafIDeCZs1KLr44eeKJtisBAAD6mSA3gmbOTB57LLnyyrYrAQAA+pkgN4Jmzqx73SsBAID1IciNoG23TXbbTZADAADWjyA3wmbNSi66KHnyybYrAQAA+pUgN8JmzkwefDC57rq2KwEAAPqVIDfCZs2qe90rAQCAdSXIjbCddkp23FGQAwAA1p0g14JZs2qQa5q2KwEAAPqRINeCmTOTe+5Jbrqp7UoAAIB+JMi1wDg5AABgfQhyLdhtt2TqVEEOAABYN10NcqWULUopZ5ZSfltKubGUsn83j9cvSlk2Tg4AAGBtdbtF7jNJftI0zXOT7Jnkxi4fr2/MnJncdlty++1tVwIAAPSbrgW5UsrkJLOSfClJmqZ5ommaB7p1vH4zNE7uwgvbrQMAAOg/3WyRm5ZkXpIvl1KuKqWcWkrZtIvH6yt77JFsuGFy9dVtVwIAAPSbbga5CUlekOTkpmn2TvJIkvev+KRSyvGllMtLKZfPmzevi+X0lgkTkuc+N7n++rYrAQAA+k03g9zcJHObprl06f0zU4Pd0zRNc0rTNDOappkxderULpbTewYHBTkAAGDtdS3INU1zV5I5pZTdlj40O8kN3TpePxocrJOdPPRQ25UAAAD9pNuzVr49yemllGuT7JXk/+/y8frK7rvX/Q3iLQAAsBYmdPPFm6a5OsmMbh6jnw0O1v311yf77dduLQAAQP/odoscqzFtWjJxonFyAADA2hHkWjR+fPK85wlyAADA2hHkWmbmSgAAYG0Jci0bHEzuuCN54IG2KwEAAPqFINcyM1cCAABrS5Br2dDMlddd124dAABA/xDkWrbTTskmmxgnBwAAdE6Qa9m4ccn06YIcAADQOUGuB5i5EgAAWBuCXA8YHEzuuiu57762KwEAAPqBINcDhiY80SoHAAB0QpDrAUNLEAhyAABAJwS5HrDDDsmkSZYgAAAAOiPI9YBSzFwJAAB0TpDrEWauBAAAOiXI9YjBwWTevLoBAACsjiDXI8xcCQAAdEqQ6xFmrgQAADolyPWIZz4zmTxZkAMAANZMkOsRpdTulZYgAAAA1kSQ6yFDM1c2TduVAAAAvUyQ6yGDg8l99yV33912JQAAQC8T5HqImSsBAIBOCHI9xMyVAABAJwS5HrLttslWWwlyAADA6glyPWRo5kpBDgAAWB1BrscMLUFg5koAAGBVBLkeMziYPPhg8sc/tl0JAADQqwS5HmPmSgAAYE0EuR4jyAEAAGsiyPWYbbZJpk4V5AAAgFUT5HqQmSsBAIDVEeR60FCQM3MlAACwMoJcDxocTB5+OJkzp+1KAACAXiTI9SATngAAAKsjyPUgQQ4AAFgdQa4HTZmSPOMZghwAALByglyPMnMlAACwKoJcjxocTG64IVmypO1KAACAXiPI9ajBweSRR5Lbbmu7EgAAoNcIcj1qaMKTG25otw4AAKD3CHI9avr0uhfkAACAFQlyPWrLLZPttjPhCQAA8KcEuR5m5koAAGBl1hjkSimbllLGLb29aynl8FLKBt0vjenTkxtvNHMlAADwdJ20yF2QZGIp5VlJzknyhiT/1s2iqIZmrrz99rYrAQAAekknQa40TfNoklcl+demaV6bZLC7ZZGY8AQAAFi5joJcKWX/JH+d5Oylj43vXkkMGVqCwDg5AABgeZ0EuXclOSHJd5umub6U8uwk53W3LBIzVwIAACs3YU1PaJrm/CTnJ8nSSU/mN03zjm4XRjV9uq6VAADA03Uya+XXSymbl1I2TXJdkhtKKe/tfmkktXvlDTeYuRIAAFimk66V05umeSjJEUl+nGRa6syVjIDp0+vMlXPmtF0JAADQKzoJchssXTfuiCQ/aJpmUZKmu2UxxIQnAADAijoJcl9IcmuSTZNcUErZKclD3SyKZYaWIBDkAACAIZ1MdvLZJJ9d7qHbSimHdK8klrfVVskznmHCEwAAYJlOJjuZXEr5p1LK5Uu3T6W2zjFCBge1yAEAAMt00rXytCQPJzly6fZQki93syiebmgJgsbIRAAAIB10rUyyc9M0r17u/odKKVd3qyD+1OBgnbny9tuTnXZquxoAAKBtnbTIPVZKOXDoTinlgCSPda8kVmTCEwAAYHmdtMi9JclXSimTk5Qk9yV5YzeL4umGliC44YbksMParQUAAGhfJ7NWXp1kz1LK5kvvW3pghA3NXKlFDgAASFYT5Eop/20VjydJmqb5py7VxEoMTXgCAACwujFyk9awMYIGB81cCQAAVKtskWua5kMjWQirN316smCBmSsBAIDOZq2kByw/4QkAADC2CXJ9whIEAADAEEGuT0yZkmy7rRY5AACgg+UHSikbJXl1koHln980zf/uXlmszOCgFjkAAKCzFrnvJ3llksVJHlluY4QNLUFg5koAABjb1tgil2T7pmkO7XolrNHgYJ25cs6cZMcd264GAABoSyctcr8qpezR9UpYIxOeAAAASWdB7sAkV5RSfldKubaU8ptSyrXdLow/ZQkCAAAg6axr5cu6XgUdGZq5UoscAACMbasMcqWUzZumeSjJwyNYD2swNOEJAAAwdq2uRe7rSV6e5IokTZKy3M+aJM/uYl2swuBg8pWv1JkrS1nz8wEAgNFnlUGuaZqXL91PG7lyWJPp05OHHzZzJQAAjGWdjJFLKWXLJM9JMnHosaZpLuhWUaza8hOeCHIAADA2rXHWylLKcUkuSPKfST60dP/B7pbFqgwFOROeAADA2NXJ8gPvTLJvktuapjkkyd5JHuhqVazSlCnJNtuY8AQAAMayToLcwqZpFiZJKWWjpml+m2S37pbF6gwOapEDAICxrJMgN7eUskWS7yX5aSnl+0lu625ZrM7QEgRN03YlAABAG9Y42UnTNH+59OYHSynnJZmc5CedHqCUMj7J5UnuGJoJk/UzOFhnrpw7N9lhh7arAQAARtpqW+RKKeNLKb8dut80zflN0/ygaZon1uIY70xy47oWyJ+aPr3uda8EAICxabVBrmmaJ5P8rpSyThPdl1K2T/IXSU5dl99n5ZZfggAAABh7OllHbssk15dSLkvyyNCDTdMc3sHv/nOSf0gyaVVPKKUcn+T4JNnRwmgd2XrrOnOlFjkAABibOgly/3NdXriU8vIk9zRNc0Up5eBVPa9pmlOSnJIkM2bMMH1HhwYHkyuuaLsKAACgDZ3MWnnY0rFxT21JDuvg9w5Icngp5dYk30zyolLK19ajVpZz+OHJNdck117bdiUAAMBI6yTI/flKHnvZmn6paZoTmqbZvmmagSSvS/Lzpmlev5b1sQqvf32y4YbJl77UdiUAAMBIW2WQK6W8pZTymyS7lVKuXW77QxLtQC3beuvkVa9KvvrVZOHCtqsBAABG0upa5L6e5BVJfrB0P7Tts7Yta03T/MIacsPvuOOS++9Pzjqr7UoAAICRtMog1zTNg03T3No0zdFN09y23HbfSBbIqh1ySDJtWnKqxR0AAGBM6WSMHD1q3LjaKnfeecnNN7ddDQAAMFIEuT73xjfWQGfSEwAAGDsEuT73zGcmf/EXyb/9W7JoUdvVAAAAI0GQGwWOOy65667kRz9quxIAAGAkCHKjwGGHJdttl3zxi21XAgAAjARBbhSYMCE59tjkxz9O5s5tuxoAAKDbBLlR4k1vSpYsqWPlAACA0U2QGyV23jmZPbvOXrlkSdvVAAAA3STIjSLHHZfcemty7rltVwIAAHSTIDeKHHFEstVWyamntl0JAADQTYLcKDJxYvKGNyTf/W4yf37b1QAAAN0iyI0yxx1XFwb/939vuxIAAKBbBLlRZvfdk/32q90rm6btagAAgG4Q5Eah445LbrwxufjitisBAAC6QZAbhY46Ktl00+QrX2m7EgAAoBsEuVFos82SV7wiOeusZPHitqsBAACGmyA3Sh15ZJ258uc/b7sSAABguAlyo9TLXlZb5s44o+1KAACA4SbIjVITJyavfGXtXvnEE21XAwAADCdBbhQ76qjk/vuTc89tuxIAAGA4CXKj2EtekkyenHzrW21XAgAADCdBbhTbaKPkiCOS730vefzxtqsBAACGiyA3yh11VPLgg8k557RdCQAAMFwEuVFu9uxkyy11rwQAgNFEkBvlNtwwedWrku9/P3nssbarAQAAhoMgNwYcdVSyYEHyk5+0XQkAADAcBLkx4JBDkq231r0SAABGC0FuDJgwIXn1q5P/+I/k0UfbrgYAAFhfgtwYceSRNcSdfXbblQAAAOtLkBsjDjoo2Wab5Iwz2q4EAABYX4LcGDF+fPKa19QWuQUL2q4GAABYH4LcGHLUUXUJgv/4j7YrAQAA1ocgN4YceGCy3Xa6VwIAQL8T5MaQceOS1742+fGPk4cearsaAABgXQlyY8xRRyWPP5784AdtVwIAAKwrQW6M2W+/ZPvtLQ4OAAD9TJAbY8aNq2vK/ed/JvPnt10NAACwLgS5MejNb06WLEne9762KwEAANaFIDcGTZ+evOc9yWmnJeed13Y1AADA2hLkxqgTT0x23jk5/vi6thwAANA/BLkxauONky98Ibn55uSkk9quBgAAWBuC3Bg2e3ZyzDHJP/5jcu21bVcDAAB0SpAb4z71qWTLLZO/+ZvkySfbrgYAAOiEIDfGTZmS/PM/J5ddlnzuc21XAwAAdEKQI0cfnRx6aPKBDyS33952NQAAwJoIcqSU5OSTk6ZJ3vrWugcAAHqXIEeSZGAg+fCHk7PPTr797barAQAAVkeQ4ynveEeyzz51f//9bVcDAACsiiDHUyZMSE49NZk/P3nf+9quBgAAWBVBjqfZa6/k2GOT009PFi1quxoAAGBlBDn+xEtfmjz6aHLFFW1XAgAArIwgx5+YNavuL7ig3ToAAICVE+T4E9tskzz3ucn557ddCQAAsDKCHCt10EHJRRclTz7ZdiUAAMCKBDlWatas5KGHkmuuabsSAABgRYIcK2WcHAAA9C5BjpXafvvk2c82Tg4AAHqRIMcqHXRQcuGFyZIlbVcCAAAsT5BjlWbNSu69N7nxxrYrAQAAlifIsUoHHVT3ulcCAEBvEeRYpYGBOlbOhCcAANBbBDlWqZTaKnf++UnTtF0NAAAwRJBjtWbNSu66K7n55rYrAQAAhghyrJZxcgAA0HsEOVZr112Tbbc1Tg4AAHqJIMdqlVK7V2qRAwCA3iHIsUazZiW3357cdlvblQAAAIkgRwdmzap7rXIAANAbBDnWaPfdky23NE4OAAB6hSDHGo0bl8ycqUUOAAB6hSBHRw46qK4l98c/tl0JAAAgyNGRoXFyulcCAED7BDk6stdeyaRJghwAAPQCQY6OTJiQHHCAcXIAANALBDk6dtBByQ03JPPmtV0JAACMbYIcHRsaJ3fhhe3WAQAAY50gR8dmzEg23tg4OQAAaJsgR8c23DDZf3/j5AAAoG2CHGvloIOSa65JHnig7UoAAGDs6lqQK6XsUEo5r5RyQynl+lLKO7t1LEbOrFlJ0yQXXdR2JQAAMHZ1s0VucZK/b5pmepL9krytlDK9i8djBLzwhbWL5S9+0XYlAAAwdnUtyDVNc2fTNFcuvf1wkhuTPKtbx2NkbLxxHSf385+3XQkAAIxdIzJGrpQykGTvJJeOxPHorhe/OLn66uTee9uuBAAAxqauB7lSymZJvpPkXU3TPLSSnx9fSrm8lHL5PCtN94XZs+s4ufPOa7sSAAAYm7oa5EopG6SGuNObpjlrZc9pmuaUpmlmNE0zY+rUqd0sh2Gy777JpEnJz37WdiUAADA2dXPWypLkS0lubJrmn7p1HEbehAl1GYJzz227EgAAGJu62SJ3QJI3JHlRKeXqpdthXTweI+jFL05uvjm5/fa2KwEAgLFnQrdeuGmai5KUbr0+7Zo9u+7PPTc59th2awEAgLFmRGatZPQZHEy23dY4OQAAaIMgxzopJXnRi+p6ck3TdjUAADC2CHKssxe/OLnrruSGG9quBAAAxhZBjnU2NE5O90oAABhZghzrbKedkp13tgwBAACMNEGO9TJ7dnL++cnixW1XAgAAY4cgx3p58YuThx5KLr+87UoAAGDsEORYL4ccUvfGyQEAwMgR5FgvW2+d7LWXcXIAADCSBDnW2+zZya9+lTz6aNuVAADA2CDIsd5mz06eeCL55S/brgQAAMYGQY71NnNmssEGxskBAMBIEeRYb5ttluy3n3FyAAAwUgQ5hsXs2cmVVyb33dd2JQAAMPoJcgyL2bOTpkl+8Yu2KwEAgNFPkGNYvPCFtYulcXIAANB9ghzDYoMNklmzjJMDAICRIMgxbGbPTn7/+2TOnLYrAQCA0U2QY9jMnl33WuUAAKC7BDmGzR57JFOnCnIAANBtghzDZty45EUvqkGuadquBgAARi9BjmE1e3Zy553Jb37TdiUAADD8yzjFAAAWsUlEQVR6CXIMqyOOSDbeOPnUp9quBAAARi9BjmE1dWrylrckp5+e3Hxz29UAAMDoJMgx7N7znrqu3Ec/2nYlAAAwOglyDLvttkuOPz75939P/vCHtqsBAIDRR5CjK/7hH+oslh/7WNuVAADA6CPI0RXPelZy3HHJl7+c3H5729UAAMDoIsjRNe97X91//OPt1gEAAKONIEfX7LhjcuyxyamnJnfc0XY1AAAweghydNUJJyRLlmiVAwCA4STI0VUDA8kxxySnnJLceWfb1QAAwOggyNF1J5yQLF6cfOITbVcCAACjgyBH1+28c/L61yef/3xy991tVwMAAP1PkGNEfOADyeOPJ5/6VNuVAABA/xPkGBG77pocfXTyuc8l8+a1XQ0AAPQ3QY4R89//e/LYY8mnP912JQAA0N8EOUbM856XHHlk8pnPJLfd1nY1AADQvwQ5RtTHP56UkrztbUnTtF0NAAD0J0GOEbXTTsmHP5ycfXby7W+3XQ0AAPQnQY4R9/a3J/vsk7zjHcn997ddDQAA9B9BjhE3YULyxS8m8+cn739/29UAAED/EeRoxd57J+9+d3LKKcmFF7ZdDQAA9BdBjtZ88IN1zNzxx9fFwgEAgM4IcrRm002Tk09OfvvbOpslAADQGUGOVr3sZcnRRycf+UgNdAAAwJoJcrTu05+urXPHH58sWdJ2NQAA0PsEOVq37bbJJz5RJz057bS2qwEAgN4nyNET3vSm5KCDkve+N7nrrrarAQCA3ibI0RNKSb7wheTRR5PjjtPFEgCg3z3wQHLHHW1XMXoJcvSM3Xar4+XOPjs58cS2qwEAYH0cc0z9fvfrX7ddyegkyNFT3vKW5M1vTk46KfnOd9quBgCAdXHffcmPfpQ88kidpdzs5MNPkKOnlJJ87nPJfvvVqzjXXdd2RQAArK3vfS9ZvDj55jeT8eOTl740mTu37apGF0GOnrPRRrU1btKk5JWvrFd0AADoH2eckey8c3LkkclPfpLcf38Nc77XDR9Bjp70zGcmZ52VzJlTFwx/8sm2KwIAoBPz5yc/+1kNcaUke++d/OAHyc03Jy9/ee1uyfoT5OhZ+++f/Ou/Jueck3zgA21XAwBAJ7773XoR/sgjlz128MHJN76RXHpp8trXJosWtVbeqCHI0dOOOy5561uTf/zH2scaAIDedsYZyXOek+y559Mff9Wrks9/Pvnxj5Njj7Xc1PoS5Oh5n/50MnNmXTT8qqvargYAgFW5557k5z9Pjjqqdqtc0d/8TfKRjySnn578t/+WNM3I1zhaCHL0vA03TL797WTKlOSII5K77267IgAAVuass2pL2/LdKld0wgnJu96VfOYz9YI960aQoy9su23tbz1vXp3x6P77264IAIAVnXFG8tznJrvvvurnlJJ86lPJq1+d/MM/JBddNHL1jSaCHH1jxoy6JsmNN9aFJR9+uO2KAAAYctddyfnnr7pb5fLGjUu+9KVk2rT6/HvuGZkaRxNBjr7ykpfUKz2XX5684hXJo4+2XREAAEldB3jJkjorZScmT07OPLOuLffXf225qbUlyNF3XvnK5KtfTS64IHnNa5Innmi7IlZm4cLaFRYAGBvOOCMZHKxbp/bcM/k//6euO/fhD3evttFIkKMvHX10csopdfrav/qrZPHititieUuWJIcfnuywQ3Liicljj7VdEQDQTX/8Y3Lhhauf5GRV3vSm5Jhjkv/9v+v6wXRGkKNvHXdcnenoO9+pJwBrkfSOz342+elPkxe8oJ6UBweTs89uuyoAoFvOPLMuJbAuQa6U5F//tX5f+Ou/TubOHf76RiNBjr72rnclJ51Uu1q+7W3WIukF11+fvP/9yctfnvzyl3UtmYkT6/0jjkhuvbXtCgGA4XbGGcnzn19nrFwXm2xSw+DChXXyk0WLhre+0UiQo+994APJ+96XfP7zyXvfK8y16Yknkte/Ptl88+TUU+sVtkMOSa6+Ovn4x2sr3fTpdSHQxx9vu1oAYDjMnVsv3q5La9zydtutfn/41a/qWnOsniBH3ysl+ehHk7/7u7omyTveoZtlWz74wRravvjFuvbfkA03rOvE/Pa3yWGHJf/jfyR77FEnrAEA+tu3v1336xvkktoa97a31e903/ve+r/eaCbIMSqUUsdlvec9deajY481AcpIu+ii2ur25jfXmUVXZocdareJn/ykhu1DDqm/oxUVAPrXGWcke++dPOc5w/N6n/pUXT/4mGOSt741Of305A9/8H1hRaXpob/IjBkzmssvv7ztMuhjTVO77f3P/5m86lXJ17+ebLRR21WNfg89VKcPHjeutshNmtTZ7xx3XL2Kd/jhyVe+kmyxRfdrBQCGz223JQMDtXfU+98/vK/7lrfUC8UPP1wf22675M/+LDnggLrfe+/a62e0KaVc0TTNjDU9b8JIFAMjpZTabW/SpDoRyuGHJ2edlWy6aduVjW7vfndy++21q2QnIS6p4+i+9a16Mn7Pe5J99qmtdXvv3d1aAYDhM5zdKpe3007Jj35UFwm/7ro6bu6Xv6z773ynPmfnnet3j2c+c3iP3S90rWRUeuc7k9NOq4tLvvSlyQMPtF3R6PW979W/9fvfX0PZ2iil/ltdcEGd/GT//esg5x7qKAAArMYZZ9RukM9+dndef/z42uvnLW9Jvva15JZb6pp1X/tactddyV/8Re3lMxYJcoxaxx5bW3wuu6yOxbrnnrYrGn3uuiv5m7+p68WdeOK6v87++ydXXZXMnFlf701vSh59dPjqBACG31VXJb/+9fC3xq3JdtvV9ebOPDP5zW+S17xmbC5XIMgxqr3mNckPfpD87nfJrFl11kSGR9PUMW4LFtR1/Na3j/rUqXUSlP/1v+p4uX32qZOm/Pmf137we+5ZB1E/85nJ5Mn1JP5Xf1VbA+fMGZ7/JgBgzebOTf72b5N9961DJV73unbqOPTQOlP2T39av5OMtR49xsgx6h16aPKf/1mb3p/3vNqf+s//vG6HHJJsuWXbFfanD30oOfvs5DOfqWvDDYfx4+vr7r9/XR/w1lvr+MZNN61Bb+j2ppsm8+Yl556bfOMb9Xd33bX+m774xfXfdfLk4akJAKjmz08+9rE6Q/iSJXVGyQ98IHnGM9qr6dhj6wXdE0+ss2OfdFJ7tYw0s1YyZtx+e/L979erNuedV1uSxo2r/bqHgt0BByQTXN5Yo3/6p+Tv/z554xuTL32p/h3b0DR1APTPflb/Xc8/v3bJHDcumT27dtE84ohk4sR26gOA0eDhh5NPfzr55CeTRx5J3vCGunbswEDblVVNkxx/fB1n//nP19bCftbprJWCHGPSokXJpZfWL/8//WkdR/fkk8lWW9XufK9+dW3ZsXTBnzr11DqO7TWvSb75zdqK1iueeCK5+OLaAvv1r9epi7fcsvajf9ObzIgJAGtj0aLkc5+rSzvNn1+Xdvrwh4evJ85wWry4fof7yU/qRGyveEXbFa07QQ7WwoMP1lad7343+Y//qLMfTZqUvPzlNdQdeqglDJI6eczRR9eZQL///d5eu2XJkuTnP69j6M46q86KuffeNdD91V/V0A4ArNwll9SWrWuvrb1cPvrROiauly1YUIdXXH998otfJP/lv7Rd0boR5GAdPf54DQDf+U4NK/PnJxtvnBx2WO2H/dKXjs3ulz/8YfKXf1nHr/3kJ8kmm7RdUefuv7+OpfvSl5Irr6wB9JBDkpe9rP67Puc5bVcIAL3hwQfruLeTT64TjP3Lv9RhCqW0XVln7r67TpL20EN1IpRddkl23LFOytIvBDkYBosXJxddVEPdGWfUJQye9aw6NuxNb+remim95rzzaujZY486wUg/nQxXdPXVde2Zs89eNovpLrvUQHfYYclBBxlTB8Do8uSTax4K0TT1+8473lGXF3r722s3yn78zP/975MDD6wTow2ZPLkGuuW3I4/sze9yghwMs0WLaqvUqafWFqklS2pXgze/ubZUjdYv/5dcUscLDgzUyUSmTGm7ouFzyy3Jj3+c/OhHtRV24cLa+rrffsm229bul0PblCnLbu+4Yw30nV6dbJo6Xu+yy+qVwhe9qI4v6JermwD0l3vvrRdhzz23bjffXC9aPv/5dTmf5z+/bgMD9bPo1luTv/u7epFz772TL3yh97tRrslDD9UulrffvvLtvvvq3+ZFL2q70j/VE0GulHJoks8kGZ/k1KZpPra65wty9Iu5c5N/+7faVe/WW+uEGvvuW692lVJnTRzaSqmPb7FFDQNbb/2n26ab1q4M991Xt/vvf/rtjTaq0+sPbc94xsiEgGuvrS1UW21VWya32677x2zLY4/V/vQ/+lFd3PS+++oH4f33r3xdmsmTk8HBZPfd6zZ0e5ttasvtr3+9bLvsstpFd3nTptUxmK94RV3j0MQ6wEhavLieq7baqrMLkY89VtdkveGGuv3+98n229fZng84oN3p56ljwy68cFlwu+aa+tm12Wb1c3yPPZKbbqqf6zffvOxzbdKk+tl1zTX1e8WHP1xb4sbCEJIFC+pQi14c7996kCuljE/y+yR/nmRukl8nObppmhtW9TuCHP1maEKNL3952YlxyZK6LX/7ySeTBx6oX+YXLVq7Y0yeXFuKHn982WObbbYs1O2ySx2vNvS/ctP86bZo0Z9uixfX/WOP1WmFFyyo++W3hQtry9OFF9bgMRYtWVJD9r33Lgt3t9xSr/Jdd13d33ffsudPmlT/dkn9UBwcrCF/333roOspU+qsmj/8YZ1gZ+HC+u/5kpfUUPeCF9RQt+I2cWJvzRAK9I/7769f1K+5pn6Rv+aaeu5auLD+fPPN60Wobbet29Dtxx9fFtxuuaWeD5N6LhoYSO64Y9lr7Lxz7co2FOye+9x6MfOJJ+p09QsWPH1bsqR+vk2eXC90Tp7cm1+oe82iRTVEX3fdss+g665b9h1kww3r+LDZs2tL0777Jhts8PTXWLCg/t611y57PzzrWcnHP157nNC+Xghy+yf5YNM0L116/4QkaZrmo6v6HUGO0a5p6pf8+fNrIJg/v24LFtQPsi23XNZ9b8st62Pjx9cgOGdOvZr2+98/fbv11mUfrqsyblw9ka9smzixho9Jk2qgWP725Ml16v6xGuI60TR1LMH119ftppvq32vffWso22yzVf/uo4/WCwE//GHd7rhj9ccaP37ZNm7cym8v3yK8stulrP52suz2yh4buj1kZbdX9fOhv9fy+1VZ8fc6qaVTK3vtTp+7vsfq1Lp8NK/Nf9favtZwWt1rr3gRavnHVnyNdXk/dPPfuhctXlw/J+bMWfbY1Km1a92ee9bwdf/9tcv3PffU/dB27731M2LXXWtX8KFtcLBOELXhhjWkXXVV7bHxy1/W/dCYpI03rp9dTzzReb0bb/z0ULf8eWj589TK/s1Xtl/T7SFr+//O2rxXhvP/yzvvrK2iQxeEx42r/z5DPUKGQnQ/TUbGyvVCkHtNkkObpjlu6f03JHlh0zR/t8Lzjk9yfJLsuOOO+9x2221dqQdGq8WL67a6L+JtLdhN55qmTsRyyy31KviK21Cr7JNP1m2opXdoP7QNfeldWcvw0P3ln7P87aE6VvdFevmPjJXdXtVjawp8K/4tVry/qlpWfO01Wdlrd/rctbW+v9+t/661fa2V/bybAXVNFxI6eW+u7bF7aLqAYVVKnchhaEzUnnt23jV/KCys2JqzOk1TW4Yuuqi29EycWC9mrWwrpfZ2ePDB2mNl+f2DD9YAuKpz1Yr/5ivbr+n28jWv7v7K/hvX5u+xrr+7stfaeutl3fl33z3ZbbfROz5/rOs0yLXeA7ZpmlOSnJLUFrmWy4G+M2HC2OjLPtqVUgeYW7Qc6AVrE+CGlFJb6yzpAiOjm9fp70iyw3L3t1/6GAAAAOuhm0Hu10meU0qZVkrZMMnrkvygi8cDAAAYE7rWIatpmsWllL9L8p+pyw+c1jTN9d06HgAAwFjR1ZE1TdP8KMmPunkMAACAscZcdgAAAH1GkAMAAOgzghwAAECfEeQAAAD6jCAHAADQZwQ5AACAPiPIAQAA9BlBDgAAoM8IcgAAAH1GkAMAAOgzghwAAECfEeQAAAD6jCAHAADQZwQ5AACAPlOapmm7hqeUUuYlua3tOlZi6yTz2y6CUc17jJHgfcZI8D6j27zHGAltvs92appm6pqe1FNBrleVUi5vmmZG23UwenmPMRK8zxgJ3md0m/cYI6Ef3me6VgIAAPQZQQ4AAKDPCHKdOaXtAhj1vMcYCd5njATvM7rNe4yR0PPvM2PkAAAA+owWOQAAgD4jyAEAAPQZQW41SimHllJ+V0q5uZTy/rbrYXQopexQSjmvlHJDKeX6Uso7lz6+VSnlp6WUm5but2y7VvpbKWV8KeWqUsoPl96fVkq5dOk57VullA3brpH+VkrZopRyZinlt6WUG0sp+zuXMdxKKe9e+nl5XSnlG6WUic5nrK9SymmllHtKKdct99hKz1+l+uzS99u1pZQXtFf5MoLcKpRSxif5XJKXJZme5OhSyvR2q2KUWJzk75ummZ5kvyRvW/reen+Sc5umeU6Sc5feh/XxziQ3Lnf/40k+3TTNLknuT/LmVqpiNPlMkp80TfPcJHumvt+cyxg2pZRnJXlHkhlN0+yeZHyS18X5jPX3b0kOXeGxVZ2/XpbkOUu345OcPEI1rpYgt2r/JcnNTdPc0jTNE0m+meSVLdfEKNA0zZ1N01y59PbDqV98npX6/vrK0qd9JckR7VTIaFBK2T7JXyQ5den9kuRFSc5c+hTvMdZLKWVykllJvpQkTdM80TTNA3EuY/hNSLJxKWVCkk2S3BnnM9ZT0zQXJLlvhYdXdf56ZZJ/b6pLkmxRStluZCpdNUFu1Z6VZM5y9+cufQyGTSllIMneSS5Nsm3TNHcu/dFdSbZtqSxGh39O8g9Jliy9PyXJA03TLF563zmN9TUtybwkX17ahffUUsqmcS5jGDVNc0eSTya5PTXAPZjkijif0R2rOn/1ZC4Q5KAlpZTNknwnybuapnlo+Z81dV0Qa4OwTkopL09yT9M0V7RdC6PahCQvSHJy0zR7J3kkK3SjdC5jfS0do/TK1AsHz0yyaf60OxwMu344fwlyq3ZHkh2Wu7/90sdgvZVSNkgNcac3TXPW0ofvHmqmX7q/p6366HsHJDm8lHJrarfwF6WOZdpiadekxDmN9Tc3ydymaS5dev/M1GDnXMZwenGSPzRNM69pmkVJzko9xzmf0Q2rOn/1ZC4Q5Fbt10mes3RWpA1TB9b+oOWaGAWWjlX6UpIbm6b5p+V+9IMkxyy9fUyS7490bYwOTdOc0DTN9k3TDKSeu37eNM1fJzkvyWuWPs17jPXSNM1dSeaUUnZb+tDsJDfEuYzhdXuS/Uopmyz9/Bx6nzmf0Q2rOn/9IMn/t3T2yv2SPLhcF8zWlNpqyMqUUg5LHWcyPslpTdN8pOWSGAVKKQcmuTDJb7Js/NIHUsfJnZFkxyS3JTmyaZoVB+HCWimlHJzkPU3TvLyU8uzUFrqtklyV5PVN0zzeZn30t1LKXqkT6myY5JYkx6ZeJHYuY9iUUj6U5KjUWZ+vSnJc6vgk5zPWWSnlG0kOTrJ1kruTnJjke1nJ+WvpRYT/k9qt99EkxzZNc3kbdS9PkAMAAOgzulYCAAD0GUEOAACgzwhyAAAAfUaQAwAA6DOCHAAAQJ8R5AAAAPqMIAcAANBn/h9VlPupwKLTRQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizations\n",
    "print(\"Training Loss\")\n",
    "visualize_loss(loss=op[0], _name=\"train loss\", _only_epoch=True)\n",
    "\n",
    "# if len(op[1]) > 0:\n",
    "\n",
    "print(\"Exact Match\")\n",
    "visualize_loss(loss=op[1], loss2=op[4], _label=\"train\", _label2=\"test\", _name=\"Exact Match\", _only_epoch=True)\n",
    "# visualize_loss(loss=op[1], _label=\"train em\", _label2=\"test em\", _only_epoch=True)\n",
    "\n",
    "print(\"F-Measure\")\n",
    "visualize_loss(loss=op[2], loss2=op[5], _label=\"train\", _label2=\"test\", _name=\"F-Measure\")\n",
    "\n",
    "# op[3]\n",
    "# print(op[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(op, open('./performance/domain-glove-17-07-2018/op.dump', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing (temp)\n",
    "# models = { 'ques_model': ques_model,\n",
    "#            'para_model': para_model,\n",
    "#            'mlstm_model':  mlstm_model,\n",
    "#            'pointer_decoder_model': pointer_decoder_model\n",
    "#          }\n",
    "# save_model(loc=macros['save_model_loc'], models=models, epochs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 150,  150,  150,  150,  150,  150,  150,  150,  150,  150,\n",
      "         150,  150,  150,  150,  150,  150,  150,  150,  150,  150,\n",
      "         150,  150,  150,  150,  150,  150,  150,  150,  150,  150,\n",
      "         150,  150,  150,  150,  150,  150,  150,  150,  150,  150,\n",
      "         150,  150,  150,  150,  150,  150,  150,  150,  150,  150], device='cuda:0')\n",
      "tensor([ 150,  150,  150,  150,  150,  150,  150,  150,  150,  150], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# # Try loading the model\n",
    "# ques_model = torch.load(os.path.join(macros['save_model_loc'], 'ques_model.torch'))\n",
    "# print(\"Ques Model\\n\", ques_model)\n",
    "\n",
    "# para_model = torch.load(os.path.join(macros['save_model_loc'], 'para_model.torch'))\n",
    "# print(\"Para Model\\n\", para_model)\n",
    "\n",
    "# mlstm_model = torch.load(os.path.join(macros['save_model_loc'], 'mlstm_model.torch'))\n",
    "# print(\"MLSTM Model\\n\", mlstm_model)\n",
    "\n",
    "# pointer_decoder_model = torch.load(os.path.join(macros['save_model_loc'], 'pointer_decoder_model.torch'))\n",
    "# print(\"Pointer Decoder model\\n\", pointer_decoder_model)\n",
    "\n",
    "# Create dummy data for testing the predict fn\n",
    "q = np.random.randint(0, len(vectors), (1, 30))\n",
    "p = np.random.randint(0, len(vectors), (1, 200))\n",
    "qa = np.repeat(q,macros['batch_size'],axis=0)\n",
    "pa = np.repeat(p,macros['batch_size'],axis=0)\n",
    "\n",
    "qb = np.repeat(q,10,axis=0)\n",
    "pb = np.repeat(p,10,axis=0)\n",
    "\n",
    "# print(p_repeat.shape)\n",
    "# print(q_repeat.shape)\n",
    "\n",
    "ysa, yea, _ = predict(torch.tensor(pa, dtype=torch.long, device=device), \n",
    "                                   torch.tensor(qa, dtype=torch.long, device=device),\n",
    "                                   ques_model=ques_model.eval(),\n",
    "                                   para_model=para_model.eval(),\n",
    "                                   mlstm_model=mlstm_model.eval(),\n",
    "                                   pointer_decoder_model=pointer_decoder_model.eval(),\n",
    "                                    macros=macros,\n",
    "                                    debug=macros['debug'])\n",
    "\n",
    "ysb, yeb, _ = predict(torch.tensor(pb, dtype=torch.long, device=device), \n",
    "                                   torch.tensor(qb, dtype=torch.long, device=device),\n",
    "                                   ques_model=ques_model.eval(),\n",
    "                                   para_model=para_model.eval(),\n",
    "                                   mlstm_model=mlstm_model.eval(),\n",
    "                                   pointer_decoder_model=pointer_decoder_model.eval(),\n",
    "                                    macros=macros,\n",
    "                                    debug=macros['debug'])\n",
    "\n",
    "print(torch.argmax(ysa.squeeze(), dim=1))\n",
    "print(torch.argmax(ysb.squeeze(), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_cap_start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fa5a9cdceeff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_cap_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_cap_start' is not defined"
     ]
    }
   ],
   "source": [
    "torch.argmax(y_cap_start.squeeze(), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
